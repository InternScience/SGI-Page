\appendix

\section{Appendix}

\subsection{Authors}

\textbf{Lead Authors}

Wanghan Xu$^{1,2}$, Yuhao Zhou$^{1,3}$, Yifan Zhou$^{1,2}$, Qinglong Cao$^{2}$, Shuo Li$^{1,4}$, Jia Bu$^{1,5}$

\textbf{Core Authors}

Bo Liu$^{6}$, Yixin Chen$^{1,7}$, Xuming He$^{1,8}$, Xiangyu Zhao$^{1,6}$, Xiang Zhuang$^{1,8}$, Fengxiang Wang$^{1,9}$, Zhiwang Zhou$^{1,10}$

\textbf{Contributors}

% 生物-孙思琦组
% 冯乾泰，黄文轩，卫佳其，吴浩，阳岳瑾，王光帅，许晟
Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu,
% 生物-何军军组
% 黄子炎，刘欣瑶，刘继垚，唐诚，李威，陈莹，宁俊智，蒋鹏飞，马成龙，杜烨，嵇长凯，许辉辉，胡铭
Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu,
% 生物-任昱宸组
% 郑蒋滨，陈鑫，吴宇成，蒋菲菲
Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, 
% 陈曦，唐相儒
Chen Xi, Xiangru Tang, 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 化学
% 符宇辰，Yingzhou Lu，Yuanyuan Zhang
Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 材料
% 孙李灏，李成博，马金哲，刘万昊
Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 天文
% 刘雅婷，巫国诚
Yating Liu, Kuo-Cheng Wu,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 物理-苏茂组
% 柴声都
Shengdu Chai,
% 物理-唐诗翔组
% 谢海润，张金欧文，王景，张舒飞，曹文博，任俊杰，崔涛镛
Hairun Xie, Jinouwen Zhang, Jing Wang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui,
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 能源
% 神经科学
% 伍佳敏，郑啟豪，姚周珩，邓俊涛，孙一介
Zhouheng Yao, Juntao Deng, Yijie Sun, 
% 地球-零风华
% 刘烽，魏旺栩，徐竞屹，李张瑞，龚俊超，郭子杰
Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, 
% 地球-ljt
% 孔浩，
% 信息
% 姚智宇，陈造宇，彭天豪
Zhiyu Yao, Zaoyu Chen, Tianhao Peng, 
% 数学
% 余方晨
Fangchen Yu

\textbf{Scientific Directors}

Bo Zhang$^{1}$, Dongzhan Zhou$^{1}$, Shixiang Tang$^{1}$, Jiaheng Liu$^{1,11}$, Fenghua Ling$^{1}$, Yan Lu$^{1}$, Yuchen Ren$^{1}$, Ben Fei$^{1,12}$, Zhen Zhao$^{1}$, Xinyu Gu$^{1}$, Rui Su$^{1}$, Xiao-Ming Wu$^{6}$, Weikang Si$^{13}$, Yang Liu$^{14}$, Hao Chen$^{1}$, Xiangchao Yan$^{1}$, Xue Yang$^{2}$, Junchi Yan$^{2}$, Jiamin Wu$^{1}$, Qihao Zheng$^{1}$, Chenhui Li$^{5}$, Zhiqiang Gao$^{1}$, Hao Kong$^{16}$, Junjun He$^{1}$, Mao Su$^{1}$, Tianfan Fu$^{1,11}$, Peng Ye$^{1,12}$, Chunfeng Song$^{1}$, Nanqing Dong$^{1}$, Yuqiang Li$^{1}$, Huazhu Fu$^{16}$, Siqi Sun$^{1,17}$, Jintai Lin$^{15}$, Wanli Ouyang$^{1,12}$, Bowen Zhou$^{1,18}$


\textbf{Corresponding Authors}

Wenlong Zhang$^{1}$, Lei Bai$^{1}$

\textbf{Main Affiliations}

$^1$ Shanghai Artificial Intelligence Laboratory

$^2$ Shanghai Jiao Tong University

$^3$ Sichuan University

$^4$ Central South University

$^5$ East China Normal University

$^6$ The Hong Kong Polytechnic University

$^7$ University of California, Los Angeles

$^8$ Zhejiang University

$^9$ National University of Defense Technology

$^{10}$ Tongji University

$^{11}$ Nanjing University

$^{12}$ The Chinese University of Hong Kong

$^{13}$ National Institute of Metrology

$^{14}$ Aerospace Information Research Institute,Chinese Academy of Sciences

$^{15}$ Peking University

$^{16}$ The Agency for Science, Technology and Research (A*STAR)

$^{17}$ Fudan University

$^{18}$ Tsinghua University




\subsection{Disciplines and Research Directions Overview}
\label{sec: all disciplines}

\definecolor{AstronomyColor}{RGB}{220,230,241}   % 浅蓝
\definecolor{ChemistryColor}{RGB}{255,230,230}   % 浅粉
\definecolor{EarthColor}{RGB}{230,245,230}       % 浅绿
\definecolor{EnergyColor}{RGB}{255,245,230}      % 浅橙
\definecolor{InfoColor}{RGB}{240,240,255}        % 浅紫
\definecolor{LifeColor}{RGB}{255,250,240}        % 浅黄色
\definecolor{MaterialColor}{RGB}{245,245,250}    % 浅灰紫
\definecolor{MathColor}{RGB}{250,245,255}        % 浅紫粉
\definecolor{NeuroColor}{RGB}{245,255,250}       % 浅青绿
\definecolor{PhysicsColor}{RGB}{250,245,240}     % 浅棕


\begin{longtable}{p{2cm}p{6cm}p{7cm}}

\caption{\textbf{Disciplines And Research Directions}: Overview of 10 scientific domains and representative research topics curated for scientist-aligned SGI-Bench workflows.}
\label{tab:Disciplines and Research Directions Overview}\\

\hline
\textbf{Disciplines} & \textbf{Research Directions} & \textbf{Description} \\
\hline
\endfirsthead
\hline
\textbf{Disciplines} & \textbf{Research Directions} & \textbf{Description} \\
\hline
\endhead

% ------------------ Astronomy ------------------
\rowcolor{AstronomyColor} Astronomy & Gravitational Wave Detection and Parameter Estimation & Analyzing data from interferometers like LIGO and Virgo to detect gravitational waves from compact binary coalescences (black holes, neutron stars) and precisely estimate their physical properties like mass, spin, and location to test general relativity. \\
\rowcolor{AstronomyColor} Astronomy & Fast Radio Burst Detection and Localization & Searching radio telescope data for millisecond-duration, extragalactic radio flashes (FRBs) and using interferometry to pinpoint their host galaxies, aiming to uncover the mysterious physical mechanisms that produce them. \\
\rowcolor{AstronomyColor} Astronomy & Real Time Optical Transient Survey Based on ZTF & Utilizing the Zwicky Transient Facility (ZTF) to scan the night sky, identifying new or changing celestial objects like supernovae and kilonovae, and issuing rapid alerts to the global astronomical community for multi-wavelength follow-up observations. \\
\rowcolor{AstronomyColor} Astronomy & Formula Regression & Applying symbolic regression and other machine learning techniques to large astronomical datasets to automatically discover novel mathematical formulas or physical laws that describe the behavior of celestial objects and phenomena. \\

% ------------------ Chemistry ------------------
\rowcolor{ChemistryColor} Chemistry & Molecular Interaction & Computationally simulating and quantifying the non-covalent forces between molecules, such as hydrogen bonds and van der Waals forces, to understand molecular recognition, protein-ligand binding, and self-assembly. \\
\rowcolor{ChemistryColor} Chemistry & Target Based Drug Design & Employing computational methods to design drug candidates that specifically bind to a known biological target, such as a protein's active site, thereby modulating its function to achieve a therapeutic effect. \\
\rowcolor{ChemistryColor} Chemistry & De Novo Drug Design & Using generative AI models to computationally design entirely new molecules with desired pharmacological properties, without starting from an existing chemical scaffold, to explore novel regions of chemical space. \\
\rowcolor{ChemistryColor} Chemistry & Chemical Molecular Synthesis Pathway Planning & Developing algorithms, often based on retrosynthesis, to devise the most efficient and practical multi-step reaction routes for synthesizing a target molecule, optimizing for yield, cost, and sustainability. \\
\rowcolor{ChemistryColor} Chemistry & Molecular Property Prediction & Building and applying machine learning models (e.g., QSAR) to predict key chemical and physical properties of molecules, such as toxicity, solubility, and bioactivity, to accelerate materials discovery and drug development. \\

% ------------------ Earth ------------------
\rowcolor{EarthColor} Earth & Seismic Wave Detection & Using networks of seismometers to detect and analyze seismic waves from earthquakes and other sources, enabling the study of fault lines and the tomographic imaging of the Earth's mantle and core. \\
\rowcolor{EarthColor} Earth & Ocean Heat Content & Aggregating and analyzing temperature data from sources like Argo floats and satellites to calculate the total thermal energy stored within the ocean, a critical indicator for quantifying global warming and climate change. \\
\rowcolor{EarthColor} Earth & Atmospheric Differential Equation & Numerically solving the complex systems of partial differential equations (e.g., Navier-Stokes equations) that govern atmospheric fluid dynamics and thermodynamics to produce accurate weather forecasts and climate projections. \\
\rowcolor{EarthColor} Earth & Typhoon Wind Pressure Relationship & Developing and validating models that describe the physical relationship between a typhoon's central pressure and its maximum sustained wind speeds, crucial for forecasting storm intensity and assessing potential damage. \\
\rowcolor{EarthColor} Earth & Vegetation Coverage Rate & Processing satellite and aerial imagery using spectral indices like NDVI to quantify the fraction of land covered by vegetation, which is vital for monitoring ecosystem health, agriculture, and deforestation. \\
\rowcolor{EarthColor} Earth & Glacier Estimation & Combining satellite altimetry, gravimetry (GRACE), and imagery to measure changes in glacier volume and mass balance over time, providing direct evidence of the impacts of climate change. \\
\rowcolor{EarthColor} Earth & Ozone Pollution and Its Causes & Investigating the chemical reactions between precursor pollutants (like NOx and VOCs) under sunlight that form harmful ground-level ozone, and modeling its transport and concentration in urban and rural areas. \\
\rowcolor{EarthColor} Earth & Emission Inversion Based on Satellite Remote Sensing and 4D-Var & Using advanced data assimilation techniques (4D-Var) to combine satellite measurements of atmospheric composition with chemical transport models, thereby inferring the location and strength of pollutant emission sources on the ground. \\
\rowcolor{EarthColor} Earth & Emission Inversion Based on Local Mass Conservation & Applying mass balance principles to high-resolution measurements (e.g., from aircraft) around a specific region to calculate the net flux and estimate emissions of greenhouse gases or pollutants from sources like cities or industrial facilities. \\
\rowcolor{EarthColor} Earth & Multiple Seismic Wave Attenuations & Modeling the progressive energy loss of seismic waves as they propagate through different geological materials, which helps in characterizing subsurface structures and identifying resources like oil and gas. \\

% ------------------ Energy ------------------
\rowcolor{EnergyColor} Energy & Optimal Power Flow Calculation & Developing algorithms to solve complex optimization problems for electrical grids, determining the best generator outputs to meet demand reliably while minimizing generation costs and transmission losses. \\
\rowcolor{EnergyColor} Energy & Fengguang New Energy Power Forecasting & Creating predictive models using meteorological data (wind speed, solar irradiance) and machine learning to accurately forecast the power output of wind and solar farms, which is essential for stable grid management. \\

% ------------------ Information ------------------
\rowcolor{InfoColor} Information & Multimodal Understanding & Building AI systems that can process, interpret, and reason about information from multiple sources simultaneously, such as text, images, audio, and video, to achieve a more holistic understanding. \\
\rowcolor{InfoColor} Information & Dialogue System & Designing and training conversational AI agents (chatbots) that can engage in natural, coherent, and context-aware conversations with humans for tasks like customer service or information retrieval. \\
\rowcolor{InfoColor} Information & Code Generation & Developing large language models and other AI techniques to automatically write, complete, and debug computer code based on natural language descriptions or functional specifications. \\
\rowcolor{InfoColor} Information & Sensor Spatial Characteristics Phase Free Reconstruction & Creating novel algorithms to reconstruct the spatial sensitivity pattern of a sensor (like a microphone or antenna) using only the magnitude of its measurements, without needing phase information, which is often difficult to obtain. \\

% ------------------ Life ------------------
\rowcolor{LifeColor} Life & De Novo Protein Sequencing & Developing computational methods to determine the amino acid sequence of a novel protein directly from its tandem mass spectrometry data, without relying on a reference genome. \\
\rowcolor{LifeColor} Life & Small Molecule Inference & Using computational models to predict the biological effects of small molecules, such as their binding targets, mechanism of action, or potential toxicity, based on their chemical structure. \\
\rowcolor{LifeColor} Life & Disease Biomarker Discovery & Analyzing high-throughput biological data (e.g., genomics, proteomics) with statistical and machine learning methods to identify molecules whose presence or level can indicate a specific disease state. \\
\rowcolor{LifeColor} Life & Tumor Neoantigen Discovery & Identifying unique peptides that arise from mutations in cancer cells, which can be recognized by the immune system, for the development of personalized cancer vaccines and immunotherapies. \\
\rowcolor{LifeColor} Life & RNA Tertiary Structure Prediction & Computationally predicting the complex three-dimensional folded structure of RNA molecules from their primary sequence to understand their function in cellular processes like gene regulation and catalysis. \\
\rowcolor{LifeColor} Life & Protein Structure & Predicting the three-dimensional atomic coordinates of a protein from its amino acid sequence using methods like deep learning (e.g., AlphaFold) or homology modeling to understand its biological function. \\
\rowcolor{LifeColor} Life & Genome Function Prediction & Annotating the functions of genes, regulatory elements, and non-coding regions across the genome by integrating diverse data types like DNA sequence, gene expression, and epigenetic modifications. \\
\rowcolor{LifeColor} Life & Automatic Development of Medical Imaging Algorithms & Creating AI-powered systems that can automatically generate and optimize image analysis pipelines for tasks like segmentation, registration, and classification in various medical imaging modalities (MRI, CT). \\
\rowcolor{LifeColor} Life & AI Drug Discovery & Applying a range of AI and machine learning techniques across the entire drug discovery pipeline, from identifying novel drug targets and designing molecules to predicting clinical trial outcomes. \\
\rowcolor{LifeColor} Life & Tumor Immunotherapy & Designing and developing therapeutic strategies, such as checkpoint inhibitors or CAR-T cells, that stimulate and enhance the patient's own immune system to recognize and attack cancer cells. \\
\rowcolor{LifeColor} Life & Revealing the Mechanisms of the Tumor Microenvironment & Studying the complex interplay between cancer cells, immune cells, stromal cells, and the extracellular matrix to understand how this environment promotes tumor growth and metastasis. \\
\rowcolor{LifeColor} Life & AI Assisted Antibody Design & Using machine learning models to design and optimize antibodies with high affinity and specificity for a given antigen, accelerating the development of new therapeutics and diagnostics. \\
\rowcolor{LifeColor} Life & Protein Structure Prediction & Developing and applying computational algorithms, particularly deep learning models, to accurately predict the 3D structure of proteins from their amino acid sequence. \\
\rowcolor{LifeColor} Life & Early Screening and Risk Stratification of Pancreatic Cancer & Developing novel diagnostic tools, such as blood-based biomarkers or AI-driven imaging analysis, to detect pancreatic cancer at an early, more treatable stage and to classify patients by risk level. \\
\rowcolor{LifeColor} Life & Protein Protein Interaction Prediction & Developing computational methods to predict which proteins in a cell will physically bind to each other, in order to map out the cellular signaling pathways and protein complexes. \\
\rowcolor{LifeColor} Life & Discovery of Immunotherapy Targets & Analyzing tumor and immune cell data to identify new molecular targets, such as surface proteins or mutated peptides, that can be exploited for cancer immunotherapy. \\
\rowcolor{LifeColor} Life & Biomarker Discovery & Identifying molecular signatures (genes, proteins, metabolites) in patient samples that can be used for disease diagnosis, prognosis, or predicting response to therapy. \\
\rowcolor{LifeColor} Life & Strain Metabolic Reconstruction & Creating comprehensive computational models of the metabolic networks of microbial strains to understand their physiology and guide metabolic engineering for producing valuable chemicals. \\
\rowcolor{LifeColor} Life & Regulatory Element Design & Designing synthetic DNA or RNA sequences, such as promoters and enhancers, to precisely control the expression of specific genes for applications in biotechnology and synthetic biology. \\
\rowcolor{LifeColor} Life & Computational Drug Design & Utilizing molecular modeling, simulation, and machine learning to design and optimize small molecules that can effectively bind to a biological target and modulate its activity. \\
\rowcolor{LifeColor} Life & Design of Regulatory Regions for mRNA Vaccine Drugs & Engineering the untranslated regions (UTRs) and other elements of mRNA sequences to optimize their stability, translation efficiency, and immune response for next-generation vaccine development. \\
\rowcolor{LifeColor} Life & Medical Image Understanding & Developing deep learning models to analyze and interpret complex medical images (e.g., X-rays, MRIs, pathology slides) to assist clinicians in diagnosis, treatment planning, and disease monitoring. \\

% ------------------ Material ------------------
\rowcolor{MaterialColor} Material & Polymer Thermoelectric & Designing and synthesizing polymer-based materials that can efficiently convert waste heat into useful electrical energy, focusing on enhancing their thermoelectric figure of merit (ZT). \\
\rowcolor{MaterialColor} Material & Thermal Electrocatalysis & Investigating how to use thermal energy to enhance the performance and efficiency of catalytic materials in electrochemical reactions, such as in fuel cells or water splitting. \\
\rowcolor{MaterialColor} Material & Nano Adsorption Materials & Developing porous nanomaterials like metal-organic frameworks (MOFs) or zeolites with high surface area and specific chemical properties for applications in gas separation, storage, and carbon capture. \\
\rowcolor{MaterialColor} Material & Chloride Solid State Electrolyte & Researching and developing novel solid-state materials that conduct chloride ions, aiming to create safer and more energy-dense all-solid-state batteries. \\
\rowcolor{MaterialColor} Material & Oxygen Evolution Reaction Catalytic Materials & Designing efficient, stable, and low-cost catalysts to accelerate the oxygen evolution reaction (OER), a key bottleneck in processes like water splitting for hydrogen production. \\
\rowcolor{MaterialColor} Material & KRF Resin Polymerization Reaction & Investigating and optimizing the chemical reaction conditions and mechanisms for the polymerization of ketone-resol-formaldehyde (KRF) resins to control their final properties for industrial applications. \\
\rowcolor{MaterialColor} Material & Polymer Thermoelectric & Researching and developing organic and composite polymer materials with high electrical conductivity and low thermal conductivity for flexible and lightweight thermoelectric devices. \\

% ------------------ Mathematics ------------------
\rowcolor{MathColor} Mathematics & Differential Privacy & Developing mathematical frameworks and algorithms that allow for the analysis of sensitive datasets while providing rigorous, provable guarantees about the privacy of individuals in the data. \\
\rowcolor{MathColor} Mathematics & Coordinate Descent Optimization Algorithm & Designing and analyzing efficient optimization algorithms that solve complex problems by iteratively optimizing one variable or a small block of variables at a time, while keeping others fixed. \\
\rowcolor{MathColor} Mathematics & Matrix Completion & Developing algorithms to accurately recover a full data matrix from a small subset of its observed entries, with applications in recommender systems and image inpainting. \\
\rowcolor{MathColor} Mathematics & Numerical Methods for Differential Equations & Devising and implementing stable and accurate computational algorithms (e.g., Runge-Kutta methods) for finding approximate solutions to differential equations that model real-world phenomena. \\
\rowcolor{MathColor} Mathematics & Shortest Path Planning & Developing and applying graph-based algorithms like Dijkstra's or A* to find the most efficient route between two points in a network, with applications in logistics, robotics, and network routing. \\

% ------------------ Neuroscience ------------------
\rowcolor{NeuroColor} Neuroscience & Visual Decoding & Using machine learning models to analyze brain activity patterns, typically from fMRI or electrophysiology, to reconstruct or identify the visual images a person is seeing. \\
\rowcolor{NeuroColor} Neuroscience & Motion Decoding & Developing brain-computer interfaces that can interpret neural signals from the motor cortex to predict intended movements, enabling control of prosthetic limbs or external devices. \\
\rowcolor{NeuroColor} Neuroscience & Emotion Recognition & Analyzing neurophysiological signals (like EEG) or behavioral cues (like facial expressions) with AI to identify and classify human emotional states. \\
\rowcolor{NeuroColor} Neuroscience & Electron Microscopy Neuron Segmentation & Creating automated computational pipelines, often using deep learning, to trace and segment individual neurons and their connections in large-scale electron microscopy volumes of brain tissue. \\
\rowcolor{NeuroColor} Neuroscience & Neural Activity and Behavior Prediction & Building statistical and dynamical models that link the activity of neural populations to specific behaviors, in order to understand the neural codes underlying perception, decision-making, and action. \\

% ------------------ Physics ------------------
\rowcolor{PhysicsColor} Physics & Computational Condensed Matter Physics & Using first-principles simulations (like Density Functional Theory) and many-body techniques to predict the electronic, magnetic, and structural properties of materials from fundamental quantum mechanics. \\
\rowcolor{PhysicsColor} Physics & Zeeman Effect Experiment & Precisely measuring the splitting of atomic spectral lines in the presence of an external magnetic field to probe the quantum mechanical properties of atoms, such as electron spin and angular momentum. \\
\rowcolor{PhysicsColor} Physics & Research on Soft Condensed Matter Physics and Glass Transition Dynamics & Investigating the physical principles governing the behavior of soft materials (polymers, colloids) and studying the complex, slow dynamics associated with the transition from a liquid to a glassy state. \\
\rowcolor{PhysicsColor} Physics & Deep PDE Solving to Enhance Model Expressiveness & Developing novel deep learning architectures, such as physics-informed neural networks (PINNs), to solve complex partial differential equations and improve the predictive power of physics-based models. \\
\rowcolor{PhysicsColor} Physics & Chaotic Behavior in Circuit Systems & Studying and modeling the emergence of chaos and other nonlinear dynamical behaviors in electronic circuits, such as the Chua's circuit, to understand fundamental principles of complex systems. \\
\rowcolor{PhysicsColor} Physics & Research on General Machine Learning Potential Function Model Architecture & Developing universal machine learning frameworks to accurately model the potential energy surface of molecular systems, enabling large-scale molecular dynamics simulations with quantum accuracy. \\
\rowcolor{PhysicsColor} Physics & Nuclear Magnetic Resonance and Its Imaging Experiment & Utilizing the principles of nuclear magnetic resonance to probe the structure and dynamics of molecules in materials and to create non-invasive medical images (MRI) of biological tissues. \\
\rowcolor{PhysicsColor} Physics & Quadrupole Mass Spectrometer & Studying the principles of using combined electric and magnetic fields in a quadrupole mass analyzer to separate ions based on their mass-to-charge ratio for chemical analysis. \\
\rowcolor{PhysicsColor} Physics & Research on Superconducting Mechanisms, Discovery of Superconducting Materials and Process Optimization & Investigating the fundamental quantum mechanisms of superconductivity, computationally searching for new materials with higher critical temperatures, and optimizing their synthesis for practical applications. \\

\hline
\end{longtable}

\subsection{Cases}

\subsubsection{Scientific Deep Research}

\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Astronomy,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

The Dispersion Measure (DM) of a Fast Radio Burst (FRB) is the integrated column density of free electrons along the line of sight. The observed value, $DM_{obs}$, is generally considered the sum of four primary components:
$DM_{obs} = DM_{MW} + DM_{halo} + DM_{IGM} + DM_{host,obs}$
where $DM_{MW}$ is the contribution from the Milky Way's interstellar medium, $DM_{halo}$ is from the Milky Way's halo, $DM_{IGM}$ is from the intergalactic medium, and $DM_{host,obs}$ is the contribution from the host galaxy in the observer's frame. The host contribution in its rest frame, $DM_{host,rest}$, is related to the observed value by $DM_{host,rest} = DM_{host,obs} / (1+z)$.
The Rotation Measure (RM) describes the Faraday rotation of a linearly polarized signal passing through a magnetized plasma. For the host galaxy, its contribution to the RM  as $RM_{host}$, which is highly relevant with $\langle B_{||} \rangle$, the average line-of-sight magnetic field strength in the host galaxy's environment, measured in microgauss ($\mu G$).
Astronomers have precisely localized the repeating FRB 20180814A and identified its host galaxy. The total observed dispersion measure is $DM_{obs} = 189.4 \ \text{pc} \cdot \text{cm}^{-3}$, and the spectroscopic redshift of the host is $z = 0.06835$. After subtracting the Galactic contribution, the extragalactic rotation measure is found to be $RM_{extragalactic} \approx 655 \ \text{rad} \cdot \text{m}^{-2}$, which is assumed to originate primarily from the FRB's host galaxy environment. Based on a detailed Bayesian model presented in the source paper, the total contribution from extragalactic sources (IGM + host) is determined to be $DM_{extragalactic,obs} = 64 \ \text{pc} \cdot \text{cm}^{-3}$, within which the IGM contribution is estimated as $DM_{IGM} = 45 \ \text{pc} \cdot \text{cm}^{-3}$.
Based on the information above, calculate the lower limit of the average line-of-sight magnetic field strength, $\langle B_{||} \rangle$, in the FRB's host galaxy environment. Provide a numerical answer in units of microgauss ($\mu G$), rounded to the nearest integer.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Search for the relevant paper about Sub-arcminute localization of 13 repeating fast radio bursts detected by CHIME/FRB.

\textbf{\textcolor{CaseOrange}{Step 2.}} Based on Macquart, $DM_{host,obs}=61.515 \text{pc} \cdot \text{cm}^{-3}$.

\textbf{\textcolor{CaseOrange}{Step 3.}} Calculate the contribution of the host galaxy to the observer coordinate system $(DM_{host,obs}=5.885  \text{pc} \cdot \text{cm}^{-3})$.

\textbf{\textcolor{CaseOrange}{Step 4.}} Calculate the contribution of the host galaxy in the stationary coordinate system $(DM_{host,rest}=5.508 \text{pc} \cdot \text{cm}^{-3})$.

\textbf{\textcolor{CaseOrange}{Step 5.}} Calculate the average magnetic field intensity $\langle B_{||} \rangle = 46 \mathrm{\mu G}$.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

46

\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Chemistry,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

In computational chemistry, the accurate parsing of a molecule's structure is fundamental to predicting its properties. A critical structural attribute is aromaticity, and its determination often follows Huckel's rule.

Consider the neutral molecule, an isomer of Naphthalene, represented by the following SMILES string:

c1cccc2cccc-2cc1

For the entire conjugated system of this molecule to be considered aromatic, how many $\pi$-electrons in total must its $\pi$-electron system contain?

Provide the answer as a single integer.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Find the article title "DrugAgent: Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration".

\textbf{\textcolor{CaseOrange}{Step 2.}} Parse the SMILES Structure: The SMILES string c1cccc2cccc-2cc1 describes the molecule Azulene, a bicyclic conjugated system formed by the fusion of a five-membered ring and a seven-membered ring. Correctly identifying this non-standard structure is the first hurdle.

\textbf{\textcolor{CaseOrange}{Step 3.}} Correspondence to Document: This step directly corresponds to the initial input processing stage shown in Figure 1 (b) 'DrugCoder' (Page 3), where a 'SMILES string' is taken as input before the 'Molecule Graph Construction' module.

\textbf{\textcolor{CaseOrange}{Step 4.}} Define the System for Analysis: The key phrase in the question is 'entire conjugated system.' Azulene's two rings form a single, continuous, planar $\pi$-conjugated system. The most critical trap is to avoid analyzing the five- and seven-membered rings separately, which would lead to an incorrect conclusion.

\textbf{\textcolor{CaseOrange}{Step 5.}} Correspondence to Document: This conceptual step is an implicit requirement of the 'Molecule Graph Construction' module in Figure 1 (b) (Page 3). A correct graph cannot be built without correctly identifying the holistic nature of the conjugated system, which determines the properties of the graph's nodes (atoms) and edges (bonds).

\textbf{\textcolor{CaseOrange}{Step 6.}} Count the Total $\pi$-Electrons: The entire conjugated system of Azulene is composed of 10 carbon atoms. In this neutral hydrocarbon, each carbon atom participating in the conjugation contributes one $\pi$-electron. Therefore, the total number of $\pi$-electrons is 10.

\textbf{\textcolor{CaseOrange}{Step 7.}} Correspondence to Document: This calculation is a core part of the feature extraction process. This concept is explicitly mentioned in the 'Idea Space' section (lines 12-13, Page 5 of the PDF), which suggests to 'extract molecular descriptors and fingerprints from the SMILES strings'. The $\pi$-electron count is a fundamental molecular descriptor.

\textbf{\textcolor{CaseOrange}{Step 8.}} Verify with Huckel's Rule: Apply the total $\pi$-electron count (10) to Huckel's rule, 4n + 2. Setting 4n + 2 = 10 gives 4n = 8, which solves to n = 2. Since ‘n' is an integer, the system satisfies the rule and is aromatic. The question asks for the total number of $\pi$-electrons, which is 10.

\textbf{\textcolor{CaseOrange}{Step 9.}} Correspondence to Document: This verification step is critical for assigning correct properties to the constructed molecular graph, which is the foundation for all downstream tasks, such as 'ADMET Prediction' mentioned in Table 1 (Page 3). An incorrect determination of aromaticity would lead to a flawed graph and an inaccurate final prediction.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

10

\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Earth,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

The diurnal variation of the NO\(_2\) column concentration \( \Omega \) over a city is governed by local mass balance, incorporating emissions, chemical loss, and photochemical production. The governing equation is:

\[
\frac{d\Omega}{dt} = E(t) + P(t) - \frac{\Omega}{\tau}
\]

Where:

\[
E(t) = 3.0 \times e^{-t/2} \quad (\text{NO}_x \text{ emission rate in molec/cm}^2/\text{h},\ t \text{ in hours starting from 8:00 AM})
\]

\[
P(t) = 1.5 \times t \quad (\text{Photochemical NO}_2 \text{ production rate in molec/cm}^2/\text{h}^2)
\]

\[
\tau = 1.5\ \text{hours} \quad (\text{NO}_2 \text{ effective lifetime})
\]

At \( t=1 \) (9:00 AM), the observed concentration is \( \Omega_1 = 4.2 \).

Questions:

1. What was the initial NO\(_2\) column concentration \( \Omega_0 \) at \( t=0 \) (8:00 AM)?

2. At what time \( t_{\text{peak}} \) does \( \Omega(t) \) reach its maximum value between 8:00 AM and 12:00 PM?

3. At the time of the peak concentration, which is larger, the photochemical production term \( P(t) \) or the emission term \( E(t) \), and by how much? Round the results of the first and third questions to two decimal places.

Present your final answers as numbers separated by commas.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Find paper "Constraint of anthropogenic NO$_x$ emissions in China from different sectors: a new methodology using multiple satellite retrievals".

\textbf{\textcolor{CaseOrange}{Step 2.}} Solving for $\Omega_0$: Corresponding Text: Equation (1) on Page 6: $\frac{\delta \Omega_{\mathrm{NO}_x}}{\delta t} = E - \frac{\Omega_{\mathrm{NO}_x}}{\tau}$. This problem adds a chemical production term $P(t)$ to this equation.

\textbf{\textcolor{CaseOrange}{Step 3.}} Formulate the governing equation: $\frac{d\Omega}{dt} + \frac{1}{1.5}\Omega = 3e^{-t/2} + 1.5t$.

\textbf{\textcolor{CaseOrange}{Step 4.}} Solve this first-order linear differential equation using the integrating factor method, which is used in the paper to derive the key discrete solution (Equation (2) on Page 6). The integrating factor is $\mu(t) = \exp\left(\int \frac{2}{3}dt\right) = \exp\left(\frac{2t}{3}\right)$.

\textbf{\textcolor{CaseOrange}{Step 5.}} Integrate from the initial time ($t=0$) to the observation time ($t=1$):

\textbf{\textcolor{CaseOrange}{Step 6.}} $\left[\Omega \exp\left(\frac{2t}{3}\right)\right] \Big|_0^1 = \int_0^1 \exp\left(\frac{2u}{3}\right)\left[3e^{-u/2} + 1.5u\right] du$

\textbf{\textcolor{CaseOrange}{Step 7.}} This yields $\Omega_1 \exp\left(\frac{2}{3}\right) - \Omega_0 = 4.449$.

\textbf{\textcolor{CaseOrange}{Step 8.}} Substitute $\Omega_1 = 4.2$ and solve for $\Omega_0$: $(4.2 \times 1.9477) - \Omega_0 \approx 4.449$, resulting in $\Omega_0 \approx 3.73$.

\textbf{\textcolor{CaseOrange}{Step 9.}} Solving for $t_{\text{peak}}$: Corresponding Text: At the peak, $\frac{d\Omega}{dt} = 0$, which is a direct application of the mass conservation equation. The analysis must also consider the assumptions of ``short lifetime'' and ``photochemistry dominance'' mentioned on Page 7.

\textbf{\textcolor{CaseOrange}{Step 10.}} Find the complete function describing concentration evolution over time, $\Omega(t)$. Solving the differential equation gives: $\Omega(t) = 18\exp(-t/2) + 2.25t - 3.375 - 10.894\exp(-2t/3)$.

\textbf{\textcolor{CaseOrange}{Step 11.}} Differentiate $\Omega(t)$: $\frac{d\Omega}{dt} = -9\exp(-t/2) + 2.25 + 7.263\exp(-2t/3)$.

\textbf{\textcolor{CaseOrange}{Step 12.}} Analyze the sign of $\frac{d\Omega}{dt}$. Calculating the derivative values at $t=1, 2, 3, 4$ hours shows it is consistently positive.

\textbf{\textcolor{CaseOrange}{Step 13.}} Conclusion: Within the given time window $[0, 4]$ hours, the concentration $\Omega(t)$ is monotonically increasing, and no peak occurs. This means the strength of the sources ($E(t) + P(t)$) is always greater than the sink ($\Omega/\tau$) throughout the morning.

\textbf{\textcolor{CaseOrange}{Step 14.}} Comparing $P(t)$ and $E(t)$: Corresponding Text: A core aspect of the paper's method is analyzing contributions from different sources (e.g., the four emission sectors). Here we compare two different source terms.

\textbf{\textcolor{CaseOrange}{Step 15.}} Since the concentration is monotonically increasing with no peak, we choose the end of the time window ($t=4$) to assess the relative importance of the sources.

\textbf{\textcolor{CaseOrange}{Step 16.}} Calculate the values at $t=4$: $E(4) = 3.0 \times e^{-2} \approx 0.406$.

\textbf{\textcolor{CaseOrange}{Step 17.}} $P(4) = 1.5 \times 4 = 6.0$.

\textbf{\textcolor{CaseOrange}{Step 18.}} Compare and calculate the difference: $P(4) - E(4) \approx 5.59$. This result indicates that at this time, photochemical production has become a significantly more important source of NO$_2$ than anthropogenic emissions.

\textbf{\textcolor{CaseOrange}{Step 29.}} Final Answer: $3.73$, no peak, $5.59$

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

3.73, no peak, 5.59

\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Energy,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

A parabolic trough solar collector at steady state follows the energy balance
\[
q_u = F_r\left[ K_\theta (\tau \alpha) G - U_L (T_f - T_a) \right]
\]
and instantaneous efficiency
\[
\eta = \frac{q_u}{G}.
\]
The heat removal factor depends on mass flow via
\[
F_r = \frac{\dot{m} c_p}{A U_L} \left[1 - \exp\left(-\frac{F' A U_L}{\dot{m} c_p}\right)\right].
\]
Given: \( F' = 0.94 \), \( A = 6.00\,\mathrm{m}^2 \) (receiver heat-transfer area), \( U_L = 2.20\,\mathrm{W/m}^2\cdot\mathrm{K} \), \( (\tau\alpha) = 0.90 \), \( K_\theta = 0.96 \), \( G = 950\,\mathrm{W/m}^2 \), \( T_f = 150^\circ\mathrm{C} \), \( T_a = 35^\circ\mathrm{C} \), \( c_p = 4180\,\mathrm{J}/\mathrm{kg}\cdot\mathrm{K} \), and baseline mass flow \( \dot{m} = 0.12\,\mathrm{kg}/\mathrm{s} \). Answer the following (round to two decimals; use ENGLISH commas, no spaces, no units):

(1) Baseline heat removal factor \( F_r \).

(2) Baseline efficiency \( \eta \).

(3) Minimum mass flow (kg/s) required to guarantee \( \eta \geq 0.58 \) under the same operating conditions.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Find paper 2D-interval forecasts for solar power production.

\textbf{\textcolor{CaseOrange}{Step 2.}} Compute temperature difference: \( \Delta T = T_f - T_a = 150 - 35 = 115\,\mathrm{K} \).

\textbf{\textcolor{CaseOrange}{Step 3.}} Compute absorbed solar term with IAM: \( S = K_\theta (\tau\alpha) G = 0.96 \times 0.90 \times 950 = 0.864 \times 950 = 820.80\,\mathrm{W/m}^2 \).

\textbf{\textcolor{CaseOrange}{Step 4.}} Compute loss term: \( U_L \Delta T = 2.20 \times 115 = 253.00\,\mathrm{W/m}^2 \).

\textbf{\textcolor{CaseOrange}{Step 5.}} Baseline heat removal factor \( F_r \): first find \( \dot{m} c_p = 0.12 \times 4180 = 501.60\,\mathrm{W/K} \), and \( A U_L = 6.00 \times 2.20 = 13.20\,\mathrm{W/K} \). Define \( x = \frac{F' A U_L}{\dot{m} c_p} = \frac{0.94 \times 13.20}{501.60} = \frac{12.408}{501.60} = 0.02474 \). Then \( F_r = \frac{1 - e^{-x}}{x} = \frac{1 - e^{-0.02474}}{0.02474} \approx 0.99 \) (more precisely 0.988--0.989). \( \rightarrow \) (1) \( F_r = 0.99 \) (two decimals).

\textbf{\textcolor{CaseOrange}{Step 6.}} Baseline useful gain and efficiency: \( q_u = F_r (S - U_L \Delta T) = 0.989 \times (820.80 - 253.00) \approx 0.989 \times 567.80 \approx 561.60\,\mathrm{W/m}^2 \). \( \eta = q_u/G = 561.60/950 = 0.5912 \rightarrow \) (2) \( 0.59 \).

\textbf{\textcolor{CaseOrange}{Step 7.}} Target efficiency requirement: \( \eta_\mathrm{target} = 0.58 \Rightarrow \) required heat removal factor \( F_{r,\mathrm{req}} = \frac{\eta_\mathrm{target} \times G}{S - U_L \Delta T} = \frac{0.58 \times 950}{567.80} = \frac{551.00}{567.80} = 0.9704 \).

\textbf{\textcolor{CaseOrange}{Step 8.}} Solve for minimum mass flow producing \( F_r \geq F_{r,\mathrm{req}} \) using \( F_r = \frac{1 - e^{-x}}{x} \) with \( x = \frac{F' A U_L}{\dot{m} c_p} \). For small \( x \), \( \frac{1 - e^{-x}}{x} \) is monotone decreasing in \( x \) and \( \approx 1 - \frac{x}{2} \). Set \( 1 - \frac{x}{2} \approx 0.9704 \Rightarrow x \approx 0.0592 \). Then \( \dot{m} c_p = \frac{F' A U_L}{x} = \frac{12.408}{0.0592} = 209.6\,\mathrm{W/K} \Rightarrow \dot{m} = \frac{\dot{m} c_p}{c_p} = \frac{209.6}{4180} = 0.0501\,\mathrm{kg/s} \rightarrow \) (3) \( 0.05 \) (two decimals).

\textbf{\textcolor{CaseOrange}{Step 9.}} Check: With \( \dot{m} = 0.05\,\mathrm{kg/s} \), \( x = 12.408/(0.05 \times 4180) = 12.408/209 \approx 0.0594 \Rightarrow F_r \approx \frac{1-e^{-0.0594}}{0.0594} \approx 0.97 \), yielding \( \eta \approx 0.58 \) as required.


\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

0.99,0.59,0.05

\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Information,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

In the research of electromagnetic measurement focusing on broadband planar near-field \(E\)-field reconstruction, a microstrip patch-based \(4 \times 5\) array antenna is used as the Antenna Under Test (AUT). The AUT's planar near-field scanning is performed in a region close to its aperture, and the \(E\)-field at this region is transformed to two parallel observation planes (\(S_1\) and \(S_2\)) via spatial convolution. The transformation satisfies the field distribution similarity theory: the ratio of the observation distances \((d_2/d_1)\) between \(S_2\) and \(S_1\) equals the ratio of the corresponding test frequencies \((f_2/f_1)\). For the \(E\)-field dataset on \(S_2\) (target frequency \(f_2\)), undersampling is applied (sampling interval larger than \(\lambda_2/2\), where \(\lambda_2\) is the wavelength at \(f_2\)) to form a defective dataset \(X_2\). To reconstruct \(X_2\), K-means clustering is first used to classify \(X_2''\), with the optimal number of clusters determined by the ``elbow point'' of the SSE (sum of squared errors) curve. Then Voronoi cell classification is employed, where the comprehensive index \(L(p_m) = q_1 S(p_m) + q_2 D(p_m)\) \((q_1 + q_2 = 1)\) is calculated to divide each cluster into deep interpolation regions (requiring 24 supplementary samples per point) and shallow interpolation regions (requiring 8 supplementary samples per point). It is known that: 

1) The test frequency \(f_1 = 28\,\mathrm{GHz}\), and the observation distance \(d_1 = 214.29\,\mathrm{mm}\) (corresponding to \(20\lambda_1\), \(\lambda_1\) is the wavelength at \(f_1\)); 

2) The scanning area of the near-field region close to the AUT aperture is a square, and the sampling interval of \(X_2\) is \(0.8\lambda_2\); 

3) The total number of sampling points in \(X_2\) is \(1681\); 

4) For a specific cluster after K-means classification, the normalized cell area \(S(p_m)\) of sampling points in the deep interpolation region is \(1.2\) times that of points in the shallow region, and the normalized gradient \(D(p_m)\) of shallow region points is \(0.7\) times that of deep region points; 

5) The weight \(q_1\) is set to \(0.6\) to prioritize area-based judgment for dynamic clusters. 

If the number of sampling points in this cluster where \(L(p_m) \geq 0.6\) is \(112\), calculate the total number of supplementary interpolation samples for this cluster, unit: pieces. Do not keep any decimal places in the result.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Retrieve core data from the paper "An Efficient Data Reconstruction Method for Broadband Planar Near-Field Measurements Based on the Field Distribution Similarity."

\textbf{\textcolor{CaseOrange}{Step 2.}} From Section III.A "Simulations": \( X_2'' \) (defective dataset at \( f_2 \)) is a \( 41 \times 41 \) sampling grid, so total sampling points of \( X_2'' = 41 \times 41 = 1681 \); optimal K-means clustering number \( k=5 \) (determined by SSE curve's elbow point); deep interpolation requires 24 samples per point, shallow interpolation requires 8 samples per point.

\textbf{\textcolor{CaseOrange}{Step 3.}} Calculate the total number of sampling points in the target cluster: \( X_2'' \) is evenly divided into 5 clusters (paper's clustering logic for uniform data distribution). Single cluster points = Total \( X_2'' \) points \( \div k = 1681 \div 5 = 336.2 \). Since sampling points are discrete integers, round to the nearest integer: 336 pieces.

\textbf{\textcolor{CaseOrange}{Step 4.}} Determine the number of deep and shallow interpolation points in the cluster: The question specifies deep region points = \( \frac{1}{3} \) of cluster total points. Deep region points = \( 336 \times \frac{1}{3} = 112 \) pieces; shallow region points = Total cluster points - Deep region points = \( 336 - 112 = 224 \) pieces. (This ratio is consistent with the paper's "deep regions are undersampled, sparse points" logic, no fabricated data.)

\textbf{\textcolor{CaseOrange}{Step 5.}} Calculate total supplementary interpolation samples: Supplementary samples for deep region = Deep region points \(\times\) Samples per deep point = \( 112 \times 24 = 2688 \) pieces; Supplementary samples for shallow region = Shallow region points \(\times\) Samples per shallow point = \( 224 \times 8 = 1792 \) pieces; Total supplementary samples = \( 2688 + 1792 = 4480 \) pieces.


\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

4480

\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Life,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

In the DeepSTARR model, a human enhancer contains two identical p53 core motifs (\texttt{RRRCWWGYYY}) at positions $+50$ and $+150$. Experimental data show:
\begin{itemize}
    \item Mutating the $+50$ motif alone reduces H3K27ac signal to $35\%$ of wild-type
    \item Mutating the $+150$ motif alone reduces H3K27ac signal to $82\%$ of wild-type
    \item DNase I footprinting shows TF binding at the $+50$ motif but no binding at the $+150$ motif
    \item Changing the 5' flanking sequence of the $+150$ motif from ``GGG'' to ``CTC'' confers TF binding ability
    \item Known effects of flanking sequences on p53 binding:
    \begin{itemize}
        \item Optimal flank ``GGG'' : increases binding affinity by $8$-fold
        \item Suboptimal flank ``CTC'' : increases binding affinity by $3$-fold
        \item Random flank: binding affinity $= 1$ (baseline)
    \end{itemize}
\end{itemize}
Assume H3K27ac signal strength is proportional to p53 binding affinity, and total signal equals the sum of both motifs' binding affinities.

If the $+50$ motif's flank is changed from ``GGG'' to ``CTC'' and the $+150$ motif's flank is changed from ``GGG'' to ``CTC'', what is the predicted H3K27ac signal as a percentage of wild-type? The result retains the integer.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Find the article title ``DeepSTARR predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancers''

\textbf{\textcolor{CaseOrange}{Step 2.}} Determine wild-type binding affinities

$+50$ motif: flank ``GGG'' $\rightarrow$ affinity $= 8$ (Article: Fig. 4 \& related text -- flanking sequences significantly influence motif importance by altering TF binding affinity)

$+150$ motif: flank ``GGG'' but no DNase footprint $\rightarrow$ affinity $= 1$ (Article: Fig. 6d -- motifs without DNase I footprints show minimal functional contribution)

\textbf{\textcolor{CaseOrange}{Step 3.}} Total affinity $= 8 + 1 = 9$.

\textbf{\textcolor{CaseOrange}{Step 4.}} Calculate modified binding affinities

$+50$ motif: flank ``CTC'' $\rightarrow$ affinity $= 3$ (Article: Fig.~4b -- flanking sequences quantitatively modulate motif contribution)

$+150$ motif: flank ``CTC'' $\rightarrow$ affinity $= 3$ (now gains binding ability)

\textbf{\textcolor{CaseOrange}{Step 5.}} Total affinity $= 3 + 3 = 6$.

\textbf{\textcolor{CaseOrange}{Step 6.}} 4. Calculate signal percentage

\textbf{\textcolor{CaseOrange}{Step 7.}} Modified signal $= \left(\frac{6}{9}\right) \times 100\% \approx 66.7\% \rightarrow 67$, So the answer is 67 (Article: Linear relationship between binding affinity and enhancer activity demonstrated in multiple figures)


\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

67

\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Material,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Polymer composite materials have the advantages of flexibility, low cost, and environmental friendliness, and are considered the most promising candidate materials for low-grade heat collection, thermal sensing, and sustainable energy development. Solid-state $i$-TE materials can undergo thermal power changes according to electrode conditions in a fixed temperature and humidity environment. So, when the relative humidity increases from 50\% to 70\%, what changes will occur in the thermal power of the poly(vinylidene fluoride-co-hexafluoropropane) sample on the $p$-type dual copper electrode?

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Find paper: Reversible bipolar thermopower of ionic thermoelectric polymer composite for cyclic energy generation

\textbf{\textcolor{CaseOrange}{Step 2.}} Understanding the working principle of poly (vinylidene fluoride-co-hexafluoropropane) materials for p-type dual copper electrodes: the porous structure and hydrophilicity of sodium salts tend to absorb moisture from humid environments and can fill the space of the poly (vinylidene fluoride-co-hexafluoropropane) matrix,

\textbf{\textcolor{CaseOrange}{Step 3.}} Identifying the impact of increased water absorption on thermopower: increased water absorption leads to an increase in thermopower (i.e., the Seebeck coefficient, $S$), but does not alter the p-type characteristics of the material,

\textbf{\textcolor{CaseOrange}{Step 4.}} The result of comparative reasoning is that when the relative humidity increases from $50\%$ to $70\%$, the thermopower of the poly (vinylidene fluoride-co-hexafluoropropane) sample of the p-type dual copper electrode will increase.


\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

Increase

\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Math,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

A third-order homogeneous linear ordinary differential equation, $f'''(z) - 3 f'(z) + \beta f(z) = 0$ (where $\beta$ is a real parameter), is analyzed using a Legendre collocation matrix method. The function $f(z)$ is approximated by a truncated Legendre series with $N=3$.

To determine the coefficient vector $A = [a_0, a_1, a_2, a_3]^T$, a $4\times4$ homogeneous linear system $\widetilde{W} A = 0$ is constructed. For the system to have a non-trivial solution, it must satisfy the following four conditions:

\[
f(0) = 0, \ f'(0) = 0 \\
\]

The differential equation is satisfied at the collocation point(z=1). The differential equation is satisfied at the collocation point(z=-1).

For the system to have a non-trivial solution, the parameter $\beta$ must satisfy $\beta^2 = K$. Calculate the value of the constant $K$. Round your answer to the nearest integer.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Find the article title ``Numerical solution for high-order linear complex differential equations with variable coefficients''

\textbf{\textcolor{CaseOrange}{Step 2.}} Establish High-Order Derivative Relations. The $n$-th derivative is expressed in matrix form as $f^{(n)}(z) = L(z)(M^T)^n A$. For $N=3$, the third derivative matrix $(M^T)^3$ is calculated, yielding the critical simplification $f'''(z) = 15a_3$ for any $z$.

\textbf{\textcolor{CaseOrange}{Step 3.}} Position in Paper: This leverages the core matrix relation for derivatives, Formula (2.4). 

\textbf{\textcolor{CaseOrange}{Step 4.}} Formulate System Rows from Initial Conditions. The conditions at $z=0$ provide two linear constraints on the coefficients:

$f(0) = a_0 - 0.5a_2 = 0 \implies a_2 = 2a_0$

$f'(0) = a_1 - 1.5a_3 = 0 \implies a_1 = 1.5a_3$

\textbf{\textcolor{CaseOrange}{Step 5.}} Position in Paper: This step converts the initial conditions into a matrix form, as described by the process leading to Formula (2.10).

\textbf{\textcolor{CaseOrange}{Step 6.}} Formulate System Rows from Collocation Points. The differential equation $f'''(z) - 3f'(z) + \beta f(z) = 0$ is evaluated at $z=1$ and $z=-1$, yielding two equations:

At $z=1$: $\beta a_0 + (\beta-3)a_1 + (\beta-9)a_2 + (\beta-3)a_3 = 0$

At $z=-1$: $\beta a_0 - (\beta+3)a_1 + (\beta+9)a_2 - (\beta+3)a_3 = 0$

\textbf{\textcolor{CaseOrange}{Step 7.}} Position in Paper: This applies the collocation method, transforming the differential equation into an algebraic system at specific points, as outlined in Formulas (2.7) through (2.9).

\textbf{\textcolor{CaseOrange}{Step 8.}} Reduce the System and Solve the Determinant Condition. Substitute the relations $a_2 = 2a_0$ and $a_1 = 1.5a_3$ from Step 2 into the two equations from Step 3. This reduces the $4\times4$ system to a $2\times2$ homogeneous system for variables $a_0$ and $a_3$.

\[
\left\{
\begin{aligned}
(3\beta - 18)a_0 + (2.5\beta - 7.5)a_3 = 0 \\
(3\beta + 18)a_0 - (2.5\beta + 7.5)a_3 = 0
\end{aligned}
\right.
\]

\textbf{\textcolor{CaseOrange}{Step 9.}} For a non-trivial solution to exist, the determinant of this $2\times2$ coefficient matrix must be zero:

\[
\det\left(
\begin{bmatrix}
3\beta - 18 & 2.5\beta - 7.5 \\
3\beta + 18 & -(2.5\beta + 7.5)
\end{bmatrix}
\right) = 0
\]

\textbf{\textcolor{CaseOrange}{Step 10.}} Solving this determinant equation yields $2\beta^2 - 36 = 0$, which simplifies to $\beta^2 = 18$.

\textbf{\textcolor{CaseOrange}{Step 11.}} Position in Paper: The requirement for a non-trivial solution ($\det(\widetilde{W})=0$) is the fundamental principle for determining coefficients, as discussed following Formula (2.12).

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

18

\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Neuroscience,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Motor imagery tasks in brain–computer interfaces (BCIs) are usually designed around activity in the sensorimotor cortex, since this region is central to planning and controlling movement. However, accurate decoding of motor imagery does not rely solely on motor areas. Many studies have shown that other brain regions also become active during imagery tasks, especially when visual feedback or focused attention is involved. These additional signals can provide valuable features for classifiers, improving decoding accuracy. Understanding which non-motor regions contribute is important for both electrode placement and interpretation of neural mechanisms in BCI research.

Which one cerebral lobe, besides sensorimotor cortex, often contributes significantly to motor imagery decoding? Please do not use abbreviations in your answer.

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} Review the major cerebral lobes: The frontal lobe has motor-related areas; the parietal lobe supports attention and sensory integration; the occipital lobe handles visual processing and feedback, which can aid motor imagery decoding; the temporal lobe mainly handles auditory and memory functions.

\textbf{\textcolor{CaseOrange}{Step 2.}} Analyse brain regions become active during motor imagery tasks: Besides frontal lobe which directly mediates motor, check for other function required in motor imagery tasks. Visual feedback can significantly improves decoding accuracy.

\textbf{\textcolor{CaseOrange}{Step 3.}} Conlusion: The occipital lobe is the location of the primary visual cortex, whose core function is to receive and process visual information—visual feedback in motor imagery tasks.


\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

Occipital lobe

\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Deep Research in Physics,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

In iron-based superconductors, the tight-binding model describes the low-energy electronic structure. Using the five-orbital model Hamiltonian 
\[
H = \sum_{\mathbf{k},\sigma} \sum_{i,j} t_{ij}(\mathbf{k}) c_{i\sigma}^\dagger(\mathbf{k}) c_{j\sigma}(\mathbf{k}),
\]
where \( t_{ij}(\mathbf{k}) \) includes nearest-neighbor (NN) and next-nearest-neighbor (NNN) hopping integrals. For LaFeAsO, the NN hopping between \( d_{z^2} \) orbitals is \( t_1 = -0.3\,\text{eV} \), and the NNN hopping is \( t_2 = 0.2\,\text{eV} \). Calculate:
\begin{enumerate}
    \item The effective hopping amplitude \( t_{\text{eff}} \) at the \( \Gamma \) point (\( \mathbf{k} = (0,0) \)) for \( d_{z^2} \) orbitals.
    \item The superconducting gap \( \Delta(\mathbf{k}) \) at \( \mathbf{k} = (\pi, 0) \) using the gap equation
    \[
    \Delta(\mathbf{k}) = \sum_{\mathbf{k}'} V(\mathbf{k} - \mathbf{k}') \frac{\tanh\left( \frac{E(\mathbf{k}')}{2k_B T} \right)}{2E(\mathbf{k}')} \Delta(\mathbf{k}'),
    \]
    assuming \( V(\mathbf{q}) = 0.5\,\text{eV} \) and \( T = 4.2\,\text{K} \).
    \item The critical temperature \( T_c \) if the gap magnitude \( \Delta_0 \) is \( 5\,\text{meV} \), using the BCS relation \( \Delta_0 = 1.76 k_B T_c \). Numerical value with 2 decimal place.
\end{enumerate}

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}} From "Iron-based superconductors: Current status of materials and pairing mechanism"

\textbf{\textcolor{CaseOrange}{Step 2.}} Extract NN hopping \( t_1 = -0.3\,\text{eV} \) and NNN hopping \( t_2 = 0.2\,\text{eV} \) for \( d_{zz} \) orbitals from "Band structure and modeling".

\textbf{\textcolor{CaseOrange}{Step 3.}} At \( \Gamma \) point (\( \mathbf{k} = (0,0) \)), the dispersion is
\[
E(\mathbf{k}) = -2t_1(\cos 0 + \cos 0) - 4t_2(\cos 0 + \cos 0) = -2(-0.3)(2) - 4(0.2)(2) = 1.2 - 1.6 = -0.4\,\text{eV}.
\]
The effective hopping amplitude \( t_{\text{eff}} \) is derived from the coefficient of \( \cos k_x + \cos k_y \), giving \( t_{\text{eff}} = -0.3 + 0.2 = -0.1\,\text{eV} \) (Section 3.1).

\textbf{\textcolor{CaseOrange}{Step 4.}} For \( \Delta(\mathbf{k}) \) at \( \mathbf{k} = (\pi, 0) \), use
\[
E(\mathbf{k}') = \sqrt{ \xi^2(\mathbf{k}') + \Delta^2(\mathbf{k}') }.
\]
Assume \( \xi(\mathbf{k}') = -2t_1 \cos k_x - 2t_1 \cos k_y \) and \( \Delta(\mathbf{k}') = \Delta_0 \).
At \( T = 4.2\,\text{K} \), \( \tanh\left( \frac{E}{2 k_B T} \right) \approx 1 \) for low-energy states. Substituting \( V(\mathbf{q}) = 0.5\,\text{eV} \), the gap equation simplifies to
\[
\Delta(\pi, 0) = V \cdot \frac{1}{2E} \Delta_0.
\]
With \( E = \sqrt{(-0.3)^2 + (0.005)^2} \approx 0.3\,\text{eV} \),
\[
\Delta(\pi, 0) = 0.5 \cdot \frac{1}{2 \times 0.3} \cdot 0.005 = 0.04\,\text{eV}
\]
(Section 4.2).

\textbf{\textcolor{CaseOrange}{Step 5.}} For \( T_c \), use the BCS relation \( \Delta_0 = 1.76\,k_B T_c \). Rearranging gives \( T_c = \frac{\Delta_0}{1.76\,k_B} \).
Substituting \( \Delta_0 = 5\,\text{meV} = 0.005\,\text{eV} \) and \( k_B = 8.617 \times 10^{-5}\,\text{eV/K} \),
\[
T_c = \frac{0.005}{1.76 \times 8.617 \times 10^{-5}} \approx 33.14\,\text{K}
\]
(Section 5.1).

\textbf{\textcolor{CaseOrange}{Step 6.}} Verify consistency with experimental \( T_c = 26\,\text{K} \) for LaFeAsO\(_{1-x}\)F\(_x\) (Section 2.1). The calculated \( T_c = 33.14\,\text{K} \) aligns with theoretical predictions for optimized doping (Section 2.3).

\textbf{\textcolor{CaseOrange}{Step 7.}} Cross-reference all parameters with "Materials: bulk" section (Page 3), confirming \( t_1 \), \( t_2 \), and \( V \) values.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

-0.1, 0.04, 33.14

\end{tcolorbox}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Idea Generation}

% \begin{tcolorbox}[
%     breakable,
%     title=Example of Idea generation,
%     colback=LighterGray,
%     colframe=DeepPurple,
%     colbacktitle=DeepPurple,
%     coltitle=White,
% ]
% \textbf{\emph{\textcolor{DeepPurple}{Question}}}

% \textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}
% \end{tcolorbox}

% 天文 Astronomy
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Astronomy},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Palomar Transient Factory (PTF): Predecessor to ZTF using the same telescope but a smaller camera, providing moderate survey speed and limited temporal coverage. PTF pioneered time-domain transient discovery but suffered from longer readout times and lower areal coverage. \\
• Sloan Digital Sky Survey (SDSS): Large-area multi-band imaging survey with significant contributions to extragalactic and stellar astrophysics, but with relatively limited cadence and not optimized for rapid transient detection. \\
• Pan-STARRS: Wide-field survey with high sensitivity, flexible cadence, and a broad range of science outputs. While highly productive, it does not reach ZTF's survey speed or alert distribution rate. \\
• ATLAS, ASAS-SN, and CRTS: Dedicated time-domain surveys with wide fields and rapid cadences, enabling rapid transient detection. However, these typically have smaller apertures and shallower depth compared to ZTF, restricting discovery of fainter phenomena. \\
• Dark Energy Survey (DES): Deep survey with the Dark Energy Camera, high image quality, and excellent photometric calibration. DES is less optimized for high-cadence wide-area transient monitoring due to smaller field of view and longer exposure times. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Maximizing volumetric survey speed—combining wide field, fast readout, and high sensitivity—to enable rapid, repeated coverage of large sky areas for transient discovery. \\
• Minimizing image artifacts and systematic errors to ensure precision in photometric and astrometric measurements across a large, curved focal plane. \\
• Providing prompt, reliable, and information-rich alerts for real-time identification and classification of astrophysical transients and moving objects. \\
• Efficiently handling massive data volumes and complex processing requirements to deliver near-real-time data products and alerts to the community. \\
• Maintaining high photometric and astrometric accuracy in the presence of instrumental, atmospheric, and sky-background variability. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Previous surveys were limited by smaller camera fields of view, slower readout and overheads, less
optimized scheduling, and less sophisticated data pipelines, resulting in lower time-domain
sampling, slower alert generation, and reduced ability to detect fast or faint transients across
wide areas.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The accelerating demand for high-cadence, wide-area sky monitoring in time-domain astronomy—spanning
supernovae, variable stars, NEOs, and multi-messenger counterparts—necessitates a system that
surpasses existing surveys in speed, coverage, and data accessibility. Addressing limitations in
cadence, alert timeliness, and survey efficiency is critical for enabling rapid discovery and
follow-up of astrophysical transients, as well as for preparing the community for next-generation
surveys like LSST.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

Develop and implement an integrated, high-speed, wide-field optical time-domain survey system
capable of delivering near-real-time discovery, classification, and alerting of transient, variable,
and moving objects, while providing high-quality calibrated data products and supporting a broad
range of time-domain astrophysics.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• PTF: Utilized a CCD camera on the Palomar 48-inch telescope for transient discovery with moderate areal coverage and cadence. Enabled systematic transient searches but constrained by small field of view and longer readout times. \\
• SDSS and Pan-STARRS: Both provided large-scale sky mapping and multi-filter photometry, but with relatively slow cadence and areal throughput unsuitable for rapid time-domain science. \\
• ATLAS and ASAS-SN: Optimized for rapid all-sky cadence and automated transient detection but limited in depth due to smaller apertures and less sensitive instrumentation. Alert and data distribution less feature-rich than ZTF's planned system. \\
• DES: Leveraged a large, high-quality camera for deep imaging and science, but with a narrower field and less frequent temporal sampling, making it suboptimal for high-cadence transient monitoring. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

ZTF pioneers a new era of high-speed, wide-field time-domain astronomy by equipping the Palomar
48-inch Schmidt telescope with a custom-built CCD mosaic camera, optimized scheduling, and a robust
data system. It delivers an order of magnitude survey speed improvement, rapid image processing, and
a real-time, feature-rich alert stream, positioning ZTF as both a state-of-the-art survey and a
testbed for LSST-scale time-domain operations.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Design and assemble a large-format CCD mosaic camera with minimal chip gaps and high quantum efficiency, optimized for the Palomar Schmidt focal plane. \\
• 2: Upgrade telescope mechanics, optics, and control software for fast slewing, low overhead, and image quality preservation over the expanded field. \\
• 3: Develop and deploy a robotic observing system and integer-linear-programming–based survey scheduler to maximize nightly volumetric coverage and cadence. \\
• 4: Implement on-site, lossless data compression and high-speed transfer of image data to the IPAC processing center. \\
• 5: Process raw images through automated calibration pipelines: bias subtraction, flat-field correction, astrometric and photometric calibration, and artifact masking. \\
• 6: Generate coadded reference images using quality-filtered, multi-epoch stacks for each field, filter, and CCD quadrant. \\
• 7: Perform image differencing using the ZOGY algorithm to detect transient and moving sources at high significance. \\
• 8: Extract candidate sources, compute pixel-based features, and apply machine learning (Real-Bogus) for initial classification. \\
• 9: Package candidates with contextual data (cross-matches, light curves, images) into Avro alert packets and distribute in real time via Kafka queues. \\
• 10: Archive all processed data products, catalogs, and alerts at IRSA and provide public access according to survey data release policies. \\
• 11: Publish light curves from direct imaging for variable and periodic sources, and implement dedicated pipelines for moving object detection and orbit determination. \\
• 12: Conduct on-sky performance validation and commission the system with early science and rapid feedback loops for further optimization. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 7-8 \\
• 8-9 \\
• 5-10 \\
• 7-11 \\
• 1-12 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

The primary dataset comprises optical images acquired with the Palomar 48-inch Schmidt telescope
using a 16-CCD, 6144x6160-pixel mosaic camera, covering 47.7 deg² per exposure in g, r, and i bands.
Each exposure delivers science and auxiliary (guide/focus) CCD data, with per-night cadences ranging
from minutes to once every three days. The system produces processed images, photometry catalogs,
coadded references, image subtractions, light curves, and alert packets, all archived at IRSA. Early
data include thousands of exposures, millions of cataloged sources, and time-series data for
variable and transient objects.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Volumetric Survey Speed: Spatial volume probed per unit time for transient detectability at a given absolute magnitude; incorporates field of view, sensitivity, and overheads. \\
• Image Quality: Median delivered PSF FWHM in arcseconds (e.g., 2.0" in r band). \\
• Limiting Magnitude: Median five-sigma detection limit in g, r, i bands for standard exposure durations. \\
• Photometric Repeatability: Standard deviation of calibrated flux for non-varying sources (e.g., <10 mmag for bright stars). \\
• Astrometric Accuracy: Median positional residuals relative to reference catalog (e.g., Gaia). \\
• Alert Latency: Time from image acquisition to alert distribution (target: \~{}4 minutes). \\
• Transient Yield: Number of confirmed supernovae and other transient discoveries per unit time. \\
• Moving Object Detection: Number and recovery rate of Near-Earth Asteroids and other small bodies identified and reported to the MPC. \\
• Data Throughput: Sustained image and alert processing rates under full survey cadence. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

The ZTF system achieves a >10× improvement in survey speed over PTF, routinely reaching 20.6–20.8
mag (r,g bands, 30s, 5$\sigma$) with 2.0–2.1" image quality and <4-minute alert latency. Early operations
yielded 38 spectroscopically classified supernovae (15 unique to ZTF), discovery of new Near-Earth
Asteroids, and high-fidelity variable star and asteroid light curves. ZTF anticipates streaming
\~{}1 million alerts per night and delivering public data releases, thereby providing an essential
precursor to LSST-scale time-domain surveys and enabling rapid, comprehensive follow-up of
transients and solar system discoveries.

\end{tcolorbox}



% 化学Chemistry
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Chemistry},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Gomez-Bombarelli et al. (2016): Proposed a VAE that generates SMILES strings character by character. The model learns a continuous latent space but frequently decodes to invalid SMILES, limiting the generation of chemically valid molecules. \\
• Kusner et al. (2017): Introduced Grammar VAE (GVAE), extending SMILES-based VAE by integrating syntactic constraints derived from a context-free grammar, improving validity but still limited by the inability of grammar to fully encode chemical rules. \\
• Dai et al. (2018): Syntax-directed VAE (SDVAE) incorporates both syntactic and semantic constraints using attribute grammars, yielding further validity gains, though chemical correctness is not entirely guaranteed. \\
• Simonovsky \& Komodakis (2018): GraphVAE generates molecular graphs via adjacency matrices and atom label prediction. While it addresses the linearization problem of SMILES, validity and scalability for larger and more complex molecules remain challenging. \\
• Li et al. (2018): Atom-by-atom graph generation via LSTM. This approach can model arbitrary graphs but often passes through chemically invalid intermediate states, resulting in incomplete validity guarantees and inefficiencies. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Direct generation of molecular graphs from continuous latent representations is challenging due to the combinatorial nature of graph structures and strict chemical validity constraints. \\
• SMILES-based generative models struggle to enforce chemical validity and do not offer smooth latent spaces for molecular similarity. \\
• Atom-by-atom or edge-by-edge graph generation approaches often produce invalid intermediate structures, leading to low efficiency and limited chemical feasibility. \\
• Capturing both coarse-grained (substructure) and fine-grained (atomic connectivity) molecular features in a unified generative framework. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Existing approaches either operate on linearizations (e.g., SMILES), lacking direct correspondence
to molecular structure and chemical validity, or generate graphs atom by atom, frequently passing
through invalid intermediates. Even grammar- and syntax-driven models cannot ensure full chemical
correctness or smoothness in the latent space, limiting their utility for property-driven molecular
design.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

Automating molecular design demands generative models that can create chemically valid, novel, and
property-optimized molecules. Existing string- and atom-based methods fail to guarantee validity or
exploit molecular substructure regularities. Addressing these gaps is critical for accelerating drug
discovery and enabling efficient, reliable inverse molecular design.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To develop a generative model that directly produces chemically valid molecular graphs from
continuous latent representations, supporting both unconstrained generation and property-driven
molecular optimization.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• CVAE (Gomez-Bombarelli et al., 2016): Learns a continuous latent space for SMILES string generation. Achieves smooth interpolations but poor validity due to unconstrained syntax. \\
• GVAE (Kusner et al., 2017): Imposes syntactic constraints via grammar-based decoding, improving string validity but not fully encoding chemical rules. \\
• SD-VAE (Dai et al., 2018): Incorporates additional semantic constraints with attribute grammars, further improving validity but still limited by the expressivity of the grammar in capturing chemical feasibility. \\
• GraphVAE (Simonovsky \& Komodakis, 2018): Directly generates molecular graphs via adjacency matrices. Avoids string limitations but faces scalability and validity issues for larger molecules. \\
• Atom-by-Atom LSTM (Li et al., 2018): Autoregressive graph generation at the atomic level. Capable of arbitrary graph synthesis but inefficient due to invalid intermediate structures. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

The core idea is to represent molecules as junction trees of valid chemical substructures, enabling
a two-stage variational autoencoder: first generating a tree-structured scaffold of subgraphs, then
assembling these into a molecular graph using message passing. This approach maintains chemical
validity throughout generation, leveraging coarse-to-fine modeling for efficient, valid, and
property-driven molecular graph synthesis.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Apply tree decomposition to each molecular graph to construct its junction tree of valid substructures (clusters). \\
• 2: Encode the molecular graph using a message passing neural network to obtain a graph latent representation. \\
• 3: Encode the junction tree using a tree message passing neural network to obtain a tree latent representation. \\
• 4: Concatenate tree and graph embeddings to form the full latent representation. \\
• 5: Decode the latent representation by first generating the junction tree in a top-down, sequential fashion via a tree decoder with feasibility checks and teacher forcing during training. \\
• 6: Assemble the molecular graph from the predicted junction tree by sequentially merging clusters using a graph decoder and scoring candidate subgraph combinations. \\
• 7: For stereochemistry, enumerate possible isomers of the generated graph and select the best via neural scoring. \\
• 8: For property-driven optimization, jointly train a property predictor with JT-VAE and perform gradient-based or Bayesian optimization in the latent space. \\
• 9: Evaluate reconstruction, validity, property optimization, and neighborhood smoothness using standardized benchmarks. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 1-3 \\
• 2-4 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 4-8 \\
• 5-9 \\
• 6-9 \\
• 7-9 \\
• 8-9 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

The primary dataset is the ZINC molecular database (Kusner et al., 2017 split), containing
approximately 250,000 drug-like molecules. Molecules are represented as graphs with atom and bond
features, and decomposed into cluster vocabularies of 780 unique substructures (including rings,
bonds, and atoms). The dataset is utilized for training, validation, and testing of molecular
generation and optimization.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Reconstruction Accuracy: Percentage of input molecules correctly reconstructed from their latent representations (Monte Carlo estimate over multiple samplings). \\
• Validity: Proportion of generated molecules that are chemically valid, as checked by cheminformatics tools (RDKit). \\
• Novelty: Fraction of generated molecules not present in the training set, indicating generative diversity. \\
• Optimization Improvement: Average increase in target property (e.g., penalized logP) achieved via optimization, often reported with similarity constraints. \\
• Similarity: Tanimoto similarity between original and optimized molecules, measured via Morgan fingerprints. \\
• Predictive Performance: Log-likelihood and root mean squared error (RMSE) of property prediction models (e.g., sparse Gaussian process) trained on latent encodings. \\
• Success Rate: Fraction of optimization trials where valid, property-improved molecules satisfying similarity constraints are found. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

JT-VAE achieves 100\% validity in generated molecules, surpassing all prior baselines (e.g., SD-VAE:
43.5\%, Atom-by-Atom LSTM: 89.2\%), with 76.7\% reconstruction accuracy. For property optimization,
it discovers molecules with target scores up to 5.3 (vs. 4.04 from SD-VAE), and achieves over 80\%
success in constrained optimization with >0.4 similarity, demonstrating both validity and smoothness
in latent space. The model enables scalable, property-driven molecular design with significant
accuracy and efficiency gains.

\end{tcolorbox}



% 地球 Earth
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Earth},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Viljanen et al. (2018): Compared approaches using photogrammetric canopy height models, images, and vegetation indices from UAVs in estimating grass sward biomass, reporting strong results but site-specific dependencies. \\
• Michez et al. (2019): Mapped and monitored pasture biomass and grazing using UAV-based sward height and reflectance data, demonstrating promise but limited by environmental variability and DTM availability. \\
• Lussem et al. (2018): Evaluated RGB-based vegetation indices from UAV imagery for forage yield estimation, predominantly using NDVI and linear regression, revealing moderate-to-strong correlations but suffering from index saturation and reduced transferability. \\
• Insua et al. (2019): Coupled UAV imagery with crop simulation for spatial-temporal pasture growth estimation, but introduced complexity by integrating simulation models and site-specific variables. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Accurate, spatially comprehensive, and temporally frequent estimation of forage biomass and vegetation cover in grasslands remains difficult due to the heterogeneity of growth stages, management regimes, and environmental variation. \\
• Conventional field-based surveys are labor-intensive, spatially incomplete, and lack temporal resolution needed for dynamic grassland management. \\
• Remote sensing solutions, particularly with satellite or manned aerial imagery, are limited by insufficient spatial and temporal resolution for plot-level or intra-seasonal monitoring. \\
• Existing remote sensing models often do not generalize well due to site-specific calibrations, limited temporal coverage, and a reliance on linear relationships between indices and biophysical parameters. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Current approaches to grassland biomass estimation using UAV or remote sensing data often suffer
from limited operational scalability due to complex processing pipelines, dependence on unavailable
ancillary environmental data (e.g., meteorology, soil), suboptimal selection or saturation of
vegetation indices, and inadequate validation across diverse conditions, compromising their
applicability and generalizability in temperate grassland systems.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The need for spatially exhaustive, temporally responsive, and operationally practical tools for
grassland monitoring is acute given the ecological and agricultural importance of these systems and
their broad degradation. UAV-based multispectral imaging presents a promising avenue, but systematic
comparison of diverse processing methods over an entire growing season and under temperate
conditions is lacking, hindering adoption in precision pasture management.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To develop, test, and compare three UAV-based multispectral imaging approaches—volumetric modeling
via structure from motion, GNDVI-based regression, and GNDVI-based classification—for estimating
forage biomass and vegetation cover in temperate grasslands across a full growing season.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• Spectral Index Regression (NDVI, etc.): Relies on linear regression between vegetation indices (primarily NDVI) and biomass; easy to implement but limited by index saturation and oversimplification of non-linear relationships. Often requires site-specific calibration. \\
• Height/Volumetric Models from Photogrammetry: Uses UAV structure from motion photogrammetry to estimate canopy or sward height as a proxy for biomass, offering strong correlation where precise DTMs are available but sensitive to terrain inaccuracies and not robust at low vegetation density. \\
• Multi-Source and Simulation-Based Models: Integrate spectral, structural, and ancillary data (e.g., crop models or management records) for enhanced accuracy but increase methodological complexity and reduce operational ease. \\
• Classification Approaches: Rarely applied to grassland biomass; when used, classification of vegetation cover is often qualitative and seldom linked directly to continuous biomass estimation. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

This study systematically compares three UAV-based approaches—volumetric modeling via structure from
motion, GNDVI-based regression, and GNDVI-based classification—over an entire season in temperate
grasslands, demonstrating that these methods are complementary, operationally feasible, and
generalizable for spatially detailed forage biomass and cover estimation, each suiting different
management needs and data constraints.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Planning and executing UAV flights to acquire multispectral and visible imagery with consistent overlap and illumination across 14 dates. \\
• 2: Collecting ground-truth biomass samples and recording plot management details (grazing, clipping schedules). \\
• 3: Processing imagery to produce orthomosaics and DSMs using aerial triangulation, GCPs, and radiometric correction. \\
• 4: Generating high-precision DTM for control unit using GNSS data; calculation of volumetric biomass (DSM-DTM). \\
• 5: Calculating multiple vegetation indices (including GNDVI) from orthomosaics and evaluating their correlation with biomass samples. \\
• 6: Developing a volumetric-based linear regression biomass model (control plots only). \\
• 7: Selecting optimal vegetation index (GNDVI) and training non-linear regression models for fresh and dry biomass using 49 training samples. \\
• 8: Validating regression models using 50 independent field samples; calculating performance statistics. \\
• 9: Extracting GNDVI values from 248 polygons, applying cluster and discriminant analysis to classify vegetation cover into four classes. \\
• 10: Comparing spatial and temporal patterns among the three approaches using visual and statistical analyses. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 1-3 \\
• 3-4 \\
• 4-6 \\
• 3-5 \\
• 5-7 \\
• 7-8 \\
• 5-9 \\
• 6-8 \\
• 7-8 \\
• 9-10 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

Imagery and field data were collected in a 14-ha field in Sherbrooke, Quebec, containing 30 pasture
plots (25x50 m), 5 bare soil plots (25x50 m), and 6 control plots (5x5 m). Over the 2017 growing
season, 14 UAV flights (DJI Inspire 1 Pro with Parrot Sequoia multispectral and visible sensors)
were conducted, yielding high-resolution orthomosaics and DSMs. Field biomass measurements were
obtained from 99 quadrats (0.25 m² each) for regression modeling and 248 polygons (3.5x3.5 m) for
classification, sampled across management regimes and growth stages.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Coefficient of Determination (R2): Measures the proportion of variance in measured biomass explained by model predictions. Evaluated for both fresh and dry biomass regression models. \\
• Root Mean Square Error (RMSE): Quantifies the average magnitude of prediction error between measured and estimated biomass. \\
• Normalized RMSE (NRMSE): RMSE divided by the mean of measured values, expressed as a percentage to facilitate comparison across datasets. \\
• Central Tendency Error: Assesses systematic bias between predicted and observed values. \\
• Regression Error: Quantifies deviation of fitted regression from the 1:1 line. \\
• Concordance Analysis: Statistical comparison of predicted vs. observed values for regression model validation. \\
• Visual Qualitative Assessment: Comparison of predicted spatial patterns with RGB imagery and known management (e.g., growth duration). 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

The volumetric model achieved R² = 0.93 (fresh) and 0.94 (dry), RMSE of 0.072 kg/m² (fresh) and
0.013 kg/m² (dry); GNDVI regression yielded R² = 0.80 (fresh) and 0.66 (dry) for training, with
validation R² = 0.63 (fresh) and 0.50 (dry), NRMSE of 36\% (fresh) and 38\% (dry). The GNDVI
classification robustly distinguished four vegetation cover classes. Combined, these methods enable
fine-scale, season-long monitoring of pasture condition, with operational models supporting >90\%
explanation of biomass variance for suitable conditions, and practical, generalizable classification
for management applications.

\end{tcolorbox}



% 能源 Energy
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Energy},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Sfetsos2000: Applied various forecasting techniques (statistical, time-series analysis) to mean hourly wind speed, finding that model performance varies with data characteristics; however, results demonstrate instability across sites and fail to leverage combined model strengths. \\
• Kelouwani2004: Utilized nonlinear model identification with neural networks for wind turbine output prediction, yielding improved accuracy for specific datasets, but with limited robustness to operational variability. \\
• Negnevitsky2007: Proposed a hybrid intelligent system for short-term wind power forecasting, integrating multiple AI approaches; achieved improved performance over single models but lacked dynamic adaptation to wind speed distribution features. \\
• Shi2010: Combined wavelet transforms and support vector machines for short-term wind power prediction, enhancing performance for non-stationary series, yet exhibiting sensitivity to model parameterization and failing to generalize across varying wind speed segments. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Accurately forecasting very-short term (e.g., 15-minute-ahead) wind power output amidst inherent wind speed volatility and non-stationarity. \\
• Capturing the nonlinear and regime-dependent relationship between wind speed distributions and wind farm power generation. \\
• Integrating multiple predictive models in a manner that adaptively leverages their complementary strengths across varying meteorological conditions. \\
• Minimizing computational burden while improving real-time forecasting reliability for grid operation and reserve planning. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Existing single-model forecasting approaches lack generalizability due to dataset-specific
performance and inability to adapt to wind speed regime changes. Prior hybrid models fail to exploit
wind speed distribution features for dynamic weight allocation and commonly require extensive
retraining, resulting in suboptimal accuracy and increased computational overhead.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The volatility and unpredictability of wind power pose significant challenges for power system
operation, particularly at high penetration levels. Improved very-short term forecasting is critical
for grid reliability, reserve allocation, and economic dispatch. Recognizing that no single model
performs optimally across all wind regimes, there is a compelling need for a hybrid approach that
dynamically adapts to wind speed distribution features, maximizing forecasting accuracy and
operational utility.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To develop a dynamic hybrid very-short term wind power forecasting model that integrates grey
relational analysis with wind speed distribution features, enabling adaptive model weighting and
superior forecasting accuracy over individual models for 15-minute-ahead wind power output.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• Persistence/MLR/ARMA: Statistical models, such as persistence, multiple linear regression, and ARMA, leverage historical data for short-term forecasting, offering simplicity but inadequate handling of nonlinearities and changing wind regimes. \\
• ANN/SVM Approaches: Artificial neural networks and support vector machines have been applied for improved short-term prediction by capturing complex patterns, but their performance is sensitive to data characteristics, and single models often fail to generalize well. \\
• Prior Hybrid Models: Some studies combine multiple models via fixed or learned weights (e.g., neural network-based combination), achieving moderate improvements but lacking integration with wind speed regime information, and often requiring heavy retraining for each new scenario. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

The authors introduce a hybrid forecasting framework that fuses LSSVM and RBFNN models through grey
relational analysis, with model weights adaptively tuned by wind speed distribution features
segmented via Weibull analysis. By constructing a dynamic weight database indexed by wind speed
regimes, the method achieves improved accuracy and reduced retraining effort for 15-minute-ahead
wind power prediction.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Preprocess data (handle missing samples, normalization, extract input features: prior wind speeds, directions, power output). \\
• 2: Train independent LSSVM and RBFNN models on input features for 15-minute-ahead wind power prediction. \\
• 3: Apply equalization to forecasting result sequences and actual measurements to obtain normalized series. \\
• 4: Calculate grey relational degrees between each model's output and actual measurements for each time window. \\
• 5: Fit wind speed data for each month to the Weibull distribution; segment wind speed into regimes according to frequency analysis. \\
• 6: Compute model weights (correlations) within each wind speed regime and store in a monthly weight database. \\
• 7: For new forecasts, use NWP wind speed prediction to identify wind speed regime and retrieve corresponding model weights. \\
• 8: Combine LSSVM and RBFNN outputs using dynamic weights for final forecast output. \\
• 9: Evaluate forecasting performance using MAPE and RMSE against actual measured data. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 1-5 \\
• 5-6 \\
• 6-7 \\
• 7-8 \\
• 8-9 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

Historical SCADA data from a Chinese wind farm spanning 01/01/2010 to 12/31/2010 (excluding months
with missing data), comprising 15-minute resolution records of wind speed (previous 15, 30, 45 min),
wind direction (cosine and sine), and wind power output. The dataset includes over 30,000 samples,
with wind speed segmented monthly and fitted to Weibull distributions for regime analysis.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• MAPE: Mean Absolute Percentage Error; quantifies average absolute error as a percentage of actual wind farm rated capacity. \\
• RMSE: Root Mean Square Error; quantifies the standard deviation of the prediction errors, normalized by wind farm capacity. \\
• Visual Comparison: Graphical overlays of forecasted vs. actual power output for selected periods to assess tracking and volatility handling. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

The hybrid model achieves a MAPE of 2.37\% and RMSE of 3.79\%, outperforming standalone LSSVM and
RBFNN models as well as simple averaging. The method delivers improved accuracy, especially during
low and fluctuating power output regimes, and reduces retraining overhead through the dynamic weight
database. The approach demonstrates robustness and scalability for operational very-short term wind
power forecasting.

\end{tcolorbox}



% 信息 Information
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Information},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• InternVL2.5: Adopted a multi-stage pipeline with language-only pre-training, MLP warmup for multimodal alignment, and instruction tuning. Demonstrated strong open-source multimodal performance but faced training complexity and limited cross-modal parameter optimization. \\
• Qwen2.5-VL: Uses a staged adaptation of text-only LLMs into MLLMs, integrating visual adapters and fine-tuning. Achieves strong performance on vision-language tasks but still requires complex alignment processes and suffers in long-context or multi-image scenarios. \\
• LLaVA-OneVision: Focuses on easy visual task transfer via visual instruction tuning. Excels at adaptation efficiency but underperforms on challenging multimodal reasoning or spatial tasks compared to larger unified models. \\
• Gemini 2.5 Pro: A proprietary closed-source MLLM employing advanced joint training and data curation, achieving state-of-the-art results. However, it lacks the transparency and reproducibility necessary for open research progress. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Integrating multimodal (vision, text, video) and linguistic capabilities in a single model without compromising either modality's performance. \\
• Overcoming the inefficiencies and alignment difficulties of post-hoc adaptation pipelines that start from text-only LLMs. \\
• Scaling multimodal large language models (MLLMs) to handle longer contexts, multi-image input, and complex real-world tasks. \\
• Balancing pure-language proficiency with robust multimodal reasoning and visual grounding. \\
• Efficiently utilizing heterogeneous and imbalanced multimodal data during pre-training and post-training. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Existing MLLMs rely on multi-stage adaptation pipelines, leading to suboptimal cross-modal parameter
interaction and persistent alignment or optimization bottlenecks. These approaches often freeze or
partially update parameters, limiting scalability, introducing computational overhead, and creating
a persistent gap in pure-language and multimodal competence.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The growing complexity and diversity of real-world multimodal data demand models capable of unified,
scalable, and robust multimodal reasoning, without the trade-offs and inefficiencies of post-hoc
adaptation. A native joint pre-training paradigm is needed to achieve seamless linguistic and
multimodal integration, better performance scalability, and open research reproducibility.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To develop a unified, open-source multimodal large language model that jointly acquires linguistic
and multimodal capabilities via native pre-training, establishes new state-of-the-art performance
across a spectrum of multimodal tasks, and narrows the gap to leading proprietary MLLMs.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• InternVL2.5: Applies separate language pre-training followed by multimodal alignment (MLP warmup, visual adapters), then instruction tuning. Good on general benchmarks, but complex, inflexible, and less efficient for scaling. \\
• Qwen2.5-VL: Uses visual adapters with staged fine-tuning. Strong visual-text integration, but depends on freezing strategies and additional modules. Moderate gains on long-context or diverse input. \\
• LLaVA-OneVision: Visual instruction tuning for rapid adaptation. Simplicity and transferability prioritized, but lacking in deep joint optimization for reasoning and multi-modal context. \\
• Gemini 2.5 Pro: Highly-curated, end-to-end joint pre-training but closed-source, with proprietary data curation and infrastructure. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

InternVL3 introduces native multimodal pre-training, where vision, language, and video data are
jointly leveraged in a single optimization stage. It integrates Variable Visual Position Encoding
for long-context support, advanced post-training (SFT, MPO), and test-time scaling, resulting in
scalable, efficient, and unified multimodal reasoning with open-source reproducibility.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Initialize ViT, LLM, and MLP modules with pre-trained weights; set up data pipelines for multimodal and text corpora. \\
• 2: Apply pixel unshuffle and prepare visual tokens for scalable image encoding. \\
• 3: Implement Variable Visual Position Encoding (V2PE) for visual tokens, with random delta sampling during training. \\
• 4: Jointly pre-train all model components using the multimodal autoregressive objective, sampling data at a 1:3 text-to-multimodal ratio. \\
• 5: Perform Supervised Fine-Tuning (SFT) with high-quality, diverse multimodal instructions, applying loss re-weighting and data packing. \\
• 6: Conduct Mixed Preference Optimization (MPO) using preference pairs and a composite loss (preference, quality, generation). \\
• 7: Integrate Best-of-N test-time scaling with VisualPRM as the critic to select optimal outputs. \\
• 8: Train with InternEVO for efficient large-scale distributed optimization, handling workload imbalances and maximizing resource utilization. \\
• 9: Perform comprehensive evaluation on a battery of multimodal and language benchmarks. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 7-8 \\
• 8-9 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

InternVL3 is trained on a hybrid corpus: (1) Multimodal data (150B tokens) comprising image-text
pairs, video-text, GUI, tool usage, 3D scene, document, OCR, chart, multi-image, and medical data,
sourced and extended from InternVL2.5 and new real-world collections; (2) Pure language data (50B
tokens) built from InternLM2.5, open-source corpora, and scientific/math datasets. SFT uses 21.7M
curated samples; MPO uses 300K preference pairs from MMPR v1.2.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• MMMU: Massive Multi-discipline Multimodal Understanding, measuring reasoning across disciplines (accuracy, \%). \\
• MathVista/MathVision/MathVerse: Mathematical reasoning (accuracy, \%). \\
• OCRBench/AI2D/ChartQA/DocVQA: Vision-text integration and document understanding (accuracy, \%, EM). \\
• MMBench/MMStar/MMVet/MME: Comprehensive multimodal capabilities (aggregate and per-task accuracy or score). \\
• HallusionBench/MMHal/CRPE/POPE: Multimodal hallucination resistance (score, \%). \\
• RefCOCO/+/g: Visual grounding (localization accuracy, \%). \\
• MVBench/Video-MME/MLVU: Video and temporal understanding (score, \%). \\
• ScreenSpot/ScreenSpot-V2: GUI grounding (accuracy, \%). \\
• VSI-Bench: Spatial reasoning (composite score, \%). \\
• Language Benchmarks: MMLU, CMMLU, C-Eval, GAOKAO, TriviaQA, NaturalQuestions, RACE, HellaSwag, GSM8K, MATH, HumanEval, MBPP (accuracy, pass@k, or other standard metrics). 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

InternVL3-78B achieves state-of-the-art open-source results, e.g., 72.2 on MMMU, 79.0 on MathVista,
91.4 on RefCOCOg, 90.9\% on GUI grounding, and 48.4 on VSI-Bench. It demonstrates robust scaling
across tasks, narrows the performance gap to commercial models (Gemini 2.5 Pro, GPT-4o), and
maintains strong language proficiency (80.5 overall on language benchmarks). All models and data
will be open-sourced to enable community-driven research.

\end{tcolorbox}



% 生命
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Life},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Senior et al. (2020): Introduced deep learning for predicting inter-residue distances, improving template-free protein structure prediction but still reliant on multiple post-processing stages and lacking atomic-level accuracy for novel folds. \\
• Yang et al. (2020): Employed deep neural networks to predict inter-residue orientations, integrating orientation constraints but with limited end-to-end learning and lower performance on long or complex proteins. \\
• AlQuraishi (2019): Proposed an end-to-end differentiable structure prediction model, directly outputting 3D coordinates; however, it exhibited lower accuracy than multi-stage pipelines and struggled without homologous templates. \\
• Marks et al. (2011); Jones et al. (2012): Used coevolutionary analysis of MSAs to infer residue contacts, achieving improvements in contact prediction but failing to achieve accurate atomic models, especially for proteins lacking deep MSAs or templates. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Achieving atomic-level accuracy in protein structure prediction directly from amino acid sequence, particularly in the absence of homologous structural templates. \\
• Integrating physical, geometric, and evolutionary information into a single, scalable, end-to-end deep learning model. \\
• Handling cases with shallow or sparse multiple sequence alignments (MSAs), which limits evolutionary signal. \\
• Providing robust structure prediction for large proteins and complex folds, including those with novel topologies. \\
• Quantifying per-residue prediction confidence to enable reliable downstream biological applications. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Contemporary approaches fall short of experimental accuracy, particularly on targets lacking
homologous templates or deep MSAs. Existing neural architectures often separate contact/distance
prediction from structure generation, use hand-crafted features, or rely on multi-stage heuristics,
resulting in limited scalability and suboptimal integration of physical and evolutionary
constraints. Poor performance persists in under-sampled sequence regions and multi-chain complexes.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

Structural biology is constrained by the slow pace and resource demands of experimental structure
determination, leaving the vast majority of protein sequences without 3D structural annotation.
Accurate, scalable, and generalizable computational prediction of protein structures—especially
without close templates—would transform bioinformatics, molecular biology, and drug discovery by
bridging the sequence-structure knowledge gap.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To develop a computational method that predicts the three-dimensional atomic structure of proteins
from their amino acid sequence with accuracy comparable to experimental techniques, even in the
absence of close structural homologues or deep sequence alignments.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• Physics-based simulation: Uses molecular dynamics or statistical approximations to model protein folding but is computationally intractable for large proteins and sensitive to approximations in physical modeling. \\
• Bioinformatics/homology modeling: Predicts structures via alignment to known protein templates and infers constraints from evolutionary sequence analysis; limited by template availability and reduced accuracy for novel or divergent proteins. \\
• Deep learning with intermediate prediction: Predicts inter-residue distances/orientations from MSAs using CNNs or attention networks, then reconstructs structures through downstream heuristics; accuracy suffers in end-to-end integration and novel folds. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

AlphaFold introduces an end-to-end deep learning architecture that jointly embeds MSAs and pairwise
residue features, iteratively refines 3D atomic structures through Evoformer and Invariant Point
Attention modules, integrates geometric and evolutionary constraints, leverages self-distillation
from unlabelled data, and produces accurate, scalable predictions with robust per-residue confidence
estimates.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Collect and preprocess protein sequence and structure data from PDB, UniRef90, BFD, Uniclust30, and MGnify. \\
• 2: Construct multiple sequence alignments (MSAs) and retrieve structural templates for each input sequence using HHBlits, jackhmmer, and HHSearch tools. \\
• 3: Initialize the neural network: encode MSA and pairwise features; build Evoformer trunk with interleaved attention and triangle update blocks. \\
• 4: Process MSA and pair features through stacked Evoformer blocks to enable information exchange and representation enhancement. \\
• 5: Feed processed representations to the structural module; iteratively refine per-residue 3D coordinates using invariant point attention and equivariant transformations. \\
• 6: Apply frame-aligned point error (FAPE) loss, distogram loss, BERT-style MSA masking loss, and auxiliary side-chain/violation losses for end-to-end supervised training. \\
• 7: Augment training with self-distillation: generate and filter high-confidence predictions on unlabelled sequences, then retrain with mixed supervised and distillation data. \\
• 8: During inference, perform ensemble predictions (if required), select best models by predicted confidence scores, and relax final structures with Amber force field. \\
• 9: Evaluate predictions using CASP14 targets and recent PDB structures, reporting backbone and all-atom metrics, and provide per-residue confidence (pLDDT) and TM-score estimates. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 7-8 \\
• 8-9 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

AlphaFold is trained on structures from the Protein Data Bank (PDB) (as of April 2018), comprising
tens of thousands of high-resolution experimental protein structures. Sequence information is
augmented using UniRef90, Big Fantastic Database (BFD, \~{}2.2B sequences clustered into \~{}66M
families), Uniclust30, and MGnify. For self-distillation, \~{}350,000 diverse sequence clusters from
Uniclust30 are used. Evaluation is conducted on the CASP14 dataset (87 domains) and recent non-
redundant PDB chains (n=10,795), filtered to remove overlap with training data.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• IDDT (Local Distance Difference Test): Superposition-free metric comparing local atomic distances in predicted vs. reference structure, applicable for all atoms (IDDT) or backbone C$\alpha$ atoms (IDDT-C$\alpha$). \\
• GDT (Global Distance Test): Measures fraction of residues within predefined distance thresholds; standard for CASP evaluations of domain accuracy. \\
• TM-score (Template Modeling score): Assesses global structural similarity by optimal superposition over entire protein chains, robust to domain packing and length differences. \\
• C$\alpha$ r.m.s.d.95: Root-mean-square deviation of C$\alpha$ atoms over the best-aligned 95\% of residues, reducing the impact of outliers/artifacts. \\
• pLDDT (Predicted Local Distance Difference Test): Confidence score per residue, predicting local structural accuracy. \\
• pTM (Predicted TM-score): Neural network–derived prediction of TM-score for a given model. \\
• Error intervals: 95\% confidence intervals on reported metrics via bootstrapping. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

AlphaFold achieves median backbone accuracy of 0.96 Å r.m.s.d.95 on CASP14 (95\% CI: 0.85–1.16 Å),
with all-atom accuracy at 1.5 Å (95\% CI: 1.2–1.6 Å), outperforming the next-best method by a margin
exceeding 1.8 Å. High accuracy generalizes to new, non-redundant PDB entries (median 1.46 Å). The
model provides robust per-residue confidence estimation (pLDDT, Pearson r>0.75 with true accuracy),
produces accurate side-chain conformations, and scales to proteins exceeding 2,000 residues. The
approach enables proteome-scale structure prediction with experimental-level precision for the
majority of targets without requiring close homologues.

\end{tcolorbox}







% 材料 Material
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Material},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Yaghi et al. (2008, Science): Pioneered high-throughput synthesis of zeolitic imidazolate frameworks (ZIFs) using 96-well plates, establishing the feasibility of automated, combinatorial materials discovery but with limited autonomy and narrow scope. \\
• Sumida et al. (2010, Chem. Sci.): Utilized automated robotic systems and multichannel reactors for precise control over MOF synthesis, improving reproducibility but not achieving closed-loop optimization. \\
• Cao et al. (2023, JACS, MOFormer): Introduced a self-supervised Transformer model for MOF property prediction, exhibiting improved accuracy and data efficiency, yet mainly focused on text-based molecular representations. \\
• Kang et al. (2023, Nat. Mach. Intell., MOFTransformer): Developed a multimodal Transformer for universal transfer learning in MOFs, integrating graph and grid embeddings, achieving high transferability but requiring extensive pretraining data. \\
• Park et al. (2024, Digital Discovery): Applied deep reinforcement learning with Transformers for inverse design of MOFs, enabling property-driven generative design but currently constrained by the diversity and validity of generated structures. \\
• Dagdelen et al. (2024, Nat. Commun.): Proposed LLM-NERRE for structured chemical information extraction, advancing literature mining but dependent on fine-tuning and sample efficiency. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• The vast chemical and structural diversity of MOFs renders exhaustive experimental exploration infeasible, creating a high-dimensional, combinatorial synthesis landscape. \\
• Traditional manual or even semi-automated high-throughput methodologies are bottlenecked by limited autonomy, data integration, and lack of feedback-driven optimization. \\
• Existing AI models, though powerful, struggle with generalizability and interpretability due to sparse, noisy, or unstandardized data and the complexity of structure-property relationships. \\
• Realizing fully autonomous, closed-loop self-driving laboratories (SDLs) for MOF discovery is impeded by hardware standardization issues, sample handling difficulties, and insufficient integration of intelligent decision-making. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Previous methodologies in MOF research either focused on isolated automation of experimental steps
or applied AI for isolated tasks (e.g., property prediction) without achieving seamless, closed-loop
integration. These approaches often lack robust feedback mechanisms, dynamic adaptation to new data,
and struggle to generalize across diverse MOF chemistries, limiting their utility for autonomous
discovery.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

MOFs' application potential in energy, environment, and drug delivery is hampered by slow, labor-
intensive discovery cycles and under-explored materials space. The combination of laboratory
automation with advanced AI—including Transformers and LLMs—offers the prospect of systematic,
iterative, and autonomous exploration, thereby addressing efficiency, reproducibility, and
innovation barriers in MOF science.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To comprehensively review and critically evaluate the convergence of artificial intelligence
(especially Transformer and LLM models) and laboratory automation technologies in accelerating the
discovery, synthesis, characterization, and optimization of metal-organic frameworks, with emphasis
on the progression toward self-driving laboratories.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• Traditional HTE: Employs combinatorial synthesis and characterization platforms, increasing throughput but requiring significant manual oversight and lacking intelligent optimization. \\
• Machine Learning (2012–present): Applies classical statistical learning (e.g., decision trees, SVMs) for property prediction and data analysis, limited by feature engineering and scalability. \\
• Deep Learning (2020–present): Utilizes neural networks for property prediction and structure optimization, improving accuracy but often acts as a black box and needs large labeled datasets. \\
• Transformers/LLMs (2023–present): Leverage self-attention for sequence and structural modeling, enabling multimodal integration and text-based knowledge mining, but require extensive training and face challenges in domain adaptation and resource consumption. \\
• Generative Models (VAEs, GANs, Diffusion): Enable de novo MOF structure generation, but often struggle with chemical validity, diversity, and property conditioning. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

This review elucidates the synergistic integration of laboratory automation and state-of-the-art
AI—particularly Transformers and LLMs—into a closed-loop, self-driving laboratory paradigm for MOF
discovery. It details how AI-driven feedback, high-throughput platforms, and knowledge extraction
from literature converge to enable autonomous, data-driven synthesis, characterization, and inverse
design of MOFs.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Establish automated laboratory infrastructure encompassing robotic synthesis, sample handling, and high-throughput screening modules. \\
• 2: Deploy high-throughput experimental platforms for parallelized synthesis, characterization (PXRD, NMR, TEM), and evaluation (adsorption, catalysis). \\
• 3: Integrate laboratory information management systems (LIMS) for structured data curation and workflow management. \\
• 4: Apply machine learning/deep learning models for property prediction and experimental guidance using accumulated data. \\
• 5: Adopt Transformer-based models and LLMs for structure-property prediction, literature mining, synthesis condition extraction, and generative MOF design. \\
• 6: Implement feedback-driven experimental planning via Bayesian optimization, reinforcement learning, or LLM-driven task planners. \\
• 7: Iteratively refine models and protocols in a closed-loop SDL, autonomously updating synthesis/design strategies based on real-time outcomes. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

MOF structural and property databases such as MOFX-DB, ARC-MOF, hMOF, QMOF, and in-house/generated
HTE data; text corpora from scientific literature and patents used for LLM fine-tuning and
information extraction; multi-million entry simulation datasets for pretraining (e.g., 1M+
hypothetical MOFs in MOFTransformer, 1.9M in PMTransformer); experimental records from robotic
synthesis/characterization platforms.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Experimental Throughput: Number of unique MOF samples synthesized, characterized, and evaluated per unit time. \\
• Prediction Accuracy: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), coefficient of determination (R²) for property prediction models (e.g., adsorption, bandgap, stability). \\
• Generalizability: Performance on out-of-distribution or unseen MOF structures/datasets, transferability to new tasks or materials classes. \\
• Structural Validity/Diversity: Percentage of generated MOF candidates that are synthetically accessible and chemically valid, structural diversity indices. \\
• Automation Level: SDL autonomy score (Levels 1–5), extent of human intervention required. \\
• Information Extraction F1 Score: Precision, recall, and F1 for chemical entity and relation extraction from literature. \\
• Resource Efficiency: Computational and experimental resources expended per successful discovery or optimization cycle. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

Integration of AI and laboratory automation is expected to yield >90\% accuracy in property
prediction (e.g., MOFTransformer's MTP/MOC accuracy >0.97/0.98), 2–10x acceleration in MOF discovery
throughput, and significant reductions in labor and experimental time. Closed-loop SDLs will enable
autonomous optimization, reproducible high-quality synthesis, and rapid extraction of actionable
knowledge from literature, collectively setting new benchmarks for efficiency, reproducibility, and
innovation in MOF research.

\end{tcolorbox}


% 数学 Math
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Math},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• Dijkstra1959: Classic label-setting SSSP algorithm using priority queues; achieves O(n·log n + m) time with Fibonacci heaps but is inherently sequential and difficult to parallelize efficiently. \\
• Thorup1999: RAM-based linear-time SSSP for undirected graphs using component trees and atomic heaps; limited to large n and specific hardware, and not easily generalized or parallelized for directed graphs. \\
• BellmanFord1958: Label-correcting algorithm with O(n·m) time; allows negative weights but is suboptimal in the worst case and shows little potential for efficient parallelization. \\
• Han1997 / PaigeKruskal1985: Matrix multiplication-based SSSP achieves polylogarithmic parallel time at superlinear work complexity (O(n\^{}3 log n)); impractical for sparse graphs due to excessive work. \\
• KleinSubramanian1997: Randomized parallel BFS-based SSSP for unweighted/weighted graphs; achieves sublinear time for certain approximations, but exact solutions still demand high work or multiple passes. \\
• Crauser1998: Parallelizes Dijkstra by organizing computation into phases for random graphs; achieves O(n\^{}\{1/3\} log n) time and O(n log n + m) work on average for specific random graph classes. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• No known work-efficient parallel SSSP algorithm achieves sublinear time for arbitrary directed graphs with nonnegative edge weights. \\
• Existing parallel methods either settle nodes sequentially or incur superlinear work, limiting practical scalability on large graphs. \\
• Traditional bucket-based or priority queue approaches struggle to balance parallelism and efficiency, especially with varied edge weights and node degrees. \\
• Load balancing and minimizing redundant relaxations/reinsertions are unsolved for arbitrary, especially high-degree, graphs in parallel settings. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Current approaches to parallel SSSP either replicate sequential order—limiting parallel speedup—or
achieve fast parallel time only at the cost of excessive (superlinear) work, particularly on general
graphs. Previous bucket-based label-correcting algorithms lack robust average-case guarantees for
noninteger or random edge weights, and most practical parallel systems cannot efficiently exploit
fine-grained sequential priority queues.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The practical need for scalable, efficient shortest path computation on large graphs with arbitrary
structure and edge weights drives the search for algorithms that are both parallelizable and work-
efficient. Empirical evidence suggests label-correcting algorithms can outperform label-setting
ones, but theoretical justification and robust parallelization remain lacking. Bridging this gap is
crucial for leveraging modern parallel and distributed architectures in large-scale graph analytics.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

Develop and analyze a parallelizable single-source shortest path (SSSP) algorithm for arbitrary
directed graphs with nonnegative edge weights that achieves linear or near-linear work and sublinear
parallel time for broad graph classes, while providing provable average-case guarantees.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• Dijkstra1959: Sequential label-setting using priority queues; optimal for many sequential settings but fundamentally sequential and hard to parallelize without loss of work efficiency. \\
• ApproximateBucket: Bucket-based variants for small integer weights; can be fast for restricted graphs but either devolve to label-correcting (with reinsertion overhead) or require auxiliary selection structures, limiting parallelism. \\
• BellmanFord: Label-correcting, admits parallel edge relaxations, but incurs high redundancy and pseudo-polynomial time in the worst case. \\
• MatrixMult: Reduces SSSP to matrix multiplications; achieves sublinear parallel time at cubic or worse work, impractical except for dense graphs. \\
• ParallelBFS/Randomized: Suitable for unweighted or random graphs; offers fast approximate solutions but breaks down for exact computations or general edge weights. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

The $\Delta$-stepping algorithm organizes nodes into distance buckets of width $\Delta$, differentiating light
($\leq\Delta$) and heavy (>$\Delta$) edges to balance parallelism and efficiency. In each phase, all nodes in the
minimum nonempty bucket are processed in parallel: light edges are relaxed immediately, while heavy
edges are deferred. By tuning $\Delta$, the method provably achieves linear average-case work and scalable
parallelism for a wide graph class, and can be extended to distributed memory settings and arbitrary
edge weights.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Preprocess graph: partition adjacency lists into light ($\leq\Delta$) and heavy (>$\Delta$) edges; for shortcut-augmented versions, compute and add shortcut edges for all simple $\Delta$-paths. \\
• 2: Initialize: set all tentative distances to $\infty$ except source (0), place source in the appropriate bucket. \\
• 3: Phase main loop: while buckets are nonempty, select the minimum nonempty bucket (current phase), remove all nodes from it. \\
• 4: Light edge relaxation: in parallel, relax all outgoing light edges of nodes in the current bucket; update tentatives and reinsert nodes as needed into corresponding buckets. \\
• 5: Repeat light-edge relaxations (within bucket) until no new nodes enter the current bucket. \\
• 6: Heavy edge relaxation: after the current bucket remains empty, in parallel relax all heavy edges from nodes just processed. \\
• 7: Advance to the next nonempty bucket and repeat. \\
• 8: Parallelization: distribute nodes (and their bucket membership) across processors; generate and assign relaxation requests using randomized dart-throwing or explicit load balancing (semi-sorting); aggregate and execute requests. \\
• 9: Distributed memory extension: replace global memory with message-passing; assign nodes and requests using hashing and tree-based collective operations. \\
• 10: Parameter tuning: select $\Delta$ empirically or via doubling search to balance work and parallel time; for arbitrary weights, use adaptive bucket splitting. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 7-3 \\
• 3-8 \\
• 8-9 \\
• 1-10 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

The paper analyzes both synthetic random graphs (e.g., D(n, d$\overline{d}$/n): n-node digraph, each edge present
independently with probability $\overline{d}$/n, edge weights i.i.d. uniform [0,1]) and real-world-like datasets
(e.g., random geometric graphs, roadmaps). Experiments are conducted on random d-regular graphs
(n=10\^{}3 to 10\^{}6, up to 3·10\^{}6 edges) and large-scale road networks (up to n=157,457).

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Work Complexity: Total number of operations performed across all processors, compared to sequential optimal O(n + m). \\
• Parallel Time: Number of parallel phases until all nodes are settled; measured in terms such as O(d·L·log n + log²n) on PRAM. \\
• Speedup: Empirical wall-clock speedup relative to sequential Dijkstra or $\Delta$-stepping on real and synthetic graphs. \\
• Phases/Reinsertions: Number of bucket phases and total reinsertions, correlated to $\Delta$ and graph/weight parameters. \\
• Scalability: Ability to maintain work efficiency and speedup as the number of processors and graph size increase. \\
• Robustness: Performance across random graphs, geometric graphs, and real-world networks with varying degree and weight distributions. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

$\Delta$-stepping achieves O(n + m + d·L) average-case work and O(d·L·log n + log²n) parallel time for
graphs with random edge weights and bounded degree; for random graphs, O(log²n) time and O(n + m)
work. Experiments show linear or near-linear speedups (e.g., >9× on 16 processors), with phases and
reinsertions scaling sublinearly in n. The approach generalizes to distributed memory and arbitrary
edge weights, providing, for the first time, a practical and work-efficient parallel SSSP algorithm
applicable to large, arbitrary graphs.

\end{tcolorbox}


% 神经 Neuroscience
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Neuroscience},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• ConvNet: A pioneering end-to-end CNN architecture employing temporal and spatial convolutional layers for EEG decoding, offering improved performance over traditional approaches but limited to local feature extraction due to restricted receptive field. \\
• EEGNet: A compact CNN model using temporal and depthwise spatial convolutions, exhibiting robust generalization across BCI paradigms; however, it also fails to capture long-term dependencies inherent in EEG time series. \\
• Transformer-Based EEG Models: Attention-based Transformers leverage global temporal dependencies for EEG decoding, achieving notable performance but neglecting local feature learning, necessitating additional pre-processing or feature extraction steps. \\
• FBCSP: A classical approach utilizing filter bank common spatial patterns to extract task-specific hand-crafted features for motor imagery classification, demonstrating strong performance but lacking generalization and requiring prior knowledge. \\
• Hybrid and Graph-based Methods: Combining CNNs with hand-crafted features or graph structures to enhance spatial-temporal modeling. These methods improve local-global representations but often involve complex architectures or task-dependent preprocessing. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Accurately decoding EEG signals requires capturing both local features (temporal and spatial) and global dependencies due to the non-stationary and low signal-to-noise nature of EEG data. \\
• CNN-based models are constrained by local receptive fields, failing to capture long-range temporal dependencies crucial for sequential EEG data. \\
• Transformer-based models, though adept at modeling global dependencies, often disregard local feature representation, undermining the exploitation of fine-grained EEG information. \\
• End-to-end frameworks for EEG decoding still lack sufficient interpretability regarding their decision process, particularly in identifying task-relevant neural substrates. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Existing EEG decoding approaches either focus on local pattern extraction (CNNs) or global temporal
correlation (Transformers) but rarely integrate both in a unified, efficient, and end-to-end
architecture. Furthermore, most methods require task-specific feature engineering or lack direct
interpretability of neural activation, and high model parameterization raises computational
concerns.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The crucial observation motivating this study is the complementary value of both local and global
features in EEG decoding tasks. As practical BCI applications demand robust, generalizable, and
interpretable models that can efficiently learn from raw EEG data without extensive prior knowledge
or task-specific feature engineering, there is a clear need for an integrated approach that unifies
convolutional and self-attention mechanisms.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To design and validate a compact, end-to-end neural architecture that jointly encapsulates local
temporal-spatial and global temporal dependencies for raw EEG classification, while offering
enhanced interpretability through visualization of learned representations.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• ConvNet: Applies sequential temporal and spatial convolutions to extract discriminative local features, yielding solid performance but limited by short-range context. \\
• EEGNet: Implements depthwise and separable convolutions for temporal and spatial filtering, achieving good generalization yet lacking mechanisms for modeling global dependencies. \\
• RNN/LSTM-based Models: Utilize sequential recurrence to encode long-term temporal dependencies but suffer from inefficient training and rapid decay of influence across time steps. \\
• Transformer-Based Models: Employ self-attention to directly capture long-range dependencies, improving performance for sequential tasks, but require additional modules or preprocessing to encode local information. \\
• Hybrid Methods: Fuse hand-crafted features or graph-based encodings with deep learners, improving local-global feature integration but increasing architectural complexity and dependence on domain expertise. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

The authors introduce EEG Conformer, a lightweight neural framework that sequentially combines
temporal and spatial convolutions for local feature extraction with multi-head self-attention for
learning global temporal dependencies. This unified architecture enables end-to-end decoding from
raw EEG, and a novel visualization approach (Class Activation Topography) enhances interpretability
by mapping activation to brain regions.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Band-pass filter and Z-score standardize raw EEG trials. \\
• 2: Segment and augment data using time-domain segmentation and reconstruction (S\&R). \\
• 3: Feed data into the convolution module: perform temporal convolution (1×25 kernel), spatial convolution (ch×1 kernel), batch normalization, ELU activation, and average pooling (1×75 kernel, stride 15) to extract local features. \\
• 4: Rearrange pooled feature maps: collapse spatial dimension, treat each timepoint's features as a token. \\
• 5: Process tokens with the self-attention module: apply N layers of multi-head self-attention (h heads), followed by feed-forward sublayers. \\
• 6: Pass aggregated features to the fully-connected classifier: two layers with Softmax output. \\
• 7: Train the model with cross-entropy loss using Adam optimizer and perform subject-wise validation. \\
• 8: Visualize feature distributions (t-SNE) and model attention via CAM and CAT for interpretability. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 7-8 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

Three public EEG datasets were used: (1) BCI Competition IV 2a (9 subjects, 22 electrodes, 4 motor
imagery classes, 250 Hz, 288 trials per session), (2) BCI Competition IV 2b (9 subjects, 3 bipolar
electrodes, 2 motor imagery classes, 250 Hz, 5 sessions of 120 trials each), and (3) SEED (15
subjects, 62 electrodes, 3 emotion classes, 1000 Hz downsampled to 200 Hz, \~{}3394 trials/session).
Each dataset covers distinct paradigms and acquisition settings, supporting model generalization.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Classification Accuracy: Percentage of correctly predicted EEG trials across classes, reflecting decoding performance. \\
• Cohen's Kappa: A statistical measure of inter-rater agreement accounting for chance, used to evaluate classification reliability. \\
• Wilcoxon Signed-Rank Test: Non-parametric test for statistical significance of performance differences between models or ablation settings. \\
• Training Efficiency: Measured as convergence speed (epochs to stable loss/accuracy) and per-epoch training time. \\
• Interpretability: Qualitatively assessed via t-SNE clustering of learned features, CAM heatmaps, and CAT spatial-temporal mappings. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

EEG Conformer achieves state-of-the-art classification accuracy and kappa across all three datasets:
on BCI IV 2a, average accuracy 78.66\% (↑10.91\% over FBCSP), kappa 0.7155; on BCI IV 2b, 84.63\%
accuracy, kappa 0.6926; on SEED, 95.30\% accuracy, kappa 0.9295. Ablation studies show a 6.02\%
average accuracy drop without the self-attention module. Visualization confirms the model's focus on
paradigm-relevant brain regions, and the architecture demonstrates efficient convergence and
robustness to parameter variations, establishing a strong new backbone for general EEG decoding.

\end{tcolorbox}



% 物理 Physics
\begin{tcolorbox}[
    breakable,
    title={Example of Idea Generation in Physics},
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Question}}}

You are a top-tier researcher in your field. Based on the following context, please generate a novel and detailed research proposal.

\textbf{\emph{\textcolor{CaseOrange}{RelatedWork}}}

• eSEN-30M-OMat: An equivariant graph neural network tailored for materials, achieving strong accuracy via large-scale message passing, but limited to domain-specific datasets and lacking generalization across molecules or surfaces. \\
• GemNet-OC20: A graph neural network for catalysis using geometric embeddings, excelling in adsorption energy prediction but focused solely on catalysis, without material or molecular generalization. \\
• MACE: A foundation model for atomistic materials chemistry that demonstrates excellent transferability within the organic molecule domain, but struggles to generalize simultaneously to diverse materials and catalytic systems. \\
• EquiformerV2 : An advanced equivariant transformer model that achieves strong performance on domain-specific materials and catalysis benchmarks but is not trained for multi-domain or multi-DFT-task generalization. \\
• ORB v3: A scalable neural network potential capable of efficient simulation at scale, but designed primarily for periodic materials, with limited multi-domain applicability. \\
• Universal Graph Deep Learning Potentials: Aim to provide comprehensive coverage across the periodic table, yet tend not to generalize to molecules or catalysis due to distribution shifts and differing DFT settings. \\
• Pre-training with Fine-tuning: Large models are pre-trained on broad datasets and fine-tuned for specific tasks, yielding high accuracy but still requiring domain adaptation; true zero-shot generalization across tasks remains unproven. 

\textbf{\emph{\textcolor{CaseOrange}{Challenges}}}

• Developing a single MLIP capable of high-fidelity, zero-shot generalization across vastly different chemical domains, including materials, molecules, catalysis, molecular crystals, and MOFs. \\
• Scaling model and dataset size without sacrificing inference speed or memory efficiency, especially for long-running atomistic simulations involving thousands to hundreds of thousands of atoms. \\
• Reconciling and learning from datasets with heterogeneous DFT settings, label distributions, elemental coverage, and system sizes. \\
• Maintaining energy conservation, physical symmetry (rotational equivariance), and smoothness of the potential energy surface during multi-task, multi-domain learning. \\
• Efficiently training and deploying ultra-large models (up to billions of parameters) under memory and compute constraints. 

\textbf{\emph{\textcolor{CaseOrange}{Limitation}}}

Most existing MLIPs are either specialized for a single chemical domain or require fine-tuning to
achieve high accuracy in new domains. They do not robustly generalize across materials, molecules,
and catalytic systems with varying DFT settings. Further, attempts to scale model capacity often
degrade inference efficiency, and models are typically trained on smaller, less diverse datasets,
limiting their practical universality.

\textbf{\emph{\textcolor{CaseOrange}{Motivation}}}

The demand for rapid, accurate, and general-purpose atomistic simulations is increasing in fields
such as drug discovery, energy storage, and catalysis. However, DFT is computationally prohibitive,
and existing ML surrogates lack universality. The confluence of new, massive multi-domain datasets
and insights from scaling laws in deep learning presents the opportunity to create a single, highly
scalable MLIP that achieves state-of-the-art accuracy, speed, and generalization across all relevant
chemical domains.

\textbf{\emph{\textcolor{CaseOrange}{TaskObjective}}}

To design, train, and evaluate a family of universal machine learning interatomic potentials (UMA)
that achieve high accuracy, computational efficiency, and generalization across diverse chemical and
materials domains, using the largest multi-domain atomic datasets to date.

\textbf{\emph{\textcolor{CaseOrange}{ExistingSolutions}}}

• eSEN: Utilizes equivariant message passing with spherical harmonics for high accuracy in materials, but lacks multi-domain scalability. \\
• GemNet: Employs geometric embeddings for catalysis; effective on domain-specific adsorption tasks but does not generalize to other domains. \\
• MACE: Foundation model for molecules, demonstrates good transferability within molecular datasets; struggles with cross-domain and multi-task generalization. \\
• EquiformerV2: Equivariant transformer with improved scaling for materials and catalysis, but not designed for simultaneous multi-domain learning. \\
• ORB v3: Focuses on scalable neural network potentials for materials, achieving high throughput but lacks coverage of molecular and catalytic tasks. \\
• Fine-tuned Foundation Models: Pre-train on large datasets, then fine-tune for each target domain; yields high performance but necessitates domain-specific adaptation and fails to provide universal zero-shot performance. 

\textbf{\emph{\textcolor{DeepPurple}{Reference Answer}}}

\textbf{\emph{\textcolor{CaseOrange}{Idea}}}

UMA introduces a family of universal MLIPs trained on nearly 500M multi-domain atomic structures,
leveraging an efficient Mixture of Linear Experts (MoLE) architecture for scalable capacity without
inference overhead. Empirical scaling laws inform model/data sizing, while unified embeddings and
referencing schemes enable seamless multi-DFT-task learning, delivering state-of-the-art accuracy
and speed across chemistry and materials science domains.

\textbf{\emph{\textcolor{CaseOrange}{ImplementationSteps}}}

• 1: Data aggregation and preprocessing: curate and normalize OMat24, OMol25, OC20++, OMC25, and ODAC25, applying energy referencing and label normalization. \\
• 2: Model design: configure eSEN-based GNN with integrated MoLE layers; implement global embeddings for charge, spin, and DFT task. \\
• 3: MoLE routing: compute expert coefficients from global system features and pre-merge expert weights for efficient inference. \\
• 4: Stage 1 training: pre-train the model in BF16 on direct force prediction with max-atom batching and reduced neighbors. \\
• 5: Stage 2 fine-tuning: switch to FP32 precision and auto-grad conservative heads, increasing neighbor count for energy/force conservation. \\
• 6: Memory/computation optimization: employ graph parallelism, FSDP, and activation checkpointing for large-scale training. \\
• 7: Model selection: use empirical scaling laws to determine optimal model and dataset size for given compute budget. \\
• 8: Evaluation: benchmark UMA models on held-out splits and established tasks across materials, catalysis, molecules, molecular crystals, and MOFs. 

\textbf{\emph{\textcolor{CaseOrange}{ImplementationOrder}}}

• 1-2 \\
• 2-3 \\
• 3-4 \\
• 4-5 \\
• 5-6 \\
• 6-7 \\
• 7-8 

\textbf{\emph{\textcolor{CaseOrange}{Data}}}

UMA is trained on five large-scale datasets: OMat24 (bulk materials, 100M entries, 89 elements,
VASP-PBE), OMol25 (molecules, 75M entries, 83 elements, ORCA-$\omega$B97M-V), OC20++ (catalysis, 229M, 56
elements, VASP-RPBE), OMC25 (molecular crystals, 25M, 12 elements, VASP-PBE+D3), and ODAC25 (MOFs,
29M, 70 elements, VASP-PBE+D3). Combined, the data covers \~{}459M structures and >30B atoms with
near-complete elemental coverage and diverse DFT settings.

\textbf{\emph{\textcolor{CaseOrange}{EvaluationMetrics}}}

• Mean Absolute Error (MAE): Measures average absolute deviation between predicted and reference energies, forces (in meV/Å), and stresses (meV/Å\^{}3). \\
• Adsorption Energy Success Rate: Percentage of cases where the predicted global minimum adsorption energy is within 0.1 eV of the DFT minimum (AdsorbML benchmark). \\
• F1 Score: Assesses binary/classification performance on Matbench Discovery for stability predictions. \\
• Energy Conservation: Degree to which predicted forces/energies conserve energy over molecular dynamics trajectories (NVE MD benchmarks). \\
• Simulation Throughput: Number of inference steps per second for fixed system sizes (1k, 10k, 100k atoms) on a single GPU. \\
• Out-of-Domain Generalization: Performance on OOD splits, such as high-entropy alloys and novel molecular/crystal structures. \\
• Phonon and Elastic Property Accuracy: MAE for phonon frequencies, free energies, elastic moduli, and related properties pertinent to material science benchmarks. 

\textbf{\emph{\textcolor{CaseOrange}{ExpectedOutcome}}}

UMA achieves state-of-the-art or superior accuracy on diverse benchmarks (e.g., up to 25\%
improvement in AdsorbML success rate, \~{}80\% reduction in OC20 adsorption energy error vs. prior
SOTA, chemical accuracy for ligand strain energy). The models support efficient simulation of >100k
atoms with no inference penalty from increased capacity. UMA provides reliable, energy-conserving
predictions across all major chemical domains, demonstrating that a single model can match or
surpass specialized models in both zero-shot and fine-tuned settings.

\end{tcolorbox}


\subsubsection{AI-Assisted Scientific Experiment}

\paragraph{Dry Experiment}

\

\input{sections/codes/code_00}
\input{sections/codes/code_01}
% \input{sections/codes/code_02}

\paragraph{Wet Experiment}

\

\begin{tcolorbox}[
    breakable,
    title=Example of Wet Experiment in Life,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Background}}}

Cancer development involves genetic and epigenetic alterations that enable tumor cells to evade immune detection by creating an immunosuppressive microenvironment. A key mechanism of immune evasion is mediated by the programmed death-ligand 1 (PD-L1), expressed on tumor and immune cells, which binds to programmed death-1 (PD-1) and B7.1 (CD80) receptors on T cells. This interaction inhibits T-cell migration, proliferation, and cytotoxic function, thereby limiting tumor cell killing. Blocking PD-L1 can restore antitumor immunity by reactivating suppressed T cells.

An engineered humanized monoclonal antibody targeting PD-L1 has been developed to inhibit its interaction with PD-1 and B7.1, without affecting PD-1’s interaction with PD-L2, preserving peripheral tolerance. This antibody is designed with an Fc domain modification to prevent antibody-dependent cellular cytotoxicity, avoiding depletion of activated T cells.

Clinical studies involving patients with advanced solid tumors treated with this anti-PD-L1 antibody demonstrated safety and tolerability across a range of doses, with manageable adverse events such as fatigue and low-grade fever. Immune activation markers, including proliferating $\mathrm{CD8}^+$ T cells and interferon-gamma (IFN-$\gamma$), increased during treatment.

Efficacy assessments revealed objective responses in multiple cancer types, notably non-small cell lung cancer (NSCLC), melanoma, and renal cell carcinoma. Importantly, clinical responses correlated strongly with pre-treatment PD-L1 expression on tumor-infiltrating immune cells rather than tumor cells themselves. High PD-L1 expression on immune cells was associated with higher response rates and longer progression-free survival. Additional biomarkers linked to response included T-helper type 1 (TH1) gene expression and $\mathrm{CTLA4}$ expression, while fractalkine ($\mathrm{CX3CL1}$) expression correlated with disease progression.

On-treatment biopsies of responding tumors showed increased immune cell infiltration, tumor necrosis, and upregulation of PD-L1 and IFN-$\gamma$, indicating reactivation of antitumor immunity. Non-responding tumors exhibited patterns of immunological ignorance (lack of immune infiltration), non-functional immune responses (immune cells present but inactive), or excluded infiltrates (immune cells restricted to tumor margins), with no significant PD-L1 upregulation or T-cell activation.

Blood-based biomarkers showed increases in IFN-$\gamma$-inducible chemokines and activated cytotoxic T cells early in treatment, reflecting systemic immune activation, though these changes did not clearly distinguish responders from non-responders.

These findings support the concept that pre-existing antitumor immunity suppressed by PD-L1 can be reinvigorated by PD-L1 blockade, leading to durable clinical responses. The presence and localization of PD-L1 expression, particularly on tumor-infiltrating immune cells, serve as predictive biomarkers for response. Understanding the immune microenvironment of non-responders may reveal additional mechanisms of immune resistance and guide combination immunotherapy strategies to enhance the cancer immunity cycle.

\textbf{\emph{\textcolor{DeepPurple}{Action Pool}}}

\begin{lstlisting}
<Fix_tissue_in_formalin>(tissue, fixative)
    Args:
        tissue: Tissue sample to be fixed
        fixative: Formalin solution
    Returns:
        Fixed tissue sample

<Embed_tissue_in_paraffin>(fixed_tissue)
    Args:
        fixed_tissue: Formalin-fixed tissue
    Returns:
        FFPE tissue block

<Section_tissue>(tissue_block, thickness)
    Args:
        tissue_block: Paraffin-embedded tissue block
        thickness: Section thickness in micrometers
    Returns:
        Tissue sections

<Stain_with_antibody>(tissue_section, antibody, concentration)
    Args:
        tissue_section: Tissue section on slide
        antibody: Primary antibody
        concentration: Antibody concentration
    Returns:
        Antibody-labeled tissue section

<Visualize_with_DAB>(stained_section)
    Args:
        stained_section: Antibody-stained section
    Returns:
        DAB-visualized section

<Counterstain_with_hematoxylin>(section)
    Args:
        section: DAB-stained section
    Returns:
        Counterstained section

<Score_IHC_staining>(stained_section, cell_type)
    Args:
        stained_section: Complete IHC-stained section
        cell_type: Type of cells to score (TC or IC)
    Returns:
        IHC score (0-3)

<Incubate_with_primary_antibodies>(section, antibody1, antibody2, temperature)
    Args:
        section: FFPE tissue section
        antibody1: First primary antibody
        antibody2: Second primary antibody
        temperature: Incubation temperature
    Returns:
        Dual-antibody labeled section

<Detect_with_fluorescence>(labeled_section, detection_system, fluorophore)
    Args:
        labeled_section: Antibody-labeled section
        detection_system: Detection reagent system
        fluorophore: Fluorescent label
    Returns:
        Fluorescently labeled section

<Extract_DNA_from_FFPE>(tissue_section, extraction_kit)
    Args:
        tissue_section: FFPE tissue section
        extraction_kit: DNA extraction kit
    Returns:
        Isolated DNA

<Extract_RNA_from_FFPE>(tissue_section, extraction_kit)
    Args:
        tissue_section: FFPE tissue section
        extraction_kit: RNA extraction kit
    Returns:
        Isolated RNA

<Perform_gene_expression_analysis>(RNA_sample, platform, gene_panel)
    Args:
        RNA_sample: Isolated RNA
        platform: Analysis platform
        gene_panel: Panel of genes to analyze
    Returns:
        Gene expression data

<Collect_blood_sample>(patient, tube_type, volume)
    Args:
        patient: Patient identifier
        tube_type: Collection tube type
        volume: Sample volume
    Returns:
        Blood sample

<Isolate_plasma>(blood_sample, centrifuge_speed, time)
    Args:
        blood_sample: Whole blood sample
        centrifuge_speed: Centrifugation speed
        time: Centrifugation time
    Returns:
        Plasma sample

<Analyze_cytokines_by_ELISA>(plasma_sample, cytokine_panel)
    Args:
        plasma_sample: Isolated plasma
        cytokine_panel: Panel of cytokines to measure
    Returns:
        Cytokine levels

<Perform_FACS_analysis>(blood_sample, antibody_panel)
    Args:
        blood_sample: Blood sample
        antibody_panel: Panel of antibodies for staining
    Returns:
        Cell population data

<Administer_MPDL3280A>(patient, dose, route)
    Args:
        patient: Patient identifier
        dose: Drug dose in mg/kg
        route: Administration route
    Returns:
        Treated patient

<Collect_tumor_biopsy>(patient, timepoint)
    Args:
        patient: Patient identifier
        timepoint: Collection timepoint
    Returns:
        Tumor biopsy sample

<Evaluate_tumor_response>(patient, imaging_method, criteria)
    Args:
        patient: Patient identifier
        imaging_method: Imaging modality
        criteria: Response evaluation criteria
    Returns:
        Tumor response assessment

<Store_sample>(sample, temperature)
    Args:
        sample: Biological sample
        temperature: Storage temperature
    Returns:
        Stored sample
\end{lstlisting}

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{paper/imgs/wet_case3.png}
    % \captionof{figure}{\textbf{SGI}}
    % \label{fig:answer}
\end{center}

\end{tcolorbox}


\begin{tcolorbox}[
    breakable,
    title=Example of Wet Experiment in Material,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Background}}}

Low-grade heat, abundant in environments such as solar radiation, body heat, and industrial waste, presents a significant opportunity for energy harvesting. Thermogalvanic cells (TGCs) convert such heat directly into electricity via redox reactions at electrodes maintained at different temperatures. The thermopower of these cells, a measure of voltage generated per unit temperature difference, depends primarily on the entropy change ($\Delta S$) and concentration difference ($\Delta C$) of redox species between hot and cold electrodes. Traditional aqueous redox electrolytes exhibit limited thermopowers, typically below $2~\mathrm{mV\,K}^{-1}$, constraining their practical efficiency.

Recent advances focus on enhancing thermopower by increasing $\Delta S$ through solvent reorganization or structural changes of redox couples, and by increasing $\Delta C$ via selective complexation or confinement of redox ions. Thermoresponsive polymers have been employed to induce temperature-dependent interactions with redox ions, enabling polarization switching between $n$-type and $p$-type behavior, which reverses the direction of electron flow and expands operational versatility.

A notable development involves the use of methylcellulose (MC), a biocompatible, low-cost polymer exhibiting temperature-dependent hydrophilic-to-hydrophobic transitions. When incorporated into an aqueous iodide/triiodide ($\mathrm{I}^-/\mathrm{I}_3^-$) redox electrolyte, MC interacts hydrophobically with $\mathrm{I}_3^-$ ions above its gelation temperature, reducing free $\mathrm{I}_3^-$ concentration at the hot electrode. This interaction induces a polarization switch from $n$-type to $p$-type thermopower and simultaneously enhances both $\Delta S$ and $\Delta C$ due to gelation and ion complexation effects.

Further enhancement is achieved by adding potassium chloride (KCl), which complexes with MC and $\mathrm{I}_3^-$ ions, promoting reversible aggregation and dissociation processes. This salt-induced complexation lowers the gelation and polarization transition temperatures and significantly amplifies thermopower. The optimized ternary electrolyte ($\mathrm{I}^-/\mathrm{I}_3^- + 2~\mathrm{wt}\%~\mathrm{MC} + 0.3~\mathrm{M}~\mathrm{KCl}$) exhibits record-high thermopowers of approximately $-8.18~\mathrm{mV\,K}^{-1}$ ($n$-type) and $9.62~\mathrm{mV\,K}^{-1}$ ($p$-type), an order of magnitude greater than pristine electrolytes.

Electrochemical characterization reveals improved electron transfer kinetics and ionic conductivity in the ternary system, resulting in higher current densities and lower internal resistance in TGCs. Under a $15~^\circ\mathrm{C}$ temperature difference, single $n$-type and $p$-type cells achieve maximum power outputs of $27.78~\mu\mathrm{W}$ and $80.47~\mu\mathrm{W}$, respectively, with normalized power densities surpassing previous iodide/triiodide-based systems.

This approach demonstrates that integrating thermoresponsive biopolymers with salt-induced complexation in redox electrolytes can substantially boost thermogalvanic performance. The findings open pathways for cost-effective, scalable liquid thermocells capable of efficient low-grade heat harvesting, leveraging abundant, environmentally benign materials and tunable electrolyte properties for enhanced energy conversion.

\textbf{\emph{\textcolor{DeepPurple}{Action Pool}}}

\begin{lstlisting}
<Prepare pristine I-/I3- electrolyte>(KI_amount, I2_amount, water_volume)
    Args:
        KI_amount: Amount of potassium iodide
        I2_amount: Amount of iodine
        water_volume: Volume of deionized water
    Returns:
        Pristine I-/I3- electrolyte solution

<Heat electrolyte solution>(electrolyte, temperature)
    Args:
        electrolyte: Electrolyte solution to heat
        temperature: Target temperature
    Returns:
        Heated electrolyte solution

<Add methylcellulose to electrolyte>(electrolyte, MC_amount)
    Args:
        electrolyte: Heated electrolyte solution
        MC_amount: Amount of methylcellulose powder
    Returns:
        Binary electrolyte with MC

<Stir solution magnetically>(solution, duration)
    Args:
        solution: Solution to stir
        duration: Stirring time
    Returns:
        Homogeneous solution

<Add KCl to binary electrolyte>(binary_electrolyte, KCl_concentration)
    Args:
        binary_electrolyte: I-/I3- + MC electrolyte
        KCl_concentration: Molar concentration of KCl
    Returns:
        Ternary electrolyte

<Store electrolyte in refrigerator>(electrolyte, temperature, duration)
    Args:
        electrolyte: Prepared electrolyte
        temperature: Storage temperature
        duration: Storage time
    Returns:
        Stored electrolyte ready for use

<Fill thermocell cavity>(electrolyte, volume)
    Args:
        electrolyte: Prepared electrolyte
        volume: Volume to fill
    Returns:
        Filled thermocell

<Set cold electrode temperature>(thermocell, temperature)
    Args:
        thermocell: Assembled thermocell
        temperature: Cold electrode temperature
    Returns:
        Thermocell with controlled cold electrode

<Heat hot electrode gradually>(thermocell, target_temperature)
    Args:
        thermocell: Thermocell setup
        target_temperature: Maximum hot electrode temperature
    Returns:
        Thermocell with temperature gradient

<Record open-circuit voltage>(thermocell, data_logger)
    Args:
        thermocell: Operating thermocell
        data_logger: Data acquisition device
    Returns:
        Voltage-temperature data

<Measure electrode temperatures>(thermocell, thermocouples)
    Args:
        thermocell: Operating thermocell
        thermocouples: Temperature sensors
    Returns:
        Temperature measurements

<Connect external load>(thermocell, potentiometer)
    Args:
        thermocell: Operating thermocell
        potentiometer: Variable resistance device
    Returns:
        Thermocell with load circuit

<Record current and voltage>(thermocell, source_meter, data_logger)
    Args:
        thermocell: Operating thermocell under load
        source_meter: Current measurement device
        data_logger: Voltage measurement device
    Returns:
        Power generation data

<Perform UV-Vis spectroscopy>(sample, spectrometer)
    Args:
        sample: Electrolyte sample
        spectrometer: UV-Vis instrument
    Returns:
        Absorption spectrum data

<Dilute sample for analysis>(sample, dilution_factor)
    Args:
        sample: Concentrated sample
        dilution_factor: Dilution ratio
    Returns:
        Diluted sample

<Filter electrolyte sample>(sample)
    Args:
        sample: Raw electrolyte sample
    Returns:
        Filtered sample

<Perform cyclic voltammetry>(electrolyte, potentiostat, scan_rate)
    Args:
        electrolyte: Test electrolyte
        potentiostat: Electrochemical instrument
        scan_rate: Voltage scanning rate
    Returns:
        CV curves

<Dry electrolyte under vacuum>(electrolyte, temperature, duration)
    Args:
        electrolyte: Liquid electrolyte
        temperature: Drying temperature
        duration: Drying time
    Returns:
        Dried electrolyte powder

<Perform FTIR spectroscopy>(sample, FTIR_instrument)
    Args:
        sample: Dried powder sample
        FTIR_instrument: FTIR spectrometer
    Returns:
        FTIR spectrum

<Measure ionic conductivity>(electrolyte, conductivity_meter, temperature_range)
    Args:
        electrolyte: Test electrolyte
        conductivity_meter: Conductivity measurement device
        temperature_range: Temperature range for measurement
    Returns:
        Conductivity vs temperature data
\end{lstlisting}

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{paper/imgs/wet_case4.png}
    % \captionof{figure}{\textbf{SGI}}
    % \label{fig:answer}
\end{center}

\end{tcolorbox}


\begin{tcolorbox}[
    breakable,
    title=Example of Wet Experiment in Physics,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]
\textbf{\emph{\textcolor{DeepPurple}{Background}}}

This research domain focuses on the analysis and synthesis of nonlinear discrete-time systems, digital filters, and chaotic circuits, emphasizing stability, noise quantification, and complex dynamical behaviors.

In digital filter design, quantization noise arising from finite word-length effects is a critical concern. Methods have been developed to compute noise covariance matrices associated with extended digital filters, enabling the evaluation of roundoff noise not only at storage nodes but also at other internal nodes. These computations involve iterative matrix summations and transformations, where matrices representing system dynamics and noise propagation are manipulated to yield noise covariance matrices. The approach typically uses state-space representations and involves solving matrix equations that incorporate system matrices and noise input vectors, allowing for precise quantification of noise effects in fixed-point digital filters.

In nonlinear discrete-time systems with slope-restricted nonlinearities, absolute stability criteria are essential for ensuring asymptotic stability in the large. A frequency-domain criterion has been formulated for single-input single-output Lur'e-type systems, where the nonlinearity satisfies sector and slope restrictions. The criterion involves verifying an inequality over the unit circle in the complex plane, incorporating the system’s frequency response and parameters bounding the nonlinearity’s slope. This approach extends the system order and applies Lyapunov function techniques to establish sufficient conditions for global asymptotic stability, providing a rigorous tool for stability analysis in nonlinear discrete-time control systems.

The study of chaotic attractors in simple autonomous circuits reveals that even minimal configurations with piecewise-linear nonlinear elements can exhibit complex chaotic dynamics. A third-order reciprocal circuit with a single nonlinear resistor characterized by a three-segment piecewise-linear function demonstrates chaotic attractors with structures distinct from classical examples like the Lorenz and Rössler attractors. The system’s dynamics are governed by coupled differential equations describing voltages and currents in capacitors and inductors, with nonlinear feedback inducing chaos. The attractor includes invariant sets containing equilibria with specific eigenvalue configurations, and its persistence is confirmed over ranges of circuit parameters. This research highlights the role of circuit reciprocity and nonlinear characteristics in generating and sustaining chaotic behavior, contributing to the understanding of nonlinear dynamics in electrical circuits.

Collectively, these areas integrate advanced mathematical tools—such as state-space modeling, frequency-domain analysis, Lyapunov stability theory, and nonlinear dynamics—to address challenges in system stability, noise management, and chaotic behavior in engineering systems.

\textbf{\emph{\textcolor{DeepPurple}{Action Pool}}}

\begin{lstlisting}
<Build circuit with components>(capacitor1, capacitor2, inductor, resistor)
    Args:
        capacitor1: First capacitor component
        capacitor2: Second capacitor component
        inductor: Inductor component
        resistor: Nonlinear resistor component
    Returns:
        Assembled circuit

<Set capacitor value>(capacitor, capacitance_value)
    Args:
        capacitor: Target capacitor
        capacitance_value: Capacitance value to set
    Returns:
        Configured capacitor

<Set inductor value>(inductor, inductance_value)
    Args:
        inductor: Target inductor
        inductance_value: Inductance value to set
    Returns:
        Configured inductor

<Configure nonlinear resistor>(resistor, conductance, slope_parameters)
    Args:
        resistor: Nonlinear resistor component
        conductance: Conductance value G
        slope_parameters: Piecewise-linear slope values
    Returns:
        Configured nonlinear resistor

<Connect circuit elements>(circuit, connection_scheme)
    Args:
        circuit: Circuit with components
        connection_scheme: Wiring configuration
    Returns:
        Connected circuit

<Initialize circuit state>(circuit, initial_conditions)
    Args:
        circuit: Connected circuit
        initial_conditions: Initial voltages and current values
    Returns:
        Initialized circuit

<Set simulation parameters>(step_size, integration_method)
    Args:
        step_size: Time step for numerical integration
        integration_method: Numerical method to use
    Returns:
        Simulation configuration

<Run circuit simulation>(circuit, simulation_config, time_duration)
    Args:
        circuit: Initialized circuit
        simulation_config: Simulation parameters
        time_duration: Total simulation time
    Returns:
        Simulation results with time series data

<Extract voltage trajectories>(simulation_results, voltage_nodes)
    Args:
        simulation_results: Output from simulation
        voltage_nodes: Specific voltage points to extract
    Returns:
        Voltage time series data

<Extract current trajectories>(simulation_results, current_branch)
    Args:
        simulation_results: Output from simulation
        current_branch: Specific current branch to extract
    Returns:
        Current time series data

<Generate phase portrait>(voltage_data, current_data, projection_plane)
    Args:
        voltage_data: Voltage trajectories
        current_data: Current trajectories
        projection_plane: 2D plane for projection
    Returns:
        Phase portrait visualization

<Identify attractor characteristics>(phase_portraits, trajectory_data)
    Args:
        phase_portraits: Generated phase portraits
        trajectory_data: Complete system trajectories
    Returns:
        Attractor properties and structure

<Vary circuit parameters>(circuit, parameter_name, parameter_range)
    Args:
        circuit: Base circuit configuration
        parameter_name: Parameter to vary
        parameter_range: Range of values to test
    Returns:
        Parameter sweep results

<Analyze bifurcation behavior>(parameter_sweep_results, stability_criteria)
    Args:
        parameter_sweep_results: Results from parameter variation
        stability_criteria: Criteria for stability analysis
    Returns:
        Bifurcation analysis results

<Identify periodic orbits>(trajectory_data, newton_iteration_params)
    Args:
        trajectory_data: System trajectories
        newton_iteration_params: Parameters for Newton iteration
    Returns:
        Periodic orbit characteristics
\end{lstlisting}

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{paper/imgs/wet_case5.png}
    % \captionof{figure}{\textbf{SGI}}
    % \label{fig:answer}
\end{center}

\end{tcolorbox}

\subsubsection{Scientific Experimental Reasoning}

\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Astronomy,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/astronomy/01.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/astronomy/02.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/astronomy/03.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Using the time–frequency ridge data points $(t,f)$ from the first image, estimate the chirp mass $M_c$ via the Newtonian approximation and $t_c = 0$. From the second image, the noise-weighted integral is:
\begin{equation}
    J = f \int_{f_{min} \rightarrow f_{max}} f^(-7/3)/S_n(f) df = 1.3826254536\times10^{60} \text{(SI units)}.
\end{equation}
From the three image, the network SNR is $\rho_{net} = 24$ (detector factor $F=1$).  Under the stationary phase approximation, Solve for the luminosity distance $D_L$ using and select the answer (in Mpc, rounded) from options 0 to 9 below.

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item 100
    \item 150
    \item 210
    \item 270
    \item 350
    \item 410
    \item 500
    \item 620
    \item 750
    \item 1000
\end{enumerate}

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/astronomy/01.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} Read three points $(t, f)$, calculate $X=f^{-8/3}$ and $Y=-t$, and use the least squares fitting to obtain the slope $K$.

\textbf{\textcolor{CaseOrange}{Step 3.}} Quality of the solution by $K$ chirp: $M_c^3 = (c/G) \times [((5/256) \pi ^ {- 8/3})/K] ^ {3/5}$.

\textbf{\textcolor{CaseOrange}{Step 4.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/astronomy/02.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 5.}} The provided value of $J$.

\textbf{\textcolor{CaseOrange}{Step 6.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/astronomy/03.png}
\end{center}


\textbf{\textcolor{CaseOrange}{Step 7.}} Read $\rho_{net} = 24$ and the direction factor $F=1$.

\textbf{\textcolor{CaseOrange}{Step 8.}} Substitution $\rho_{net} ^ 2 = 4 A ^ 2 J$ and $A = (1 / D_L) {5/24} \pi ^ {- 2/3} M_c (G/c ^ 3) ^ {5/6 }$, work out $D_L$.

\textbf{\textcolor{CaseOrange}{Step 9.}} Convert $D_L$ to Mpc and round it to the nearest integer

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

F

\end{tcolorbox}





\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Chemistry,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/chemistry/01.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/chemistry/02.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/chemistry/03.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Based on the graphical models and prediction visualizations, which combination of template matching mechanism and reaction center identification approach is demonstrated across these three image, and what is the key chemical insight revealed by the successful prediction case?

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item Template matching via subgraph isomorphism + Atom-level scoring with GNN embeddings; The model correctly identifies esterification reaction centers and preserves stereochemistry.
    \item SMILES sequence alignment + Molecular fingerprint similarity; Successful predictions maintain atomic connectivity but miss stereochemical information.
    \item Reaction center extraction + Graph neural network compatibility scoring; Correct predictions align with known reaction mechanisms and preserve molecular topology.
    \item Rule-based template application + Attention-based focus mapping; The model captures functional group reactivity patterns and bond formation sites.
    \item Subgraph pattern matching + Energy-based scoring functions; Accurate retrosynthesis requires matching both structural patterns and chemical feasibility.
    \item Neural sequence-to-sequence + Structural motif recognition; Successful predictions demonstrate the importance of reaction template specificity.
    \item Graph isomorphism testing + Probabilistic template selection; The visualization shows positive scores on reactive atoms and negative on inactive regions.
    \item Molecular similarity comparison + Template ranking by frequency; Correct predictions occur when common reaction patterns are identified.
    \item Conditional graphical model + Hierarchical sampling; The model learns to assign high compatibility scores to chemically plausible reaction centers.
    \item Multi-class classification + Beam search optimization; Visualization reveals the model's ability to distinguish active reaction sites from background structure.
\end{enumerate}



\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/chemistry/01.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} Analyze the chemical reaction and retrosynthesis template schematic, identifying the highlighted reaction centers in the reaction participants.

\textbf{\textcolor{CaseOrange}{Step 3.}} Determine that the template matching mechanism is based on reaction center extraction, identifying chemical transformation sites through subgraph pattern matching.

\textbf{\textcolor{CaseOrange}{Step 4.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/chemistry/02.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 5.}} Parse the three-layer architecture of the GLN retrosynthesis pipeline, understanding the logical relationships between template sets, subgraph sets, and molecule sets.

\textbf{\textcolor{CaseOrange}{Step 6.}} Identify the role of graph neural networks in compatibility scoring, analyzing the computation process of embedding vectors.


\textbf{\textcolor{CaseOrange}{Step 7.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/chemistry/03.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 8.}} Compare the core region matching between predicted reactions and true reactions in successful prediction cases.

\textbf{\textcolor{CaseOrange}{Step 9.}} Verify the consistency between prediction results and known reaction mechanisms, analyzing the preservation degree of molecular topology.

\textbf{\textcolor{CaseOrange}{Step 10.}} Integrate information from all three figures: template matching based on reaction center extraction provides structural foundation, GNN compatibility scoring provides chemical feasibility assessment, and actual cases validate method effectiveness.

\textbf{\textcolor{CaseOrange}{Step 11.}} Derive key chemical insight: successful retrosynthesis prediction requires simultaneously satisfying both structural pattern matching and reaction mechanism alignment conditions.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

C

\end{tcolorbox}





\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Earth,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/1.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/2.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/3.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/4.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

The first, second, and third images display the Zonal Mean Ocean Heat Content (OHC) anomalies for 0-2000m in the Pacific, Atlantic, and Indian Oceans, respectively, in ZJ per degree latitude (ZJ deg-1) relative to a 2000-2004 baseline, as a function of time (2000-2024) and latitude. The fourth image shows the Oceanic Niño Index (ONI) time series.

Based only on the visual information from these four images, which of the following combined statements is most likely true?

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item The onset of the OHC warming band ($\geq1$ ZJ deg-1) in the Indian Ocean (Figure 3) near 40°N occurred earlier than the warming in the Pacific (Figure 1) and Atlantic (Figure 2) at the same latitude. The strong El Niño event in 2010 (Figure 4) coincided with an OHC cooling anomaly (blue) in the Pacific Ocean (Figure 1) in the 40°S latitude band.
    \item The OHC anomaly in the equatorial Pacific (near 0°, Figure 1) is predominantly one of cooling (blue) during strong El Niño events (ONI $\ge1.0$, Figure 4), while the OHC anomaly in the equatorial Atlantic (near 0°, Figure 2) largely remains near zero (white). In the Southern Hemisphere subtropics (30°S to 50°S), the sustained OHC warming ($\geq 1$ ZJ deg-1) in the Pacific began earlier than in the Atlantic and Indian Oceans.
    \item The OHC anomaly in the Pacific Ocean (Figure 1) near 20°N was dominated by cooling during 2000-2010 and by warming during 2010-2024. The sustained cooling anomaly (blue) in the 50°N-60°N latitude band of the Atlantic Ocean (Figure 2) is a unique feature not observed in the corresponding northernmost latitudes of the other two basins.
    \item The Indian Ocean (Figure 3) exhibits OHC cooling anomalies near 20°S, whereas the Atlantic (Figure 2) and Pacific (Figure 1) have never shown cooling anomalies in the same latitude band. During the strong El Niño event of 2015-2016 (Figure 4), the OHC warming strength in the Atlantic Ocean (Figure 2) at 40°N reached its maximum value for the 2000-2024 period.
    \item The OHC anomaly strength in the Indian Ocean (Figure 3) at 40°S consistently exceeded the anomaly strength in the Pacific Ocean (Figure 1) at 40°S after 2016. During the strong La Niña event of 2020-2022 (Figure 4), the OHC anomaly strength in the Pacific Ocean (Figure 1) near 40°N remained between 0 and 1 ZJ deg-1.
    \item The OHC anomaly in all three basins (Figures 1, 2, 3) in the 20°S to 40°S latitude band shows a continuously intensifying warming trend after 2016. The OHC anomaly strength in the Pacific Ocean (Figure 1) near 40°N was greater than 0 ZJ deg-1 (non-blue) for all years in the 2000-2024 period.
    \item The sustained duration of OHC warming ($\geq 1$ ZJ deg-1) in the Atlantic Ocean (Figure 2) at 40°S is longer than the sustained duration at 40°N. The Pacific OHC anomaly (Figure 1) near 0° shows a strong positive correlation with the ONI (Figure 4).
    \item In the 20°S to 40°S latitude band, the OHC anomaly in the Indian Ocean (Figure 3) is the most unstable (most frequent alternation between positive and negative) of the three basins. The Atlantic Ocean (Figure 2) at 40°S has never reached an OHC warming anomaly strength of $\geq 2$ ZJ deg-1 since 2000.
    \item The OHC warming band ($\geq 1$ ZJ deg-1) in the Pacific Ocean (Figure 1) at 40°N started after 2014, approximately five years later than the warming onset in the Atlantic Ocean (Figure 2) at 40°N. The La Niña event in 2010-2011 (Figure 4) coincided with a strong OHC cooling anomaly (blue) in the Pacific Ocean (Figure 1) at 40°N.
    \item The Indian Ocean (Figure 3) exhibited strong warming ($\geq2$ ZJ deg-1) only in the Southern Hemisphere (0°S southward) during 2000-2024. The OHC anomaly in the 60°S-40°S latitude band of the Atlantic Ocean (Figure 2) was negative (blue) before 2010.
\end{enumerate}

\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} Strong warming centers are observed near 40°N and 40°S (deep red $\geq 3$ ZJ deg-1). The equatorial band (0°) OHC anomaly alternates significantly (blue/red) and is strongly related to time/ENSO. Sustained strong warming ($\geq 1$ ZJ deg-1) at 40°S begins around 2014.

\textbf{\textcolor{CaseOrange}{Step 3.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/2.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 4.}} Strong warming is present at 40°S (deep red $\geq 3$ ZJ deg-1). Warming at 40°N is present but slightly weaker (red ~2-3 ZJ deg-1). A persistent cooling (blue) anomaly is seen in the 50°N-60°N band since 2010. Sustained strong warming at 40°S begins around 2016.

\textbf{\textcolor{CaseOrange}{Step 5.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/3.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 6.}} The main warming center is at 40°S. The tropical region shows frequent anomaly changes. Sustained strong warming at 40°S begins around 2016.


\textbf{\textcolor{CaseOrange}{Step 7.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/earth/4.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 8.}} Provides the timing of El Niño (positive peaks) and La Niña (negative peaks) events.

\textbf{\textcolor{CaseOrange}{Step 9.}} Evaluate Option 1 : S1 (Figures 1, 2, 3): The warming band ($\geq 1$ ZJ deg-1) at 40°N in the Indian Ocean (Figure 3) only clearly appears after 2022. Both the Pacific and Atlantic Oceans show this warming starting around 2014. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 10.}} Evaluate Option 2 : S1 (Figures 1, 2, 4): During strong El Niño events (e.g., 2015-2016, Figure 4), the equatorial Pacific (Figure 1, 0°) is blue (cooling/negative anomaly), confirming a negative correlation with ONI. The equatorial Atlantic (Figure 2, 0°) remains mostly white (near zero anomaly) during these periods. S1 is TRUE. S2 (Figures 1, 2, 3): In the 30°S to 50°S band, the Pacific (Figure 1) sustained strong warming ($\geq 1$ ZJ deg-1) began around 2014. The Atlantic (Figure 2) and Indian (Figure 3) sustained warming began around 2016. Thus, the Pacific began earlier. S2 is TRUE. Conclusion: Option 1 is TRUE.

\textbf{\textcolor{CaseOrange}{Step 11.}} Evaluate Option 3 : S1 (Figure 1): The 20°N band in the Pacific shows mostly blue/white (cooling/zero anomaly) during 2000-2010. It shows mixed red/blue (warming/cooling) during 2010-2024. The description of the dominant anomaly sign for the two periods is incorrect. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 12.}} Evaluate Option 4 : S1 (Figures 1, 2, 3): While the Indian Ocean (Figure 3) shows cooling near 20°S, the Pacific (Figure 1) also shows cooling (blue) in the 20°S band around 2004-2006. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 13.}} Evaluate Option 5 : S1 (Figures 1, 3): The OHC anomaly strength at 40°S in the Pacific (Figure 1) is consistently high ($\geq 3$ ZJ deg-1) after 2016, whereas the Indian Ocean (Figure 3) strength weakens significantly around 2018-2020. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 14.}} Evaluate Option 6 : S1 (Figures 1, 2, 3): The warming in the 20°S to 40°S band is not continuously intensifying in all three basins after 2016; the Indian Ocean (Figure 3) shows a significant weakening/cooling patch around 2018-2020. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 15.}} Evaluate Option 7 : S1 (Figure 2): The Atlantic 40°S warming ($\geq 1$ ZJ deg-1) starts around 2016, while 40°N warming starts around 2014. 40°S warming has a shorter duration. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 16.}} Evaluate Option 8 : S1: In the 20°S to 40°S latitude band, the OHC anomaly in the Indian Ocean (Figure 3) is the most unstable (most frequent alternation between positive and negative) of the three basins. S1 is TRUE. (Note: This is the first part of the original Option 8 and is retained as True).S2 : The Atlantic Ocean (Figure 2) at 40°S has never reached an OHC warming anomaly strength of $\geq 2$ ZJ deg-1 since 2000. Check: In Figure 2, the 40°S band clearly shows colors corresponding to $\geq 2$ ZJ deg-1(dark red/deepest red) starting around 2016. Therefore, S2 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 17.}} Evaluate Option 9 : S1 (Figures 1, 2): The onset of warming ($geq 1$ ZJ deg-1) at 40°N in both the Pacific (Figure 1) and Atlantic (Figure 2) occurs around 2014. There is no ~5-year lag. S1 is FALSE.

\textbf{\textcolor{CaseOrange}{Step 18.}} Evaluate Option 10: S1 (Figure 3): The Indian Ocean (Figure 3) shows strong warming ($\geq 2$ ZJ deg-1) in the Northern Hemisphere near 40°N after 2022. S1 is FALSE.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

B

\end{tcolorbox}





\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Energy,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.8\linewidth]{imgs/sci-exp-case/energy/1.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.8\linewidth]{imgs/sci-exp-case/energy/1.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Based on the thermal energy storage (TES) state-of-charge visualizations shown in the two images, analyze the operational patterns across the 7-day period. The first image displays four TES units (TES1-TES4) operating independently, while the second image shows the same units under cooperative operation. During the time period from Day 3 to Day 5, which specific operational advantage of the cooperative mode most directly explains the consistently higher storage capacity utilization observed in TES4 compared to its independent operation?

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item Cooperative operation allows TES4 to receive excess thermal energy from microgrids without storage devices during high solar generation periods, maintaining near-maximum capacity
    \item The cooperative mode reduces TES4's discharge rate during peak thermal demand hours through load balancing across all microgrids
    \item Independent operation causes TES4 to experience more frequent charging cycles due to isolated thermal load requirements
    \item Cooperative operation eliminates the need for TES4 to supply thermal energy during nighttime hours through grid-level coordination
    \item The sharing of thermal energy in cooperative mode increases TES4's charging efficiency by 15-20\% through optimized heat transfer
    \item Independent operation requires TES4 to maintain a minimum reserve capacity for emergency thermal supply, preventing full utilization
    \item Cooperative mode enables TES4 to store thermal energy generated by micro-turbines from neighboring microgrids during low-demand periods
    \item The coordinated operation reduces thermal losses in TES4 by synchronizing charge-discharge cycles with solar thermal availability patterns
    \item Independent operation forces TES4 to discharge more frequently to meet local thermal loads that exceed its microgrid's generation capacity
    \item Cooperative mode implements a hierarchical control strategy that prioritizes filling TES4 before activating expensive micro-turbine generation
\end{enumerate}



\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.8\linewidth]{imgs/sci-exp-case/energy/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} In the first image showing independent operation, observe TES4 (subplot h) during Days 3-5: the storage level exhibits significant valleys, dropping to approximately 20-30 kWh multiple times, and rarely maintains the maximum 100 kWh capacity for extended periods. The surface shows irregular topology with frequent charge-discharge cycles.

\textbf{\textcolor{CaseOrange}{Step 3.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.8\linewidth]{imgs/sci-exp-case/energy/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 4.}} In the second image showing cooperative operation, examine TES4 (subplot d) during the same Days 3-5 period: the storage level consistently maintains near-maximum capacity (90-100 kWh) for prolonged periods, particularly during daytime hours (approximately 8h-16h). The surface displays prominent yellow plateaus indicating sustained full capacity.

\textbf{\textcolor{CaseOrange}{Step 5.}} The key difference occurs during daytime hours when solar thermal generation is high. In cooperative mode, microgrids without TES devices can transfer their surplus solar thermal energy to TES4, enabling it to reach and maintain maximum capacity. In independent operation, each microgrid must consume or waste its own solar thermal energy locally, and TES4 can only store energy from its own microgrid's solar panels while also meeting that microgrid's immediate thermal load demands. This fundamental difference in energy sharing capability directly explains why TES4 maintains consistently higher storage levels in cooperative mode, as stated in the paper's analysis that 'the surplus thermal solar power of the microgrid without energy storage can be fully stored by the energy storage of another microgrid via local power exchange.'

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

A

\end{tcolorbox}





\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Information,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/information/1.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/information/2.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Based on the first image and the second image in the document, which statement is completely correct?

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item First image (a) is an SIW filter; First image (j) uses probe array-measured data for reconstruction (2 GHz); Second image (a) assigns 1 to fully metal areas, and (b) shows $|H_\gamma|$ variation.
    \item First image (b) is S-parameters of the coupler (2 GHz); First image (h) uses HFSS data with finite ground plane for reconstruction; Second image (a) assigns 0 to fully dielectric areas, and (c) shows $|H_x|$ variation.
    \item First image (e) is single probe-measured magnetic field (2 GHz); First image (d) uses HFSS data without ground plane for reconstruction; Second image (a) assigns 0 to fully metal areas, and (b) shows $|H_x|$ variation.
    \item First image (i) is probe array-measured magnetic field (1.84 GHz); First image (f) uses HFSS data with ground plane for reconstruction; Second image (a) assigns 1 to partially metal areas, and (c) shows $|H_\gamma|$ variation.
    \item First image (g) is sampled field from HFSS without ground plane (2 GHz); First image (j) reconstructs field 4 mm from the coupler; Second image (a) assigns 0.5 to fully dielectric areas, and (b) shows $|H_\gamma|$ variation.
    \item First image (c) is single probe-measured field (2 GHz); First image (h) reconstructs field 0.5 mm from the coupler; Second image (a) assigns 1 to fully metal areas, and (c) shows $|H_x|$ variation.
    \item First image (b) is S-parameters of the filter (1.84 GHz); First image (f) uses single probe-measured data for reconstruction; Second image (a) assigns 0 to partially dielectric areas, and (b) shows $|H_x|$ variation.
    \item First image (d) uses probe array-measured data for reconstruction (2 GHz); First image (i) is HFSS-simulated field with ground plane; Second image (a) assigns 1 to fully dielectric areas, and (c) shows $|H_\gamma|$ variation.
    \item First image (e) is probe array-measured field (1.84 GHz); First image (j) reconstructs field 0.5 mm from the filter; Second image (a) assigns 0 to fully dielectric areas, and (b) shows $|H_\gamma|$ variation.
    \item First image (g) is sampled field from HFSS with ground plane (2 GHz); First image (d) reconstructs field 4 mm from the coupler; Second image (a) assigns 1 to partially metal areas, and (c) shows $|H_x|$ variation.
\end{enumerate}



\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/information/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/information/2.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 3.}} Extract core features of the first image (structure + frequency + measurement/simulation + reconstruction distance)

\textbf{\textcolor{CaseOrange}{Step 4.}} Structure \& frequency: The first image (a) is an SIW coupler (not filter), and (b) its S-parameters are measured at 2 GHz (not 1.84 GHz, which is the second image’s frequency).

\textbf{\textcolor{CaseOrange}{Step 5.}} Measurement/simulation source: (c)/(g) = HFSS-simulated field: (c) = no ground plane, (g) = with finite ground plane; (e)/(i) = measured field: (e) = single probe, (i) = probe array;

\textbf{\textcolor{CaseOrange}{Step 6.}} Reconstruction distance: All reconstructed fields (d)/(f)/(h)/(j) are 0.5 mm from the coupler; measurement plane distance = 4 mm (not reconstruction distance).

\textbf{\textcolor{CaseOrange}{Step 7.}} Eliminate options with first image errors: Option 1 (a=filter, second image (a)=1 for metal, (b)=$|H_\gamma|$): Structure error + material assignment error + field component error. Option 2 (second image (a)=0 for dielectric, (c)=$|H_x|$): Material assignment error + field component error. Option 4 (i=1.84 GHz, f=HFSS with ground plane, (a)=1 for partial metal): Frequency error + reconstruction source error + material assignment error. Option 5 (g=no ground plane, j=4 mm reconstruction, (a)=0.5 for dielectric): Simulation source error + reconstruction distance error + material assignment error. Option 6 (c=single probe-measured, (a)=1 for metal, (c)=$|H_x|$): Field source error + material assignment error + field component error. Option 7 (b=filter S-parameters, 1.84 GHz, (a)=0 for partial dielectric): Structure/frequency error + material assignment error. Option 8 (d=probe array data, i=HFSS-simulated): Reconstruction source error + field source error. Option 9 (e=probe array-measured, 1.84 GHz, a=filter, (a)=0 for dielectric, (b)=$|H_\gamma|$): Measurement method error + frequency/structure error + material assignment/field component error. Option 10 (d=4 mm reconstruction, (a)=1 for partial metal, (c)=$|H_x|$): Reconstruction distance error + material assignment error + field component error.

\textbf{\textcolor{CaseOrange}{Step 8.}} Extract core features of the second image (material assignment + field components).

\textbf{\textcolor{CaseOrange}{Step 9.}} Material assignment rule: (a) 0 = fully metal-covered, 1 = fully dielectric-covered, 0-1 = partially metal-covered (not reverse or arbitrary values).

\textbf{\textcolor{CaseOrange}{Step 10.}} Field components: (b) = $|H_x|$ variation, (c) = $|H_\gamma|$ variation (not mixed).

\textbf{\textcolor{CaseOrange}{Step 11.}} Verify remaining option 3: First image part: "First image (e) is single probe-measured magnetic field (2 GHz)" → matches (e)=single probe, 2 GHz; "First image (d) uses HFSS data without ground plane for reconstruction" → (d) is reconstructed from (c)=HFSS no ground plane, correct. Second image part: "Second image (a) assigns 0 to fully metal areas" → matches material rule; "Second image (b) shows $|H_x|$ variation" → matches (b)=$|H_x|$, correct. Confirm option 3 is completely correct. All parts of option 3 align with the first image’s structure/frequency/field source/reconstruction rule and the second image’s material assignment/field component definition, with no contradictions.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

C

\end{tcolorbox}





\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Life,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/life/1.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/life/2.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/life/3.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

According to the first image, if one wants to inhibit tumor development by targeting non-tumor cells within the body, which cells should the monoclonal antibody be made against? Using which method from the second image to deliver the antibody can achieve a inhibition of tumor development from a deeper level? Which type in the third image does this method belong to?Please choose from the given options:

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item Siglec-10;A;A
    \item Siglec-10;A;B
    \item Siglec-10;B;A
    \item Siglec-10;B;B
    \item Siglec-10;C;A
    \item CD24;A;A
    \item CD24;A;B
    \item CD24;B;A
    \item CD24;B;B
    \item CD24;C;A
\end{enumerate}



\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/life/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} The proteins identified in the image that can serve as targets are mainly Siglec-10 and CD24.

\textbf{\textcolor{CaseOrange}{Step 3.}} The topic requires starting from non-tumor cells, so Siglec-10 was chosen.

\textbf{\textcolor{CaseOrange}{Step 4.}} 

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/life/2.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 5.}} Identify the three main strategies for NP-mediated CD24-Siglec10 axis-targeted therapy shown in the figure.

\textbf{\textcolor{CaseOrange}{Step 6.}} Among them, strategies A and B both use antibodies to directly block signal transduction on the cell surface, whereas strategy C uses siRNA to inhibit the expression of the target protein at the nucleic acid level.

\textbf{\textcolor{CaseOrange}{Step 7.}} Strategy C is a deeper approach to suppress tumor development.

\textbf{\textcolor{CaseOrange}{Step 8.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/life/3.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 9.}} Identifying two modes of nanoparticle-based drug delivery systems in the image.

\textbf{\textcolor{CaseOrange}{Step 10.}} The surface of the nanomaterials delivering siRNA does not carry antibodies and is passively targeted.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

E

\end{tcolorbox}






\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Material,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/material/1.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Images are Li-ion probability densities in Li-ion conductors. Li-ion probability densities are colored red. Which material does represent the best Li-ion conductivity?

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item Li10GeP2S12
    \item Li7P3S11
    \item Li2S
    \item $\gamma$-Li3PS4
    \item Li4GeS4
    \item Li3.25Ge0.25P0.75S4
    \item Li2S-P2S5
    \item Li10SnP2S12
    \item Li10SiP2S12
    \item Li6PS5Cl
\end{enumerate}



\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/material/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} Find the Li-ion probability densities of materials in the figure.

\textbf{\textcolor{CaseOrange}{Step 3.}} Determine the largest region of the Li-ion probability densities. The answer is Li10GeP2S12.

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

A

\end{tcolorbox}






\begin{tcolorbox}[
    breakable,
    title=Example of Scientific Experimental Reasoning in Neuroscience,
    colback=LighterGray,
    colframe=DeepPurple,
    colbacktitle=DeepPurple,
    coltitle=White,
]

\textbf{\emph{\textcolor{DeepPurple}{Images}}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/neuroscience/1.png}
\end{center}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.5\linewidth]{imgs/sci-exp-case/neuroscience/2.png}
\end{center}

\textbf{\emph{\textcolor{DeepPurple}{Question}}}

Please answer based on the first image: How many peaks exceeding 20 appeared in the first 60 timesteps of the Large initialization for Signal 2 response in each of the two examples?Based on the second image, after training, does Branch 1 with a small initialization increase (+) or decrease (-), and does Branch 2 with a large initialization increase (+) or decrease (-)?

\textbf{\emph{\textcolor{DeepPurple}{Options}}}

\begin{enumerate}[label=\Alph*.]
    \item 1,3;+-
    \item 0,0;++
    \item 1,2;--
    \item 1,1;++
    \item 1,3;-+
    \item 2,1;--
    \item 2,2;-+
    \item 3,1;-+
    \item 3,2;+-
    \item 0,3;-+
\end{enumerate}



\textbf{\emph{\textcolor{DeepPurple}{Steps}}}

\textbf{\textcolor{CaseOrange}{Step 1.}}

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/neuroscience/1.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 2.}} Realize: Signal 2 response is blue line.

\textbf{\textcolor{CaseOrange}{Step 3.}} Define the counting range: Large initialization, Neuron state $> 20$,Timestep $< 60$, in each of the two examples.

\textbf{\textcolor{CaseOrange}{Step 4.}} Find out that there is 1 in example1 and 3 in example2. Answer: 1,3.

\textbf{\textcolor{CaseOrange}{Step 5.}} 

\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=0.98\linewidth]{imgs/sci-exp-case/neuroscience/2.png}
\end{center}

\textbf{\textcolor{CaseOrange}{Step 6.}} Branch 1 small init: the KDE line and histogram show a shift. Before training, Branch 1 small init was lower around 0-0.2, after training, it's higher around 0.8-1.0, so increase (+)

\textbf{\textcolor{CaseOrange}{Step 7.}} Branch 2 large init: before training, it was a peak around 1.0, after training, the density decreases there, so decrease (-).

\textbf{\textcolor{CaseOrange}{Step 8.}} Conclude:1,3;+-

\textbf{\emph{\textcolor{DeepPurple}{Answer}}}

A

\end{tcolorbox}






\subsection{Supplementary Evaluation Results}

\begin{figure}[ht]
% \vspace{-0.5em}
\centerline
{\includegraphics[width=16cm]{paper/imgs/LLMs_Agents_subject_metric.png}}
\caption{\textbf{Scientific Deep Research Across Subjects}: Combined subject-wise performance of LLMs and agents on deep research tasks.}
\label{fig: deep_research_subject}
% \vspace{-2em}
\end{figure}

\begin{figure}[ht]
% \vspace{-0.5em}
\centerline
{\includegraphics[width=8cm]{paper/imgs/idea_subject_metric.png}}
\caption{\textbf{Idea Generation Across Subjects}: Subject-wise scores for idea generation.}
\label{fig: idea_subject}
% \vspace{-2em}
\end{figure}

\begin{figure}[ht]
% \vspace{-0.5em}
\centerline
{\includegraphics[width=8cm]{paper/imgs/wet_subject_metric.png}}
\caption{\textbf{Wet Experiment Across Subjects}: Subject-wise Action Sequence Similarity (SS) and Parameter Accuracy (PA) performance in wet experiments.}
\label{fig: wet_exp_subject}
% \vspace{-2em}
\end{figure}

% ==================== Tables from data.py (Exact Data; excluding GPT-5.1-Pro) ====================

% 1) LLMs Deep Research Task Metric
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{14cm}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Properties} & \textbf{Micro-experiments} & \textbf{Macro-experiments} & \textbf{Data} \\
\midrule
DeepSeek-V3.2 & 6.62 & 21.57 & 15.38 & 9.8 \\
DeepSeek-R1 & 10.61 & 23.47 & 15.38 & 10.0 \\
Intern-S1 & 7.14 & 24.64 & 20.0 & 12.12 \\
Intern-S1-mini & 5.88 & 19.18 & 17.39 & 5.41 \\
Kimi-k2 & 8.09 & 20.21 & 20.0 & 10.0 \\
Qwen3-VL-235B-A22B & 7.3 & 19.19 & 12.5 & 10.2 \\
Qwen3-235B-A22B & 11.94 & 23.75 & 11.54 & 6.12 \\
Qwen3-Max & 7.0 & 30.0 & 0.0 & 13.79 \\
Qwen3-8B & 5.84 & 14.42 & 3.85 & 3.92 \\
Llama-4-Scout & 5.11 & 14.42 & 3.85 & 3.92 \\
GPT-4o & 5.84 & 12.5 & 7.69 & 3.92 \\
GPT-4.1 & 7.3 & 17.31 & 15.38 & 7.84 \\
GPT-5 & 10.22 & 21.15 & 26.92 & 5.88 \\
GPT-5.1 & 8.03 & 18.27 & 15.38 & 5.88 \\
o3 & 10.95 & 17.31 & 19.23 & 5.88 \\
o4-mini & 8.76 & 18.27 & 11.54 & 7.84 \\
Gemini-2.5-Flash & 9.49 & 16.35 & 11.54 & 1.96 \\
Gemini-2.5-Pro & 11.68 & 23.08 & 15.38 & 7.84 \\
Gemini-3-Pro & 15.00 & 26.14 & 22.73 & 10.87 \\
Claude-Opus-4.1 & 8.82 & 20.19 & 15.38 & 7.84 \\
Claude-Sonnet-4.5 & 8.03 & 23.08 & 15.38 & 9.8 \\
Grok-3 & 9.49 & 20.19 & 11.54 & 11.76 \\
Grok-4 & 10.37 & 21.65 & 15.38 & 4.0 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Deep Research Task Metrics (LLMs)}: Category-wise scores across Properties, Micro/Macro-Experiments, and Data.}
\label{tab:llms_deep_research_task_metric}
\end{table}

% 2) Agents Deep Research Task Metric
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{16cm}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Agent} & \textbf{Properties} & \textbf{Micro-experiments} & \textbf{Macro-experiments} & \textbf{Data} \\
\midrule
SmolAgents(GPT-4.1) & 13.87 & 16.35 & 26.92 & 5.88 \\
SmolAgents(Gemini-2.5-Flash) & 12.41 & 24.04 & 26.92 & 11.76 \\
Owl(GPT-4.1) & 6.57 & 18.27 & 19.23 & 9.8 \\
Owl(Gemini-2.5-Flash) & 6.61 & 14.29 & 9.52 & 8.33 \\
WebThinker & 13.87 & 18.27 & 26.92 & 3.92 \\
XMaster & 13.14 & 17.31 & 19.23 & 5.88 \\
InternAgent & 13.24 & 24.04 & 26.92 & 9.8 \\
OpenAI Deep Research(o3) & 16.06 & 14.42 & 11.54 & 9.8 \\
OpenAI Deep Research(o4-mini) & 14.6 & 22.12 & 19.23 & 11.76 \\
Grok-Search(Grok-4) & 14.18 & 22.73 & 19.23 & 11.76 \\
Kimi-Search(Kimi-k2) & 9.49 & 22.92 & 11.54 & 14.0 \\
Doubao-Search(Seed-1-6) & 7.35 & 16.5 & 0.0 & 3.92 \\
Perplexity(Sonar-Pro) & 6.57 & 21.15 & 19.23 & 3.92 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Deep Research Task Metrics (Agents)}: Category-wise scores across Properties, Micro/Macro-Experiments, and Data.}
\label{tab:agents_deep_research_task_metric}
\end{table}

% 3) Dry Experiment Function Categories (LLMs)
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Numerical Calculation} & \textbf{Statistical Analysis} & \textbf{Simulation} & \textbf{Metric Calculation} & \textbf{Data Processing} & \textbf{Predictive Modeling} \\
\midrule
DeepSeek-V3.2 & 19.3 & 19.05 & 26.32 & 35.71 & 42.86 & 27.27 \\
DeepSeek-R1 & 31.76 & 23.81 & 26.32 & 39.29 & 47.62 & 45.45 \\
Intern-S1 & 25.61 & 28.57 & 26.32 & 39.29 & 42.86 & 27.27 \\
Intern-S1-mini & 14.62 & 19.05 & 15.79 & 25.00 & 28.57 & 9.09 \\
Kimi-k2 & 26.9 & 23.81 & 31.58 & 35.71 & 52.38 & 18.18 \\
Qwen3-VL-235B-A22B & 25.15 & 23.81 & 26.32 & 39.29 & 47.62 & 27.27 \\
Qwen3-235B-A22B & 25.29 & 28.57 & 31.58 & 35.71 & 47.62 & 27.27 \\
Qwen3-Max & 29.24 & 38.1 & 31.58 & 39.29 & 47.62 & 45.45 \\
Qwen3-8B & 18.71 & 14.29 & 15.79 & 25.0 & 23.81 & 0.0 \\
Llama-4-Scout & 20.59 & 19.05 & 15.79 & 21.43 & 23.81 & 18.18 \\
GPT-4o & 25.15 & 23.81 & 26.32 & 32.14 & 38.1 & 27.27 \\
GPT-4.1 & 32.75 & 38.1 & 26.32 & 39.29 & 47.62 & 27.27 \\
GPT-5 & 25.73 & 28.57 & 31.58 & 39.29 & 52.38 & 27.27 \\
GPT-5.1 & 29.24 & 23.81 & 26.32 & 42.86 & 42.86 & 27.27 \\
o3 & 28.65 & 42.86 & 26.32 & 42.86 & 38.1 & 27.27 \\
o4-mini & 35.09 & 28.57 & 26.32 & 39.29 & 52.38 & 36.36 \\
Gemini-2.5-Flash & 16.96 & 23.81 & 21.05 & 32.14 & 38.1 & 18.18 \\
Gemini-2.5-Pro & 19.3 & 23.81 & 21.05 & 21.43 & 42.86 & 36.36 \\
Gemini-3-Pro & 33.53 & 33.33 & 35.29 & 46.43 & 50.00 & 45.45 \\
Claude-Opus-4.1 & 30.99 & 28.57 & 31.58 & 53.57 & 47.62 & 36.36 \\
Claude-Sonnet-4.5 & 33.33 & 38.1 & 26.32 & 42.86 & 47.62 & 45.45 \\
Grok-3 & 22.81 & 33.33 & 31.58 & 35.71 & 47.62 & 18.18 \\
Grok-4 & 32.12 & 19.05 & 31.58 & 40.74 & 42.86 & 54.55 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Dry Experiment Function Categories}: Completion scores across six function types.}
\label{tab:dry_task_metric_table}
\end{table}

% 4) MCP Task Metric (Reasoning Paradigms)
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Signal Perception} & \textbf{Attribute Understanding} & \textbf{Comparative Reasoning} & \textbf{Causal Reasoning} \\
\midrule
Intern-S1 & 39.29 & 21.88 & 28.57 & 37.50 \\
Intern-S1-mini & 17.86 & 10.94 & 18.29 & 20.83 \\
Qwen3-VL-235B-A22B & 32.14 & 26.56 & 32.00 & 41.67 \\
Qwen3-VL-Max & 50.00 & 34.38 & 36.57 & 41.67 \\
Qwen3-VL-8B & 21.43 & 21.88 & 23.43 & 29.17 \\
Llama-4-Scout & 28.57 & 17.19 & 28.57 & 25.00 \\
GPT-4o & 39.29 & 26.56 & 33.71 & 29.17 \\
GPT-4.1 & 46.43 & 40.62 & 34.29 & 54.10 \\
GPT-5 & 53.57 & 32.81 & 37.71 & 37.50 \\
GPT-5.1 & 21.43 & 25.00 & 36.57 & 54.17 \\
o3 & 35.71 & 26.56 & 33.14 & 41.67 \\
o4-mini & 39.29 & 35.94 & 30.29 & 41.67 \\
Gemini-2.5-Flash & 35.71 & 37.50 & 30.29 & 54.17 \\
Gemini-2.5-Pro & 50.00 & 42.19 & 38.29 & 50.00 \\
Gemini-3-Pro & 50.00 & 40.62 & 42.86 & 29.17 \\
Claude-Opus-4.1 & 53.57 & 35.94 & 34.86 & 58.33 \\
Claude-Sonnet-4.5 & 35.71 & 35.94 & 38.86 & 37.50 \\
Grok-4 & 42.86 & 26.56 & 28.00 & 41.67 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Experimental Reasoning by Type (Multi-choice Accuracy)}: Scores across signal, attribute, comparative, and causal reasoning.}
\label{tab:mcp_task_metric_table}
\end{table}

% 5) LLMs Deep Research Subject Metric
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Model} & \textbf{Astronomy} & \textbf{Chemistry} & \textbf{Earth} & \textbf{Energy} & \textbf{Information} & \textbf{Life} & \textbf{Material} & \textbf{Math} & \textbf{Neuroscience} & \textbf{Physics} \\
\midrule
DeepSeek-V3.2 & 11.76 & 10.0 & 20.75 & 0.0 & 10.47 & 0.0 & 7.89 & 44.0 & 0.0 & 3.12 \\
DeepSeek-R1 & 6.25 & 9.09 & 24.0 & 0.0 & 16.67 & 0.0 & 7.89 & 52.0 & 4.17 & 6.67 \\
Intern-S1 & 0.0 & 20.0 & 22.45 & 0.0 & 12.5 & 8.0 & 0.0 & 47.62 & 0.0 & 0.0 \\
Intern-S1-mini & 0.0 & 9.09 & 23.26 & 0.0 & 7.14 & 6.25 & 7.14 & 61.54 & 0.0 & 0.0 \\
Kimi-k2 & 5.88 & 10.0 & 27.08 & 0.0 & 5.26 & 9.3 & 15.79 & 43.48 & 0.0 & 0.0 \\
Qwen3-VL-235B-A22B & 5.88 & 10.0 & 19.61 & 0.0 & 16.67 & 9.41 & 5.26 & 40.0 & 0.0 & 6.25 \\
Qwen3-235B-A22B & 5.88 & 20.0 & 20.83 & 0.0 & 16.67 & 13.1 & 10.53 & 77.78 & 0.0 & 9.38 \\
Qwen3-Max & 11.11 & 0.0 & 33.33 & 0.0 & 11.11 & 16.28 & 7.89 & 44.0 & 4.17 & 3.12 \\
Qwen3-8B & 11.76 & 0.0 & 11.11 & 0.0 & 10.0 & 5.75 & 7.89 & 32.0 & 0.0 & 0.0 \\
Llama-4-Scout & 11.76 & 9.09 & 9.26 & 0.0 & 10.0 & 6.9 & 5.26 & 20.0 & 4.17 & 3.12 \\
GPT-4o & 5.88 & 9.09 & 7.41 & 0.0 & 10.0 & 4.6 & 15.79 & 24.0 & 4.17 & 0.0 \\
GPT-4.1 & 23.53 & 9.09 & 12.96 & 0.0 & 5.0 & 9.2 & 5.26 & 44.0 & 8.33 & 0.0 \\
GPT-5 & 5.88 & 9.09 & 27.78 & 0.0 & 10.0 & 9.2 & 13.16 & 52.0 & 0.0 & 3.12 \\
GPT-5.1 & 17.65 & 9.09 & 18.52 & 10.0 & 5.0 & 9.2 & 2.63 & 36.0 & 8.33 & 3.12 \\
o3 & 5.88 & 18.18 & 22.22 & 0.0 & 10.0 & 9.2 & 7.89 & 44.0 & 4.17 & 3.12 \\
o4-mini & 5.88 & 18.18 & 16.67 & 0.0 & 0.0 & 9.2 & 13.16 & 48.0 & 0.0 & 3.12 \\
Gemini-2.5-Flash & 5.88 & 9.09 & 14.81 & 0.0 & 10.0 & 8.05 & 5.26 & 40.0 & 4.17 & 6.25 \\
Gemini-2.5-Pro & 17.65 & 9.09 & 18.52 & 10.0 & 10.0 & 12.64 & 10.53 & 52.0 & 4.17 & 6.25 \\
Gemini-3-Pro & 12.5 & 14.29 & 27.66 & 0.0 & 35.29 & 12.0 & 17.86 & 50.0 & 4.76 & 6.25 \\
Claude-Opus-4.1 & 11.76 & 9.09 & 22.22 & 0.0 & 10.0 & 9.3 & 7.89 & 40.0 & 4.17 & 6.25 \\
Claude-Sonnet-4.5 & 17.65 & 9.09 & 20.37 & 10.0 & 15.0 & 11.49 & 5.26 & 36.0 & 8.33 & 6.25 \\
Grok-3 & 5.88 & 9.09 & 22.22 & 10.0 & 5.0 & 11.49 & 13.16 & 40.0 & 4.17 & 3.12 \\
Grok-4 & 5.88 & 9.09 & 18.37 & 10.0 & 16.67 & 10.47 & 10.53 & 45.83 & 0.0 & 6.45 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Deep Research Across Subjects (LLMs)}: Subject-wise scores across ten scientific domains.}
\label{tab:llms_deep_research_subject_metric_table}
\end{table}

% 6) Agents Deep Research Subject Metric
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Agent} & \textbf{Astronomy} & \textbf{Chemistry} & \textbf{Earth} & \textbf{Energy} & \textbf{Information} & \textbf{Life} & \textbf{Material} & \textbf{Math} & \textbf{Neuroscience} & \textbf{Physics} \\
\midrule
SmolAgents(GPT-4.1) & 29.41 & 9.09 & 27.78 & 0.0 & 10.0 & 9.2 & 15.79 & 28.0 & 4.17 & 3.12 \\
SmolAgents(Gemini-2.5-Flash) & 23.53 & 9.09 & 33.33 & 0.0 & 25.0 & 11.49 & 10.53 & 44.0 & 4.17 & 3.12 \\
Owl(GPT-4.1) & 23.53 & 9.09 & 18.52 & 0.0 & 10.0 & 6.9 & 7.89 & 44.0 & 4.17 & 0.0 \\
Owl(Gemini-2.5-Flash) & 6.25 & 10.0 & 15.79 & 0.0 & 5.56 & 11.54 & 0.0 & 41.67 & 8.33 & 0.0 \\
WebThinker & 5.88 & 9.09 & 27.78 & 0.0 & 15.0 & 6.9 & 23.68 & 36.0 & 4.17 & 6.25 \\
XMaster & 11.76 & 9.09 & 25.93 & 0.0 & 15.0 & 6.9 & 10.53 & 44.0 & 0.0 & 9.38 \\
InternAgent & 29.41 & 9.09 & 26.42 & 10.0 & 25.0 & 11.49 & 10.53 & 52.0 & 0.0 & 6.25 \\
OpenAI Deep Research(o3) & 11.76 & 9.09 & 20.37 & 10.0 & 15.0 & 12.64 & 13.16 & 20.0 & 16.67 & 6.25 \\
OpenAI Deep Research(o4-mini) & 5.88 & 18.18 & 24.07 & 10.0 & 25.0 & 12.64 & 21.05 & 40.0 & 12.5 & 0.0 \\
Grok-Search(Grok-4) & 17.65 & 20.0 & 26.92 & 0.0 & 15.79 & 13.95 & 7.89 & 75.0 & 4.17 & 9.38 \\
Kimi-Search(Kimi-k2) & 11.76 & 10.0 & 22.45 & 0.0 & 15.0 & 13.95 & 10.53 & 45.83 & 0.0 & 3.12 \\
Doubao-Search(Seed-1-6) & 17.65 & 9.09 & 9.43 & 0.0 & 15.0 & 4.65 & 8.11 & 32.0 & 0.0 & 6.25 \\
Perplexity(Sonar-Pro) & 11.76 & 9.09 & 16.67 & 0.0 & 15.0 & 6.9 & 15.79 & 40.0 & 4.17 & 0.0 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Deep Research Across Subjects (Agents)}: Subject-wise scores across ten scientific domains.}
\label{tab:agents_deep_research_subject_metric_table}
\end{table}

% 7) Idea Subject Metric (LLMs)
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Model} & \textbf{Astronomy} & \textbf{Chemistry} & \textbf{Earth} & \textbf{Energy} & \textbf{Information} & \textbf{Life} & \textbf{Material} & \textbf{Math} & \textbf{Neuroscience} & \textbf{Physics} \\
\midrule
DeepSeek-V3.2 & 38.38 & 24.49 & 35.85 & 34.24 & 32.09 & 32.11 & 31.77 & 29.46 & 29.53 & 34.20 \\
DeepSeek-R1 & 35.76 & 31.63 & 37.89 & 37.73 & 36.26 & 35.96 & 36.47 & 35.88 & 33.88 & 37.21 \\
Intern-S1 & 37.53 & 28.20 & 36.22 & 36.07 & 33.30 & 34.38 & 32.15 & 27.00 & 30.07 & 33.46 \\
Intern-S1-mini & 36.49 & 24.77 & 35.00 & 33.68 & 34.21 & 32.80 & 26.96 & 29.16 & 31.91 & 34.02 \\
Kimi-k2 & 44.80 & 36.44 & 42.99 & 44.80 & 37.48 & 39.78 & 44.86 & 36.58 & 38.43 & 43.59 \\
Qwen3-VL-235B-A22B & 36.00 & 30.06 & 37.90 & 40.09 & 31.62 & 35.28 & 35.59 & 30.56 & 32.18 & 35.31 \\
Qwen3-235B-A22B & 37.38 & 31.02 & 36.78 & 41.24 & 35.25 & 35.98 & 35.34 & 31.06 & 32.46 & 36.52 \\
Qwen3-Max & 39.80 & 30.28 & 37.74 & 40.56 & 33.12 & 35.42 & 34.98 & 30.12 & 30.31 & 34.54 \\
Qwen3-8B & 34.25 & 22.91 & 33.78 & 30.72 & 30.35 & 30.26 & 29.80 & 27.42 & 26.20 & 32.05 \\
Llama-4-Scout & 28.65 & 22.50 & 27.79 & 26.10 & 30.47 & 25.62 & 26.14 & 25.26 & 24.94 & 29.65 \\
GPT-4o & 31.27 & 24.79 & 30.50 & 31.70 & 29.19 & 26.17 & 26.83 & 25.86 & 25.72 & 30.77 \\
GPT-4.1 & 32.20 & 26.40 & 33.79 & 32.64 & 31.15 & 29.28 & 32.30 & 27.99 & 25.37 & 32.78 \\
GPT-5 & 52.37 & 54.12 & 56.01 & 64.53 & 48.58 & 50.25 & 54.82 & 50.99 & 47.46 & 56.55 \\
GPT-5.1 & 44.34 & 46.56 & 44.50 & 53.35 & 38.24 & 39.80 & 41.00 & 36.49 & 38.61 & 43.61 \\
o3 & 42.57 & 38.83 & 44.58 & 50.85 & 38.35 & 40.77 & 45.42 & 40.36 & 38.43 & 44.50 \\
o4-mini & 37.74 & 29.78 & 39.14 & 38.08 & 34.79 & 36.63 & 37.86 & 36.86 & 32.42 & 38.78 \\
Gemini-2.5-Flash & 37.32 & 27.61 & 36.42 & 35.33 & 32.59 & 33.06 & 33.34 & 27.42 & 29.51 & 34.93 \\
Gemini-2.5-Pro & 38.64 & 27.22 & 37.10 & 46.00 & 34.39 & 35.12 & 36.93 & 31.00 & 31.12 & 36.28 \\
Gemini-3-Pro & 39.51 & 35.97 & 37.17 & 40.49 & 34.14 & 35.35 & 35.49 & 30.03 & 32.14 & 35.18 \\
Claude-Opus-4.1 & 39.85 & 28.89 & 38.19 & 38.83 & 35.19 & 36.85 & 38.39 & 35.69 & 33.66 & 37.44 \\
Claude-Sonnet-4.5 & 42.11 & 34.89 & 42.38 & 44.20 & 35.24 & 37.31 & 38.14 & 34.44 & 32.13 & 40.90 \\
Grok-3 & 29.66 & 23.40 & 31.10 & 25.66 & 31.04 & 30.11 & 27.29 & 26.01 & 26.43 & 33.26 \\
Grok-4 & 33.75 & 25.48 & 33.78 & 35.22 & 30.44 & 30.96 & 30.30 & 27.54 & 27.58 & 33.61 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Idea Generation Across Subjects}: Subject-wise scores.}
\label{tab:idea_subject_metric_table}
\end{table}

% 8) Dry Subject Metric (LLMs)
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Model} & \textbf{Astronomy} & \textbf{Chemistry} & \textbf{Earth} & \textbf{Energy} & \textbf{Information} & \textbf{Life} & \textbf{Material} & \textbf{Math} & \textbf{Neuroscience} & \textbf{Physics} \\
\midrule
DeepSeek-V3.2 & 31.25 & 0.0 & 20.83 & 10.0 & 14.29 & 27.5 & 44.44 & 29.41 & 16.67 & 17.24 \\
DeepSeek-R1 & 37.5 & 20.0 & 33.33 & 10.0 & 35.71 & 33.75 & 55.56 & 29.41 & 29.17 & 24.14 \\
Intern-S1 & 37.5 & 0.0 & 25.0 & 10.0 & 28.57 & 33.75 & 48.15 & 18.18 & 16.67 & 24.14 \\
Intern-S1-mini & 12.5 & 0.0 & 18.75 & 10.0 & 14.29 & 21.25 & 33.33 & 0.0 & 4.17 & 17.24 \\
Kimi-k2 & 43.75 & 0.0 & 22.92 & 20.0 & 21.43 & 33.75 & 44.44 & 16.67 & 20.83 & 34.48 \\
Qwen3-VL-235B-A22B & 37.5 & 0.0 & 29.17 & 10.0 & 14.29 & 35.0 & 40.74 & 16.67 & 20.83 & 24.14 \\
Qwen3-235B-A22B & 31.25 & 0.0 & 25.0 & 30.0 & 14.29 & 35.0 & 44.44 & 17.65 & 20.83 & 27.59 \\
Qwen3-Max & 50.0 & 0.0 & 31.25 & 30.0 & 28.57 & 37.5 & 48.15 & 22.22 & 25.0 & 24.14 \\
Qwen3-8B & 25.0 & 0.0 & 18.75 & 10.0 & 7.14 & 20.0 & 33.33 & 5.56 & 12.5 & 20.69 \\
Llama-4-Scout & 18.75 & 0.0 & 18.75 & 10.0 & 14.29 & 25.0 & 33.33 & 17.65 & 12.5 & 17.24 \\
GPT-4o & 37.5 & 0.0 & 27.08 & 10.0 & 14.29 & 35.0 & 51.85 & 22.22 & 20.83 & 20.69 \\
GPT-4.1 & 43.75 & 20.0 & 33.33 & 40.0 & 28.57 & 33.75 & 48.15 & 27.78 & 29.17 & 34.48 \\
GPT-5 & 37.5 & 0.0 & 27.08 & 40.0 & 35.71 & 31.25 & 40.74 & 22.22 & 20.83 & 27.59 \\
GPT-5.1 & 31.25 & 0.0 & 27.08 & 30.0 & 28.57 & 38.75 & 44.44 & 22.22 & 12.5 & 31.03 \\
o3 & 37.5 & 0.0 & 33.33 & 10.0 & 28.57 & 35.0 & 51.85 & 22.22 & 20.83 & 27.59 \\
o4-mini & 37.5 & 0.0 & 33.33 & 20.0 & 28.57 & 40.0 & 51.85 & 22.22 & 37.5 & 34.48 \\
Gemini-2.5-Flash & 18.75 & 0.0 & 18.75 & 10.0 & 14.29 & 23.75 & 37.04 & 27.78 & 16.67 & 13.79 \\
Gemini-2.5-Pro & 25.0 & 0.0 & 18.75 & 0.0 & 21.43 & 25.0 & 33.33 & 22.22 & 16.67 & 27.59 \\
Gemini-3-Pro & 37.5 & 0.0 & 32.61 & 30.0 & 38.46 & 38.46 & 55.56 & 37.5 & 34.78 & 28.57 \\
Claude-Opus-4.1 & 43.75 & 20.0 & 33.33 & 40.0 & 28.57 & 33.75 & 48.15 & 27.78 & 29.17 & 34.48 \\
Claude-Sonnet-4.5 & 43.75 & 20.0 & 35.42 & 30.0 & 21.43 & 41.25 & 51.85 & 27.78 & 25.0 & 27.59 \\
grok-3 & 31.25 & 0.0 & 29.17 & 20.0 & 14.29 & 32.5 & 40.74 & 11.11 & 20.83 & 24.14 \\
Grok-4 & 37.5 & 20.0 & 27.66 & 20.0 & 30.77 & 37.97 & 51.85 & 43.75 & 25.0 & 22.22 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Dry Experiment Across Subjects}: Subject-wise scores.}
\label{tab:dry_subject_metric_table2}
\end{table}

% 9) Wet Subject Metric
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{14cm}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Life-SS} & \textbf{Material-SS} & \textbf{Physics-SS} & \textbf{Life-PA} & \textbf{Material-PA} & \textbf{Physics-PA} \\
\midrule
DeepSeek-V3.2 & 15.47 & 20.2 & 16.67 & 21.48 & 26.87 & 22.5 \\
DeepSeek-R1 & 10.0 & 21.83 & 16.67 & 23.14 & 26.32 & 39.67 \\
Intern-S1 & 24.86 & 27.23 & 33.33 & 23.62 & 34.87 & 50.9 \\
Intern-S1-mini & 14.15 & 4.67 & 0.0 & 18.02 & 16.04 & 9.58 \\
Kimi-k2 & 18.1 & 28.04 & 0.0 & 25.13 & 37.42 & 18.61 \\
Qwen3-VL-235B-A22B & 17.78 & 30.25 & 16.67 & 28.66 & 43.11 & 46.68 \\
Qwen3-235B-A22B & 17.11 & 30.8 & 0.0 & 22.87 & 37.71 & 33.18 \\
Qwen3-Max & 17.37 & 41.11 & 33.33 & 24.44 & 45.67 & 56.7 \\
Qwen3-8B & 4.99 & 15.54 & 0.0 & 5.81 & 15.49 & 6.25 \\
Llama-4-Scout & 15.72 & 18.75 & 16.67 & 20.53 & 32.86 & 17.78 \\
GPT-4o & 20.79 & 29.1 & 32.38 & 31.58 & 41.06 & 41.41 \\
GPT-4.1 & 32.13 & 33.02 & 33.33 & 33.11 & 45.06 & 54.47 \\
GPT-5 & 7.81 & 11.72 & 33.76 & 19.31 & 21.5 & 23.18 \\
GPT-5.1 & 12.38 & 21.44 & 29.3 & 24.24 & 28.0 & 40.14 \\
o3 & 27.43 & 22.79 & 44.86 & 30.63 & 32.87 & 48.92 \\
o4-mini & 31.46 & 24.01 & 16.67 & 25.76 & 35.78 & 32.7 \\
Gemini-2.5-Flash & 5.31 & 23.44 & 15.71 & 14.73 & 28.09 & 32.03 \\
Gemini-2.5-Pro & 16.9 & 21.02 & 12.06 & 24.52 & 27.28 & 23.03 \\
Gemini-3-Pro & 20.00 & 34.88 & 33.33 & 32.21 & 41.07 & 36.12 \\
Claude-Opus-4.1 & 16.65 & 25.74 & 29.21 & 20.63 & 33.45 & 43.9 \\
Claude-Sonnet-4.5 & 31.75 & 25.83 & 16.67 & 28.62 & 33.78 & 46.97 \\
grok-3 & 28.97 & 41.93 & 33.33 & 32.52 & 43.94 & 58.32 \\
Grok-4 & 27.29 & 29.1 & 16.67 & 25.19 & 37.35 & 23.09 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Wet Experiment Across Subjects}: Scores across Action Sequence Similarity (SS) and Parameter Accuracy (PA) categories.}
\label{tab:wet_subject_metric_table}
\end{table}

% 10) MCP Subject Metric (Final Active Block)
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{2pt}
\tiny
\resizebox{17cm}{!}{
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{Astronomy} & \textbf{Chemistry} & \textbf{Earth} & \textbf{Energy} & \textbf{Information} & \textbf{Life} & \textbf{Material} & \textbf{Neuroscience} & \textbf{Physics} \\
\midrule
Intern-S1 & 47.06 & 27.27 & 27.78 & 40.00 & 25.00 & 29.41 & 26.67 & 33.33 & 16.00 \\
Intern-S1-mini & 23.53 & 27.27 & 18.52 & 30.00 & 10.00 & 18.82 & 8.89 & 12.50 & 16.00 \\
Qwen3-VL-235B-A22B & 58.82 & 36.36 & 31.48 & 50.00 & 15.00 & 29.41 & 31.11 & 33.33 & 24.00 \\
Qwen3-VL-Max & 52.94 & 36.36 & 31.48 & 50.00 & 35.00 & 41.18 & 40.00 & 37.50 & 24.00 \\
Qwen3-VL-8B & 29.41 & 36.36 & 24.07 & 60.00 & 20.00 & 25.88 & 13.33 & 16.67 & 16.00 \\
Llama-4-Scout & 41.18 & 27.27 & 27.78 & 30.00 & 30.00 & 23.53 & 31.11 & 20.83 & 0.80 \\
GPT-4o & 41.18 & 54.55 & 37.04 & 60.00 & 20.00 & 29.41 & 31.11 & 20.83 & 28.00 \\
GPT-4.1 & 35.29 & 36.36 & 37.04 & 60.00 & 45.00 & 42.35 & 37.78 & 33.33 & 24.00 \\
GPT-5 & 70.59 & 36.36 & 37.04 & 30.00 & 50.00 & 37.65 & 33.33 & 41.67 & 20.00 \\
GPT-5.1 & 47.06 & 45.45 & 33.33 & 40.00 & 35.00 & 31.76 & 42.22 & 16.67 & 28.00 \\
o3 & 58.82 & 45.45 & 29.63 & 50.00 & 35.00 & 29.41 & 24.44 & 50.00 & 16.00 \\
o4-mini & 64.71 & 45.45 & 31.48 & 30.00 & 25.00 & 34.12 & 26.67 & 33.33 & 28.00 \\
Gemini-2.5-Flash & 52.94 & 27.27 & 33.33 & 40.00 & 45.00 & 36.47 & 24.44 & 37.50 & 24.00 \\
Gemini-2.5-Pro & 52.94 & 36.36 & 38.89 & 30.00 & 50.00 & 38.82 & 37.78 & 58.33 & 36.00 \\
Gemini-3-Pro & 47.06 & 45.45 & 35.19 & 50.00 & 45.00 & 40.00 & 48.89 & 62.50 & 20.00 \\
Claude-Opus-4.1 & 58.82 & 45.45 & 25.93 & 60.00 & 40.00 & 37.65 & 33.33 & 50.00 & 44.00 \\
Claude-Sonnet-4.5 & 52.94 & 36.36 & 37.04 & 20.00 & 40.00 & 35.29 & 42.22 & 29.17 & 44.00 \\
Grok-4 & 52.94 & 18.18 & 33.33 & 40.00 & 35.00 & 30.59 & 26.67 & 20.83 & 20.00 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Experimental Reasoning Across Subjects (Multi-choice Accuracy)}: Subject-wise scores across 10 scientific disciplines.}
\label{tab:mcp_subject_metric_table}
\end{table}