\section{Related Work}
\label{sec:related_works}

With the rapid advancement of Large Language Models (LLMs) and multi-agent systems in scientific reasoning, numerous datasets have emerged to evaluate their capabilities across various scientific domains.

\subsection{Benchmarks in Different Disciplines}
A significant portion of existing benchmarks focuses on specific disciplines. 
In the \textbf{physical sciences}, PhyBench~\cite{qiu2025phybenchholisticevaluationphysical} examines multi-step reasoning and expression capabilities through original physics problems, while PHYX~\cite{shen2025phyxdoesmodelwits} focuses on real-world scenarios to assess physical reasoning and visual understanding. Additionally, PHYSICS~\cite{feng2025physicsbenchmarkingfoundationmodels} tests models using open-ended, university-level problems.To further address multimodal challenges, PhysUniBench~\cite{wang2025physunibenchundergraduatelevelphysicsreasoning} introduces a large-scale benchmark for undergraduate-level physics, specifically targeting the interpretation of physical diagrams and multi-step reasoning.
In \textbf{chemistry}, ChemBench~\cite{mirza2024largelanguagemodelssuperhuman} provides domain-specific data for systematic evaluation, whereas ChemMLLM~\cite{tan2025chemmllmchemicalmultimodallarge} extends this to multimodal assessment. More granular tasks are covered by benchmarks like ChemSafetyBench~\cite{zhao2024chemsafetybenchbenchmarkingllmsafety} and SpectrumWorld~\cite{yang2025spectrumworldartificialintelligencefoundation}. 
In \textbf{life sciences}, benchmarks range from the molecular level, such as DeepSEA~\cite{kathail2025leveraginggenomicdeeplearning} and GenomicsLong-Range~\cite{anonymous2024the}, to healthcare applications like BioASQ~\cite{Krithara2023} and VQA-RAD~\cite{Lau2018}, as well as agricultural applications like SeedBench~\cite{ying2025seedbenchmultitaskbenchmarkevaluating} and neuroscience with BrainBench~\cite{Luo_2024}. 
For \textbf{earth sciences}, OmniEarth-Bench~\cite{wang2025omniearthbenchholisticevaluationearths} covers a comprehensive range of fields with cross-domain tasks, EarthSE~\cite{xu2025earthsebenchmarkevaluatingearth} builds a multi-level evaluation system from foundational to open-ended exploration, and MSEarth~\cite{zhao2025msearthmultimodalscientificdataset} utilizes high-quality scientific publications for graduate-level assessment. In remote sensing, GeoBench~\cite{danish2025geobenchvlmbenchmarkingvisionlanguagemodels} and XLRS-Bench~\cite{wang2025xlrsbenchmultimodalllmsunderstand} evaluate perception and reasoning on high-resolution imagery. 
Furthermore, specialized benchmarks exist for other fields, including \textbf{material science} (MoleculeNet~\cite{wu2018moleculenetbenchmarkmolecularmachine}), \textbf{astronomy} (AstroLLaMA and AstroMLab~\cite{pan2024astromlab2astrollama270bmodel}), \textbf{ocean science} (OceanBench~\cite{aouni2025oceanbench}), and \textbf{climate science} (ClimaQA~\cite{manivannan2025climaqaautomatedevaluationframework}). These works primarily target deep evaluation within isolated disciplines. While benchmarks like ATLAS~\cite{liu2025atlashighdifficultymultidisciplinarybenchmark} have expanded to cover cross-disciplinary fields with high-difficulty standards, its evaluation specifically focuses on distinguishing frontier models through complex scientific reasoning and logical application tasks rather than the entire process of scientific discovery.

\subsection{Benchmarks for Different Scientific Tasks}
Concurrently, other benchmarks focus on cross-disciplinary comprehensive capabilities, though their evaluation focus is often distributed across specific stages of the scientific discovery pipeline. 
Regarding \textbf{idea generation} at the research inception stage, MOOSE-Chem2~\cite{moose} evaluates models through a win/tie/lose comparison framework that scores generated hypotheses against reference answers using multiple independent judges. AI Idea Bench 2025~\cite{qiu2025aiideabench2025} evaluates the novelty of agent-generated ideas using a dataset derived from top-tier conference papers. 
In the core layer of \textbf{knowledge processing and analysis}, some benchmarks focus on literature comprehension. For instance, SciAssess~\cite{cai2024sciassessbenchmarkingllmproficiency} decomposes analysis into memory, understanding, and reasoning layers. Others, like SFE~\cite{zhou2025scientistsexamprobingcognitive}, introduce a cognitive framework to dissect multimodal performance on raw scientific data. Complementing these, SciReasoner~\cite{wang2025scireasonerlayingscientificreasoning} targets the alignment of natural language with heterogeneous scientific representations. 
Recent works also evaluate comprehensive \textbf{academic survey} capabilities: DeepResearch Bench~\cite{du2025deepresearchbenchcomprehensivebenchmark} measures report quality and citation grounding, Manalyzer~\cite{xu2025manalyzerendtoendautomatedmetaanalysis} focuses on mitigating hallucinations in automated meta-analysis, and Scientist-Bench~\cite{tang2025airesearcherautonomousscientificinnovation} highlights the full workflow from review to paper generation. Additionally, SciArena ~\cite{zhao2025sciarenaopenevaluationplatform} proposed an open platform that dynamically evaluates and ranks the performance of base models on scientific literature tasks by collecting pairwise comparison preferences from domain researchers, and DeepResearch Arena~\cite{wan2025deepresearcharenaexamllms} utilizes seminar-grounded tasks to evaluate the orchestration of multi-stage research workflows, while AAAR-1.0~\cite{lou2025aaar10assessingaispotential} focuses on evaluating the model's ability as an AI-assisted research tool.  
In terms of \textbf{planning and execution}, evaluations often center on tool usage and coding. ToolBench~\cite{qin2023toolllmfacilitatinglargelanguage} and ToolUniverse~\cite{gao2025democratizingaiscientistsusing} explore API usage and standardization. In scientific coding, SciCode~\cite{tian2024scicoderesearchcodingbenchmark} and ScienceAgentBench~\cite{chen2025scienceagentbenchrigorousassessmentlanguage} assess code generation within realistic workflows. At a macro level, MLE-bench~\cite{chan2025mlebenchevaluatingmachinelearning} and TaskBench~\cite{shen2024taskbenchbenchmarkinglargelanguage} evaluate general planning and project management via Kaggle competitions and task decomposition graphs.
In addition, DISCOVERYWORLD~\cite{jansen2024discoveryworldvirtualenvironmentdeveloping} launched the first virtual environment for evaluating the ability of intelligent agents to perform a complete cycle of novel scientific discovery. However, it focuses on a gamified simulation environment, and its task scenarios and evaluation dimensions cannot fully reflect the complexity and high-level cognitive needs of real scientific research workflows. LLM-SRBench~\cite{shojaee2025llmsrbenchnewbenchmarkscientific} , on the other hand, focuses only on the model's ability to discover scientific equations, with a relatively simple task and process.
Despite these explorations, existing process-oriented benchmarks typically address only partial dimensions—such as knowledge understanding, data perception, or code generation—lacking a fine-grained, systematic evaluation of the entire scientific discovery lifecycle. 

\paragraph{Summary}
In summary, existing works are either confined to deep exploration of single disciplines, scattered across isolated stages of the research process, or fail to capture the complexity of actual scientific discovery scenarios. Therefore, there is an urgent need to construct a comprehensive benchmark that covers multiple disciplines and connects the long-chain workflow of scientific research.
