\begin{abstract}


\homepage\ \textbf{Page} \texttt{\url{https://prismax-team.github.io/SGI-Page/}}

\github\ \textbf{Code} \texttt{\url{https://github.com/PrismaX-Team/SGI-Bench}}

\huggingface\ \textbf{Data} \texttt{\url{https://huggingface.co/collections/PrismaX/sgi-bench}}

\team\ \textbf{Team} \texttt{\url{https://discovery.intern-ai.org.cn/sciprismax}}

\vspace{0.5cm}

\textbf{Abstract:}

Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)—the ability to autonomously conceive, investigate, and reason across scientific domains—remains lacking. 
We present a principled SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, AI-assisted experiments (dry/wet), and experimental reasoning. 
SGI-Bench comprises ~1{,}000 expert-curated, cross-disciplinary samples inspired by Science’s 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20\%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low PassAll@k in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. 
We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without retraining. 
Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.



\end{abstract}