# 从 AGI 到 SGI：Gemini‑3‑Pro 拿下 SOTA，却仍然显著不及格

- 项目主页 Page：https://InternScience.github.io/SGI-Page/
- 代码 Code：https://github.com/InternScience/SGI-Bench
- 数据 Data：https://huggingface.co/collections/InternScience/sgi-bench

如今，大模型在理解、推理、编程等方面表现突出，但AI的“科学通用能力”（SGI）尚无统一标准。SGI强调多学科、长链路、跨模态与严谨可验证性，而现有基准仅覆盖碎片能力（如学科问答、单步工具操作），难以反映真实科研中的循环与自纠错。为此，上海人工智能实验室通过引入实践探究模型（PIM），将科学探究拆解为四个循环阶段，并与 AI 能力维度对应：

![图片1：SGI 能力维度概览](md_images/teaser.png)

- Deliberation（审思/深度研究）：复杂问题下的检索、证据综合与批判评估；
- Conception（构思/创意生成）：提出新假说与可执行研究方法；
- Action（行动/实验执行）：将想法转化为计算代码（干实验）与实验室流程（湿实验）；
- Perception（感知/结果解读）：整合多模态证据并进行因果、比较等分析推理。

团队将上述四维能力的综合定义为 SGI，并发布覆盖全流程的 SGI‑Bench。首轮结果：闭源模型 Gemini‑3‑Pro 以 SGI‑Score 33.83/100 取得 SOTA，但距离“会做研究”的门槛仍显著不足。

![图片2：SGI-Score 总览](md_images/sgi_score.png)

## SGI-Bench：以科学家工作流对齐的全流程评测

SGI‑Bench 采用“科学家对齐（scientist-aligned）”的任务构造：多学科专家提供原始语料与示例需求（研究方向、图文材料等）及少量种子问题（seed questions）；研究生/博士生标注者据输入输出结构与种子问题生成题目；经规则校验、模型校验、专家复核三重清洗；最终再以多模型难度筛选剔除简单样本。最终得到 1000 多个覆盖 10 大学科（化学、生命、物理、数学等）的评测样本。

![图片3：任务定义与构造流程](md_images/pipeline.png)

![图片4：SGI-Bench 学科分布](md_images/subjects.png)


## 核心结果与洞见：今天的“强模型”，尚未成为“强科学家”

### 深度研究（Deep Research）：逐步准确率高于严格匹配，长链路“结论崩塌”

任务模拟文献元分析与多跳检索，要求在明确约束下检索并整合跨来源证据、进行定量推理，输出可核验结论。

![图片5：深度研究案例，多跳检索与定量证据整合](md_images/deepresearch_case1.png)

实验结果：

- 步骤准确率达 50%–65%，但长链条步骤中的错误导致最终结论频繁错误，答案严格匹配仅 10%–20%。  
![图片6：LLMs 在深度研究任务的表现](md_images/LLMs_deep_research_metrics.png)
- 工具增强的多智能体在逐步准确率略优，但与纯模型差距并不显著。  
![图片7：Agents 在深度研究任务的表现](md_images/Agents_deep_research_metrics.png)
- 类型上，“数据/性质”题最难，需跨文献精确检索与数值聚合；“微/宏实验”类相对较好但整体仍低于 30%，体现元分析难度的严苛性。  
![图片8：LLMs 与 Agents 在四类深度研究问题上的差异](md_images/LLMs_Agents_task_metric.png)

### 创意生成（Idea Generation）：新颖度尚可，但可行性偏低

面向方法学与方案设计，考察将灵感转化为可执行蓝图的能力（包含创新点、方法步骤，数据，指标等）。

![图片9：创意生成评测示例](md_images/idea_case.png)

实验结果：

![图片10：创意生成的指标结果](md_images/idea_metrics.png)
- 闭源模型“新颖性（Novelty）”更强，但“可行性（Feasibility）”普遍偏低。以 GPT‑5 为例：新颖性 76.08、可行性 18.87，体现“概念丰富 ≠ 可执行方案”。  
- 开源可行性上限约 20 分（如 Qwen3‑Max 20.98），多数模型 14–20 分，显示“能说清”与“能落地”之间的落差。
- 常见缺陷：缺少数据获取与预处理计划；流程接口不闭合（输入输出不对齐）；步骤顺序与依赖模糊，导致“创意→蓝图→执行”闭环断裂。

### 干实验（Dry Experiment）：可运行 ≠ 科学正确

根据科学背景，将缺失函数补全到主代码中，检验科学代码合成、数值稳健性与算法精确性，强调严格正确与可执行。

![图片11：干实验代码补全示例](md_images/code_case1.png)

实验结果：

![图片12：干实验通过率](md_images/dry_metrics.png)
- 每题含 5 个单测，最佳 Gemini‑3‑Pro 的严格通过率（全过 5 个单测）仅 36.64%，宽松通过率（至少过 1 个）41.98%，表明模型常能写对部分，但难以实现严格正确。
- 闭源模型略优于开源，但优势有限且分布重叠，“科学代码合成”仍是各架构共同短板。  
![图片13：不同类型的函数补全任务的表现](md_images/dry_task_metric.png)
- 平滑执行率（无报错运行）多在 90%+，显示“能跑”与“算对”之间存在系统性鸿沟。
- 类型上，数据处理/预测建模较稳；数值计算与仿真最弱，受离散化、稳定性与约束处理影响。例：引力波体积估计中，前向累加（np.cumsum）与自适应积分（scipy.integrate.quad）差异巨大；前者累积误差经 χ(z) 影响 dV/dz，最终体积严重偏离。  
![图片14：干实验案例，数值方法差异导致科学量偏差](md_images/code_case2.png)

### 湿实验（Wet Experiment）：动作时序、分支与参数选择是硬伤

基于实验背景与原子动作池，生成带参数的原子动作序列，以检验流程规划、顺序依赖与复杂约束的正确处理。

![图片15：湿实验协议执行示例](md_images/wet_case1.png)

实验结果：

- 序列相似度整体偏低，最佳闭源约 35.5；参数准确率最高约 40.6；部分闭源参数准确率显著下跌（约 20.7）。  
- 高发错误：插入多余步骤、遗漏关键步骤、打乱有效步骤顺序。  
![图片16：湿实验顺序相似度与参数准确率](md_images/wet_metrics.png)
- 在 NSCLC 抗 PD‑1 流程中，常见错误包括：将纵向采样简化为一次采血；PBMC 只在单一时间点分离；功能测定未按时间/刺激分组；基因组测序与免疫表型流程混用样本等，反映时间协调、分支规划与样本管理薄弱。  
![图片17：复杂湿实验案例，动作时间与分支管理的失败](md_images/wet_case2.png)

### 多模态实验推理（Experimental Reasoning）：因果推理尚可，比较推理最难

综合解读多模态证据（图像、流程、可视化等），识别跨模态线索、建模变量关系，进行比较与因果判断，输出可读推理与准确答案。

![图片18：多模态图片样例](md_images/mm_case1.png)

![图片19：实验推理示例](md_images/mm_case_1.png)

实验结果：

- 闭源整体更强：最佳闭源答案准确率约 41.9、推理有效性最高约 57.1。
- 多数模型推理有效性高于答案准确率：难以实现推理链条的完全正确。
![图片20：多模态科学推理的整体表现](md_images/mcq_metric.png)
- 推理类型上，因果推断与感知识别较稳；比较型最弱，涉及跨样本细粒度对比与一致性判别。学科上，天文最佳，物理、生命等学科挑战较大。
![图片21：按推理类型与学科的拆解](md_images/mcp_task_metric.png)


## 实时进化：测试时强化学习（TTRL）带来“无需答案”的增长

科学创意生成无“标准答案”，传统离线监督难奏效。团队提出在推理时以“新颖性奖励”自我优化的 TTRL：

- 流程：在线检索相关文献 → 计算语义相似度 → 构造新颖性奖励（越不相似越高分） → 使用 GRPO 优化策略。
![图片22：TTRL 训练框架](md_images/grpo_train_process.png)
- 基座模型：Qwen3‑8B（开源）。
- 结果：无需标注，新颖度由 49.36 提升至 62.06；生成结构更具体，从“拼装套路”走向“结构化创新”。
![图片23：奖励曲线，先满足格式，再拉升新颖](md_images/grpo_reward_curves.png)

这表明 SGI‑Bench 不止静态评测，还可在测试时借弱反馈不断涌现与进化。

## 结语：SGI-Bench 不止一个基准，更是一条路线图

SGI‑Bench 的结果为 AI 自主科研指明方向：

1. 深度研究：强化证据聚合与数值鲁棒性，提升深层研究准确性。 
2. 创意生成：引入规划感知与结构化监督，保障创意可行与执行细节完备。 
3. 代码生成：训练需超越语法，聚焦数值分析先验与算法稳定性。 
4. 湿实验协议：结合状态模拟，重点解决时序逻辑与复杂分支。 
5. 多模态推理：通过细粒度视觉定位与对比训练，提升比较推理精度。 
6. 测试时学习：优化多目标科学奖励体系，平衡新颖性、严谨性与安全性。