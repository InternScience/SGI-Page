# 从 AGI 到 SGI：Gemini‑3‑Pro 拿下 SOTA，却仍然显著不及格

- 项目主页 Page：https://InternScience.github.io/SGI-Page/
- 代码 Code：https://github.com/InternScience/SGI-Bench
- 数据 Data：https://huggingface.co/collections/InternScience/sgi-bench

近年来，大模型在知识理解、推理、编程等任务上表现突出，但AI的“科学通用能力”（SGI）尚无统一标准。SGI要求多学科、长链路、跨模态和严谨可验证性，而现有基准（如学科问答、单步工具操作）仅覆盖碎片能力，难以反映真实科研中的循环与自纠错过程。对此，上海人工智能实验室PrismaX团队引入实践探究模型（PIM），将科学探究拆解为四个循环阶段，并与AI能力维度对应：

![图片1：SGI 能力维度概览](md_images/teaser.png)

- Deliberation（审思/深度研究）：面对复杂问题进行检索、证据综合与批判评估；
- Conception（构思/创意生成）：提出新假说与可执行的研究方法；
- Action（行动/实验执行）：把想法转化为计算代码（干实验）与实验室流程（湿实验）；
- Perception（感知/结果解读）：整合多模态证据并进行因果，比较等分析推理。

把“能否做科学”从口号变成刻度，该团队将上述4种AI能力的综合作为科学通用能力SGI的定义，并发布覆盖全流程的SGI‑Bench。首轮结果显示：闭源模型 Gemini‑3‑Pro 以 SGI‑Score 33.83/100 取得 SOTA，但距离“会做研究”的门槛仍有显著差距。

![图片2：SGI-Score 总览](md_images/sgi_score.png)

## SGI-Bench：以科学家工作流对齐的全流程评测

为贴近真实科研，SGI‑Bench采用“科学家对齐（scientist-aligned）”的任务构造流程：首先由多学科专家提供原始语料与示例需求（研究方向、图文材料等），以及少量的题目作为种子问题（seed questions）；随后由研究生或博士生标注者基于输入输出结构和种子问题生成题目；经过规则校验、模型校验与专家复核三重清洗；最后通过多模型难度筛选剔除简单样本，最终得到覆盖10大学科（化学、生命、物理、数学等）的超过1000个评测样本。

![图片3：任务定义与构造流程](md_images/pipeline.png)

![图片4：SGI-Bench 学科分布](md_images/subjects.png)


## 核心结果与洞见：今天的“强模型”，尚未成为“强科学家”

### 深度研究（Deep Research）：逐步准确率高于严格匹配，长链路“结论崩塌”

深度研究任务模拟科研中的系统综述与多跳检索。它要求模型在明确的约束下，检索并整合跨来源证据，进行定量推理，最终给出可核验的结论。

![图片5：深度研究案例，多跳检索与定量证据整合](md_images/deepresearch_case1.png)

实验结果表明：

- 步骤准确率（Step‑Level Accuracy）可达 50%–65%，但在完整的推理链中仍频繁失手，严格匹配（Exact Match）普遍仅在 10%–20% 区间，最终导致结论错误。  

![图片6：LLMs 在深度研究任务的表现](md_images/LLMs_deep_research_metrics.png)

- 工具增强的多智能体在逐步准确率上略有优势，但与纯模型相比差距并不显著；

![图片7：Agents 在深度研究任务的表现](md_images/Agents_deep_research_metrics.png)

- 分类型看，“数据/性质”类题最难，需跨文献精确检索与数值聚合；“微/宏实验”类相对较好但整体仍低于 30%，体现出元分析难度对模型能力的严苛要求。

![图片8：LLMs 与 Agents 在四类深度研究问题上的差异](md_images/LLMs_Agents_task_metric.png)

### 创意生成（Idea Generation）：新颖度尚可，但可行性偏低

创意生成任务面向方法学与方案设计。它要求模型将研究灵感转化为可执行的蓝图。旨在考察模型定义问题、提出创新点，并确保实施方案参数完备、依赖一致的能力。

![图片9：创意生成评测示例](md_images/idea_case.png)

实验结果表明：

![图片10：创意生成的指标结果](md_images/idea_metrics.png)

- 闭源模型在“新颖性（Novelty）”上整体更强，但在“可行性（Feasibility）”上普遍偏低。以 GPT‑5 为例：新颖性 76.08，但可行性仅 18.87，表明“概念丰富 ≠ 可执行方案”。  
- 开源可行性上限约在 20 分左右（如 Qwen3‑Max 20.98），多数模型落在 14–20 区间，体现“想法能说清”与“方案能落地”之间的系统性落差。
- 模型提出的方法中，高频缺陷包括：缺少数据获取与预处理计划，流程接口不闭合（输入输出未对齐）、步骤顺序与依赖模糊，导致“创意→蓝图→执行”的闭环断裂。  

### 干实验（Dry Experiment）：可运行 ≠ 科学正确

干实验任务要求模型根据科学背景，补全缺失函数到主代码中，旨在检验模型的科学代码合成能力、数值稳健性以及对科学算法的精确把握，强调严格正确与可执行，而非仅“能跑通”。

![图片11：干实验代码补全示例](md_images/code_case1.png)

实验结果表明：

![图片12：干实验通过率](md_images/dry_metrics.png)

- 每个编程题包含5个单元测试，最佳的 Gemini-3-Pro 的严格通过率（通过全部5个单元测试）仅为 36.64%，而宽松通过率（至少通过1个单元测试）可达 41.98%，表明模型能够写对部分，但难以实现严格正确。

- 闭源模型在 PassAll@k 上整体略优于开源，但优势有限且分布重叠，显示“科学代码合成”仍是各架构的共同短板。

![图片13：不同类型的函数补全任务的表现](md_images/dry_task_metric.png)

- 平滑执行率（无报错运行）在多数前沿模型上超过 90%，表明“能跑”与“算对”之间存在系统性鸿沟。
- 按功能类型看，数据处理/预测建模相对更稳；数值计算与仿真最薄弱，易受离散化、稳定性与约束处理影响。下例展示了在引力波体积估计中，前向累加（np.cumsum）与自适应积分（scipy.integrate.quad）的巨大差异：前者累积误差放大，进而通过 χ(z) 影响体积元素 dV/dz，导致最终体积严重偏离。

![图片14：干实验案例，数值方法差异导致科学量偏差](md_images/code_case2.png)

### 湿实验（Wet Experiment）：动作时序、分支与参数选择是硬伤

湿实验任务要求模型基于实验背景和原子动作池，生成带参数的原子动作序列，以检验模型在实验流程规划、顺序依赖、参数设定以及对时序、分支等复杂约束的正确处理能力。

![图片15：湿实验协议执行示例](md_images/wet_case1.png)

实验结果表明：

- 序列相似度（Sequence Similarity）整体偏低，最佳闭源模型约 35.5；参数准确率（Parameter Accuracy）最高约 40.6，部分开源模型在参数上可与闭源竞争；部分闭源模型在参数上也出现明显下跌（约 20.7）。  

- 三类错误高发：插入多余步骤、遗漏关键步骤与打乱有效步骤顺序。 

![图片16：湿实验顺序相似度与参数准确率](md_images/wet_metrics.png)

- 在 NSCLC 抗 PD‑1 流程中，常见错误包括把纵向采样简化为一次采血、PBMC 仅在单一时间点分离、功能测定未按时间/刺激分组、基因组测序与免疫表型流程混用样本等，反映模型在时间协调、分支规划与样本管理上的薄弱。

![图片17：复杂湿实验案例，动作时间与分支管理的失败](md_images/wet_case2.png)

### 多模态实验推理（Experimental Reasoning）：因果推理尚可，比较推理最难

实验推理任务要求模型综合解读多模态证据（图像、流程、数据可视化等），以识别跨模态线索、建模变量关系，并进行跨样本比较和因果判断，最终输出可读的推理过程和准确答案，对应科研中将观察转化为结论的数据分析与结果解读环节。

![图片18：多模态图片样例](md_images/mm_case1.png)

![图片19：实验推理示例](md_images/mm_case_1.png)

实验结果表明：

- 多选准确率（Multi‑choice Accuracy）与推理有效性（Reasoning Validity）均显示闭源模型整体更强：最佳闭源多选准确率约 41.9，推理有效性最高约 57.1。
- 多数模型的推理有效性（RV）普遍高于多选准确率（MCA）：即使最终选项不正确，解释往往仍保持部分逻辑一致性。

![图片20：多模态科学推理的整体表现](md_images/mcq_metric.png)

- 推理类型上，因果推断与感知识别相对更稳；比较型推理最弱，涉及跨样本细粒度对比与一致性判别。学科上，天文最佳，其次化学、能源与神经科学；材料、生命、地球科学更具挑战，受视觉线索异质性与强背景依赖影响更大。

![图片21：按推理类型与学科的拆解](md_images/mcp_task_metric.png)


## 实时进化：测试时强化学习（TTRL）带来“无需答案”的增长

科学创意生成没有“标准答案”，传统离线监督难以奏效。该团队提出在推理时通过“新颖性奖励”进行自我优化的 TTRL：

- 流程：在线检索相关文献 → 计算语义相似度 → 构造新颖性奖励（越不相似越得分） → 使用 GRPO 优化策略。  

![图片22：TTRL 训练框架](md_images/grpo_train_process.png)

- 基座模型：Qwen3-8B（开源）。  
- 结果：无需标注，创意新颖度由 49.36 提升至 62.06；生成结构更具体，从“拼装套路”走向“结构化创新”。

![图片23：奖励曲线，先满足格式，再拉升新颖](md_images/grpo_reward_curves.png)

这表明 SGI-Bench 不只是静态评测，而是可以在测试时通过弱反馈不断涌现与进化的能力。下一步，将“新颖性”与“严谨性/可行性/成本/安全”等多目标奖励组合，才是走向“可靠创新”的关键。


## 结语：SGI-Bench 不止一个基准，更是一条路线图

通过SGI-Bench的结果，可以为未来AI进行自主科研指明前进方向：

1. 深度研究：需强化证据聚合与数值鲁棒性，以提升深层研究的准确性。 
2. 创意生成：引入规划感知与结构化监督，确保科学创意的可行性与执行细节。 
3. 代码生成：代码训练需超越语法层面，聚焦数值分析先验与算法稳定性。 
4. 湿实验协议：结合状态模拟技术，重点解决实验协议中的时序逻辑与复杂分支问题。 
5. 多模态推理：通过细粒度视觉定位与对比训练，提升多模态比较推理的精确度。 
6. 测试时学习：优化多目标科学奖励体系，平衡新颖性、严谨性与安全性。